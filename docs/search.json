[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rethinking2020",
    "section": "",
    "text": "Preface\nThis book is a study project of Statistical Rethinking, second edition, by Richard McElreath (McElreath 2020)."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "rethinking2020",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n\n\n\n\n\nImportant\n\n\n\nTo use the rethinking you must install it as advised by Richard McElreath as described in rethinking. You need to pay attention when it is done, but it works fine. Make sure you verify the installation with example(stan_model, package = \"rstan\", run.dontrun = TRUE) as described in the guides."
  },
  {
    "objectID": "index.html#session-info",
    "href": "index.html#session-info",
    "title": "rethinking2020",
    "section": "Session info",
    "text": "Session info\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16 ucrt)\n os       Windows 11 x64 (build 22621)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Toronto\n date     2023-10-09\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.1)\n digest        0.6.33  2023-07-07 [1] CRAN (R 4.2.3)\n evaluate      0.22    2023-09-29 [1] CRAN (R 4.3.1)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.2.3)\n htmltools     0.5.6.1 2023-10-06 [1] CRAN (R 4.3.1)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.2.3)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.2.3)\n knitr         1.44    2023-09-11 [1] CRAN (R 4.3.1)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.1)\n rmarkdown     2.25    2023-09-18 [1] CRAN (R 4.3.1)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.2.3)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.1)\n xfun          0.40    2023-08-09 [1] CRAN (R 4.3.1)\n yaml          2.3.7   2023-01-23 [1] CRAN (R 4.2.3)\n\n [1] C:/Users/Ephel/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.1/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://www.taylorandfrancis.com."
  },
  {
    "objectID": "ch01_golem.html",
    "href": "ch01_golem.html",
    "title": "1  The Golem of Prague",
    "section": "",
    "text": "This chapter is for reading and very interesting. It involves no actual coding, just a critical and open mind."
  },
  {
    "objectID": "ch02_worlds.html#garden-of-forking-data",
    "href": "ch02_worlds.html#garden-of-forking-data",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.1 Garden of forking data",
    "text": "2.1 Garden of forking data\nThere are 2 events: Blue marble represented by B and white marble represented by W\n\nevents &lt;- c(\"B\", \"W\")\n\nThe bag has 4 marbles of 2 colors, blue and white each possibilities, 0 to 4 blue marbles, is a conjecture (sample space).\n\nconjectures &lt;- lapply(0:4, \n                      function(i) c(rep(events[1], times = i), \n                                    rep(events[2], times = 4 - i))\n                      )\nconjectures &lt;- as.data.frame(do.call(rbind, conjectures))\nconjectures\n\n  V1 V2 V3 V4\n1  W  W  W  W\n2  B  W  W  W\n3  B  B  W  W\n4  B  B  B  W\n5  B  B  B  B\n\n\nwe draw 3 marbles from the bag which is the data (event)\n\ndata &lt;- c(\"B\", \"W\", \"B\")\ndata\n\n[1] \"B\" \"W\" \"B\"\n\n\nthe number of ways each conjecures could have generated the data is\n\nways &lt;- apply(X = conjectures, MARGIN = 1, \n              function(x) {\n                  sum(x == data[1]) * sum(x == data[2]) * sum(x == data[3])\n                  })\nways\n\n[1] 0 3 8 9 0\n\n\nSo the number of ways depends on the nb of blue marbles in the bag. Therefore we will assign different plausabiltity to the conjectures depending on the proportion of blue balls. This is an assumtpions on the \\(p\\), other values can be warranted.\n\nprior &lt;- apply(X = conjectures, MARGIN = 1, function(x) sum(x == events[1]) / 4)\nprior\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\nconjectures &lt;- cbind(conjectures, prior, ways)\nconjectures\n\n  V1 V2 V3 V4 prior ways\n1  W  W  W  W  0.00    0\n2  B  W  W  W  0.25    3\n3  B  B  W  W  0.50    8\n4  B  B  B  W  0.75    9\n5  B  B  B  B  1.00    0\n\n\nAnd the plausability of the prior after getting the new data is the data x the prior divided by the sum of all possible event\n\n# the multiplication of likelyhood and prior\nconjectures$post &lt;- conjectures$ways * conjectures$prior\n# the division by the sum to convert to pct summing up to 1\nconjectures$post &lt;- conjectures$post / sum(conjectures$post)  \nconjectures\n\n  V1 V2 V3 V4 prior ways       post\n1  W  W  W  W  0.00    0 0.00000000\n2  B  W  W  W  0.25    3 0.06521739\n3  B  B  W  W  0.50    8 0.34782609\n4  B  B  B  W  0.75    9 0.58695652\n5  B  B  B  B  1.00    0 0.00000000\n\n# verify the total of posterior is 1\nstopifnot(sum(conjectures$post) == 1)\n\n\n\n\n\n\n\nNote\n\n\n\nIf you run the above procedure, wou will find that the posterior is not always the same, it varies quite a bit. That’s why we run lots of sample and investigate their distribution to decide what is the right value for the parameter."
  },
  {
    "objectID": "ch02_worlds.html#building-a-model",
    "href": "ch02_worlds.html#building-a-model",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.2 Building a model",
    "text": "2.2 Building a model\nThe events are that the location on earth is earth represented by \\(L\\) or that the location on earth is water represented by \\(W\\).\n\nevents &lt;- c(\"L\", \"W\")\n\nThe conjecture is the amount of water on earth represented by the parameter p which we try to estimate. This time is a continuous value, we will use a grid to approximate the entire set of possibles conjectures\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 20)\n\nAnd , as priors, we assume that every possibilities of p is uniformely distributed.\n\nprior &lt;- rep(1, times = length(p_grid))\nprior\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nCompute likelihood for each value in the grid\n\nlikelihood &lt;- dbinom(x = 6, size = 9, prob = p_grid)\n\nCompute product of likelihood and prior\n\nunstd.posterior &lt;- likelihood * prior\n\nStandardize the posterior so it sum to 1\n\nposterior &lt;- unstd.posterior / sum(unstd.posterior)"
  },
  {
    "objectID": "ch02_worlds.html#components-of-the-model",
    "href": "ch02_worlds.html#components-of-the-model",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.3 Components of the model",
    "text": "2.3 Components of the model\n\n2.3.1 Likelihood\n\n\n2.3.2 Parameters\n\n\n2.3.3 Priors"
  },
  {
    "objectID": "ch02_worlds.html#making-the-model-go",
    "href": "ch02_worlds.html#making-the-model-go",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.4 Making the model go",
    "text": "2.4 Making the model go\n\n2.4.1 Grid approximation\n\n\n2.4.2 Quadratic approximation\n\nglobe.qa &lt;- rethinking::map(\n    alist(\n        w ~ dbinom(9, p),  # binomial likelihood\n        p ~ dunif(0, 1)  # uniform priors\n    ),\n    data = list(w = 6)\n    )\n\nrethinking::precis(globe.qa)\n\n       mean        sd      5.5%     94.5%\np 0.6666664 0.1571338 0.4155362 0.9177966"
  },
  {
    "objectID": "ch02_worlds.html#summary",
    "href": "ch02_worlds.html#summary",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.5 Summary",
    "text": "2.5 Summary\nGrid approximation can only be used with the simplest problems although if you understand it, you understand the fundamental method behind Bayesian stats.\nQuadratic approximation is a nice trick to remember as, when it can be used, it is very efficient. However I could not find a R package that actually does it.\nMCMC is, seemingly, the most used method in practice. It is the method used by the brms package"
  },
  {
    "objectID": "ch03_sampling.html#sampling-from-grid-approximation-posterior",
    "href": "ch03_sampling.html#sampling-from-grid-approximation-posterior",
    "title": "3  Sampling the Imaginary",
    "section": "3.1 Sampling from grid-approximation posterior",
    "text": "3.1 Sampling from grid-approximation posterior\nWe use the example from chapter 2.\nThe grid of \\(p\\) values has a grid size of \\(grid\\_size\\).\nThe prior is uniformly distributed and so, as discussed in Overthinking box of section 2.3.3, p. 35, \\(P(p)=\\frac{1}{1-0}=1\\).\n\nthe_grid &lt;- data.frame(\n prob = seq(from = 0, to = 1, length.out = 1000),\n prior = 1)\n\nand we calculate the posterior using the data. We compute the likelihood using the grid of priors from above, then compute the average likelihood which is the sum of the likelihood.\nThe posterior is defined in detailed in section 2.3.4, p. 37.\n\\[\n\\text{Posterior} =\n\\frac{\\text{Likelihood} \\times \\text{Prior}}\n  {\\text{Average Likelihood}}\n\\]\nNote the code stopifnot(sum(d$posterior) == 1), it is always a good idea to verify this . . . you could be surprised how often you miss the mark.\n\n# the data, see page 28\ndata &lt;- c(\"W\",\"L\",\"W\",\"W\",\"W\",\"L\",\"W\",\"L\",\"W\")\nn_success &lt;- sum(data == \"W\")\nn_trials &lt;- length(data)\n\n# compute the likelihood each value in the grid\nthe_grid &lt;- the_grid |&gt;\n    mutate(\n        likelihood = dbinom(x = n_success, size = n_trials, prob = prob),\n        posterior = likelihood * prior / sum(likelihood))\nassert_that(sum(the_grid$posterior) == 1,\n            msg = \"The total posterior prob. must equal 1.\")\n\n[1] TRUE\n\n\nwhich gives the estimated posterior probability \\(p\\) conditional on the data for each point of a grid.\nGenerate and visualize n_samples samples from the grid with the \\(p\\) values with their respective posterior probability \\(p\\) computed above.\n\nNote: We use dplyr::slice_sample because dplyr::sample_n is deprecated.\n\n\nset.seed(1223)\nthe_samples &lt;- the_grid |&gt;\n  slice_sample(n = 1e4, weight_by = posterior, replace = TRUE) |&gt;\n  # this distance from the mean is used for coloring\n  mutate(dist = abs(prob - mean(prob)))\nthe_samples$id &lt;- seq_len(nrow(the_samples))\n# str(the_samples)\n\nvisualize the sample of water proportion\n\nggplot(data = the_samples, mapping = aes(x = id, y = prob, color = dist)) +\n  geom_point(size = 0.75, alpha = 0.9) +\n  scale_color_gradientn(colors = paletteer_d(palette=\"Manu::Kotare\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = sprintf(\"%d samples\", nrow(the_samples)))\n\n\n\n\nvisualize the density\n\n# show the p density\nggplot(data = the_samples, aes(x = prob)) +\n    geom_density(aes(y=..scaled..), color = \"blue\", size = 1, fill = \"lightblue\") +\n    theme_light() +\n    labs(title = sprintf(\"%d samples\", nrow(the_samples)))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..scaled..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(scaled)` instead."
  },
  {
    "objectID": "ch03_sampling.html#sampling-to-summarize",
    "href": "ch03_sampling.html#sampling-to-summarize",
    "title": "3  Sampling the Imaginary",
    "section": "3.2 Sampling to summarize",
    "text": "3.2 Sampling to summarize\n\n3.2.1 Intervals of defined boundaries\n\nthe_grid |&gt;\n  filter(prob &lt; 0.5) |&gt;\n  summarize(sum = sum(posterior))\n\n        sum\n1 0.1718746\n\n\nand you can obtain the same result using the sampling data by counting the rows\n\nthe_samples |&gt;\n  filter(prob &lt; 0.5) |&gt;\n  count() |&gt;\n  mutate(pct = n / nrow(the_samples)) |&gt;\n  identity()\n\n     n    pct\n1 1744 0.1744\n\n\n\n\n3.2.2 Intervals of defined mass\nBeside the base R quantile function, the mean_qi function from the package ggdist will be used extensively in this project. The benefits of using this package in conjonction with posterior, tidybayes etc. will become obvious in later chapters.\n\nthe_samples |&gt;\n  ggdist::mean_qi(prob, .width = 0.8)\n\n# A tibble: 1 × 6\n   prob .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 0.636  0.444  0.813    0.8 mean   qi       \n\n\nAnd if we redo the sampling with observing 3 \\(W\\) in 3 tosses we have the grid\n\nthe_grid &lt;- data.frame(\n  prob = seq(from = 0, to = 1, length.out = 1000),\n  prior = 1) |&gt;\n  mutate(\n    likelihood = dbinom(x = 3, size = 3, prob = prob),\n    posterior = likelihood * prior / sum(likelihood))\nassert_that(sum(the_grid$posterior) == 1,\n            msg = \"The total posterior prob. must equal 1.\")\n\n[1] TRUE\n\n\nand we use it to resample\n\nset.seed(1223)\nthe_samples &lt;- the_grid |&gt;\n  slice_sample(n = 1e4, weight_by = posterior, replace = TRUE) |&gt;\n  # this distance from the mean is used for coloring\n  mutate(dist = abs(prob - mean(prob)))\nthe_samples$id &lt;- seq_len(nrow(the_samples))\n\n\nthe_samples |&gt;\n  ggdist::mean_qi(prob, .width = 0.5)\n\n# A tibble: 1 × 6\n   prob .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 0.800  0.708  0.931    0.5 mean   qi       \n\n\n\nthe_samples |&gt;\n  ggdist::mean_hdi(prob, .width = 0.5)\n\n# A tibble: 1 × 6\n   prob .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 0.800  0.838  0.998    0.5 mean   hdi      \n\n\nand we can illustrate the intervals with ggdist as follows\n\nqtl &lt;- c(0.5, 0.8, 0.95, 1)\nx_breaks &lt;- ggdist::mean_qi(.data = the_samples$prob, \n                            .width = qtl) |&gt;\n  select(y, ymin, ymax) |&gt;\n  pivot_longer(cols = c(\"y\", \"ymin\", \"ymax\")) |&gt;\n  distinct(value) |&gt;\n  arrange(value) |&gt;\n  round(digits = 2) |&gt;\n  pull()\nggplot(the_samples, aes(x=prob)) +\n         stat_halfeye(aes(fill=stat(cut_cdf_qi(\n           cdf,\n           .width = qtl,\n           labels = scales::percent_format()\n           )))) +\n  scale_x_continuous(breaks = x_breaks) +\n  scale_fill_paletteer_d(palette = \"Manu::Takahe\", direction = -1,\n                         na.translate = FALSE) +\n  theme_ggdist() +\n  theme(legend.position = c(0.1, 0.75)) +\n  labs(title = \"Intervals of defined mass\",\n       x = \"p_grid\", y = \"prob of p_grid\",fill = \"quantiles\")\n\nWarning: `stat(cut_cdf_qi(cdf, .width = qtl, labels = scales::percent_format()))` was\ndeprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(cut_cdf_qi(cdf, .width = qtl, labels =\n  scales::percent_format()))` instead.\n\n\n\n\n\n\n\n3.2.3 Point estimates (loss function)\nThe linex loss function can be very useful in business analysis. This is to be investigated later."
  },
  {
    "objectID": "ch03_sampling.html#sampling-to-simulate-prediction",
    "href": "ch03_sampling.html#sampling-to-simulate-prediction",
    "title": "3  Sampling the Imaginary",
    "section": "3.3 Sampling to simulate prediction",
    "text": "3.3 Sampling to simulate prediction"
  },
  {
    "objectID": "ch03_sampling.html#summary",
    "href": "ch03_sampling.html#summary",
    "title": "3  Sampling the Imaginary",
    "section": "3.4 Summary",
    "text": "3.4 Summary"
  },
  {
    "objectID": "ch04_linear.html#why-normal-distributions-are-normal",
    "href": "ch04_linear.html#why-normal-distributions-are-normal",
    "title": "4  Linear Models",
    "section": "4.1 Why normal distributions are normal",
    "text": "4.1 Why normal distributions are normal\nGaussian distribution\n\\[\n\\begin{equation}\nP \\left(y \\mid \\mu, \\sigma \\right) =\n\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp{\\left[-\\frac{1}{2}\n\\left(\\frac{y-\\mu}{\\sigma} \\right)^2\n\\right]}\n\\end{equation}\n\\]\ngaussian distribution expressed with \\(precision = \\tau\\) is \\(\\sigma = \\frac{1}{\\sqrt{\\tau}}\\)\n\\[\n\\begin{equation}\nP \\left(y \\mid \\mu, \\tau \\right) =\n\\frac{\\tau}{\\sqrt{2 \\pi}} \\exp{\\left[-\\frac{\\tau}{2}\n\\left(y-\\mu \\right)^2\n\\right]}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "ch04_linear.html#a-language-for-describing-model",
    "href": "ch04_linear.html#a-language-for-describing-model",
    "title": "4  Linear Models",
    "section": "4.2 A language for describing model",
    "text": "4.2 A language for describing model\n\\[\n\\begin{align*}\noutcome_i &\\sim \\mathcal{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta \\times predictor_i \\\\\n\\beta &\\sim \\mathcal{Normal}(0, 10) \\\\\n\\sigma &\\sim \\mathcal{HalfCauchy}(0, 1)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch04_linear.html#a-gaussian-model-of-height",
    "href": "ch04_linear.html#a-gaussian-model-of-height",
    "title": "4  Linear Models",
    "section": "4.3 A Gaussian model of height",
    "text": "4.3 A Gaussian model of height\n\n4.3.1 The data\n\ndata(\"Howell1\")\ndataHowel &lt;- Howell1\nrm(Howell1)\n\nwhich we can visualize using skimr\n\nskimr::skim(dataHowel) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = where(is.numeric), .fns = round, digits = 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\nData summary\n\n\nName\ndataHowel\n\n\nNumber of rows\n544\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n0\n1\n138.26\n27.60\n53.98\n125.10\n148.59\n157.48\n179.07\n▁▂▂▇▇\n\n\nweight\n0\n1\n35.61\n14.72\n4.25\n22.01\n40.06\n47.21\n62.99\n▃▂▃▇▂\n\n\nage\n0\n1\n29.34\n20.75\n0.00\n12.00\n27.00\n43.00\n88.00\n▇▆▅▂▁\n\n\nmale\n0\n1\n0.47\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\n\n\n\nselect only the adults\n\ndataHowel_gte18 &lt;- dataHowel |&gt;\n  filter(age &gt;= 18)\n\n\n\n4.3.2 The model\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu, \\sigma)\\\\\n\\mu &\\sim \\mathcal{N}(178, 20) \\\\\n\\sigma &\\sim \\mathcal{Uniform}(0, 50)\n\\end{align*}\n\\]\nWe do the prior predictive simulation with the prior \\(\\mu \\sim \\mathcal{N}(178, 20)\\)\n\npriorHowel_gte18 &lt;- list()\npriorHowel_gte18 &lt;- within(priorHowel_gte18, {\n  n &lt;- 1e3\n  sim1 &lt;- data.frame(id = seq_len(n)) |&gt;\n  mutate(mu = rnorm(n = n(), mean = 178, sd = 20),\n         sigma = runif(n = n(), min = 0, max = 50),\n         height = rnorm(n = n(), mean = mu, sd = sigma))\n  })\n\nand we do the prior predictive simulation with the prior \\(\\mu \\sim \\mathcal{N}(178, 100)\\)\n\npriorHowel_gte18 &lt;- within(priorHowel_gte18, {\n  n &lt;- 1e4\n  sim2 &lt;- data.frame(id=seq_len(1e4)) %&gt;%\n    mutate(mu = rnorm(n = n(), mean = 178, sd = 100),\n           sigma = runif(n = n(), min = 0, max = 50),\n           height = rnorm(n = n(), mean = mu, sd = sigma))\n  })\n\nand we visualize using ggplot. We first create the 2 plots of analytical distribution plots\n\nplotPrior &lt;- list()\nplotPrior$normal &lt;- data.frame(mean = 178, sd = 20) |&gt;\n  (\\(.){\n    ggplot(data=.) +\n      geom_function(fun=dnorm, args=list(mean = .$mean, sd = .$sd),\n                    color = \"olivedrab4\", size = 1) +\n      scale_x_continuous(limits = c(.$mean - 3 * .$sd, .$mean  + 3 * .$sd),\n                     breaks = scales::breaks_width(width = 25)) +\n  labs(title = bquote(mu ~ .(sprintf(\"~ dnorm(%.0f, %.0f)\", .$mean, .$sd))), \n       x = expression(mu), y = \"density\") \n  })()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# plotPrior$normal\n\nplotPrior$uniform &lt;- data.frame(min = 0, max = 50) |&gt;\n  (\\(.){\n    ggplot(data=.) +\n      geom_function(fun=dunif, args=list(min=.$min, max=.$max),\n                    color=\"rosybrown2\", size=1) +\n      scale_x_continuous(limits = c(.$min - 2, .$max  + 2),\n                         breaks = scales::breaks_width(width=10)) +\n      labs(title = bquote(mu ~ .(sprintf(\"~ dunif(%.0f, %.0f)\", .$min, .$max))), \n           x = expression(sigma), y = \"density\")\n  })()\n# plotPrior$uniform\n\nwe add the actual plot of height to the plots created by McElreath to facilitate the understanding of priors. McElreath suggests that priors should follows the scientific knowledge. See the interesting discussion at the end of section 4.3.2 on p. 84.\n\nplotPrior &lt;- within(plotPrior, {\n  # the density of actual data used for comparisons\n  actual &lt;- \n      geom_density(data = dataHowel_gte18, aes(x = height),\n                   color = \"navy\", linewidth = 1, linetype = \"dashed\")\n})\n\n\nplotPrior &lt;- within(plotPrior, {\n  # NOTE: we use (\\(.){})() with |&gt;\n  # https://towardsdatascience.com/understanding-the-native-r-pipe-98dea6d8b61b\n  sim1 &lt;- priorHowel_gte18$sim1 |&gt;\n  (\\(.){\n    ggplot(data = ., aes(x = height)) +\n      geom_density(color = \"slateblue1\", linewidth = 1) +\n      theme(legend.position = c(0.1, 0.8)) +\n      labs(\n        title = expression(paste(\"h ~ dnorm(\", mu, \",\", sigma ,\")\")),\n        subtitle = sprintf(\"sample size = %d\", nrow(.)),\n        fill = \"quantile\")})()\n  sim1 &lt;- sim1 + actual  # add the actual data (not in McElreath's curve)\n})\n# plotPrior$sim1\n\n\nplotPrior &lt;- within(plotPrior, {\n  sim2 &lt;- priorHowel_gte18$sim2 |&gt;\n  (\\(.){\n    ggplot(data = ., aes(x = height)) +\n      geom_density(color = \"peru\", linewidth = 1) +\n      geom_vline(xintercept = 0, linetype = \"dotted\", color = \"navy\") +\n      theme(legend.position = c(0.1, 0.8)) +\n      labs(\n        title = expression(paste(\"h ~ dnorm(\", mu, \",\", sigma ,\")\")),\n        subtitle = sprintf(\"sample size = %d\", nrow(.)),\n        fill = \"quantile\")\n    })()\n  sim2 &lt;- sim2 + actual  # add the actual data (not in McElreath's curve)\n})\n# plotPrior$sim2\n\nand putting the plots together with patchwork we obtain\n\npatchwork::wrap_plots(list(plotPrior$normal, plotPrior$uniform,\n                           plotPrior$sim1, plotPrior$sim2)) + \n  plot_annotation(\n  title = \"Prior Predictive Simulation for the Height Model.\"\n)\n\n\n\n\n\n\n4.3.3 Grid approximation of posterior distribution\nFirst create the grid.\n\n# create grid of mu and sigma\npostHowel_gte18 &lt;- list()\npostHowel_gte18 &lt;- within(postHowel_gte18, {\n  ngrid &lt;- 200L\n  grid &lt;- data.frame(\n    mu = seq(from = 140, to = 160, length.out = ngrid),\n    sigma = seq(from = 4, to = 9, length.out = ngrid)) |&gt;\n    expand(mu, sigma)\n  })\n\nThen we calculate the likelihood. Since probabilities are percentage this causes a numerical issue as multiple multiplications of percentages will create very small numbers, so small in fact that they will be miscalculated.\nTo resolve this problem, we use logarithms.\nThat is the likelihood function from the model defined in 4.3.2\n\\[\nP(\\mu, \\sigma \\mid h) =\n\\prod_{i=1}^n \\mathcal{N}(y_i \\mid \\mu, \\sigma) \\cdot\n\\mathcal{N}(\\mu \\mid mean = 0, sd = 10) \\cdot\n\\mathcal{U}(\\sigma | min = 0, max = 10)\n\\]\nis transformed to log.\n\nImportant: Read the end note # 73 on page 449. All the explanations, including the usage of max(post$prob) is explained.\n\n\\[\n\\log{P(\\mu, \\sigma \\mid h)} =\n\\sum_{i=1}^n \\left[ \\log{\\mathcal{N}(y_i \\mid \\mu, \\sigma)} +\n\\log{\\mathcal{N}(\\mu \\mid mean = 0, sd = 10)} +\n\\log{\\mathcal{U}(\\sigma | min = 0, max = 10)} \\right]\n\\]\nand to compute the posterior distribution we compute the likelihood which is the first element of the addition\n\\[\n\\sum_{i=1}^n \\log{\\mathcal{N}(y_i \\mid \\mu, \\sigma)}\n\\]\nas follows\n\n# The likelihood on the log scale\npostHowel_gte18 &lt;- within(postHowel_gte18, {\n  grid &lt;- grid %&gt;%\n    mutate(LL = sapply(\n      X = seq_len(nrow(.)), \n      FUN = function(i) {\n        sum(dnorm(x = dataHowel_gte18$height, \n                  mean = grid$mu[i], \n                  sd = grid$sigma[i], \n                  log = TRUE))\n        })\n      )\n  })\nglimpse(postHowel_gte18$grid)\n\nRows: 40,000\nColumns: 3\n$ mu    &lt;dbl&gt; 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140,…\n$ sigma &lt;dbl&gt; 4.000000, 4.025126, 4.050251, 4.075377, 4.100503, 4.125628, 4.15…\n$ LL    &lt;dbl&gt; -3812.776, -3777.627, -3743.158, -3709.350, -3676.189, -3643.659…\n\n\nthen the remaining 2 elements of the summation are the priors\n\\[\n\\sum_{i=1}^n \\left[\n\\log{\\mathcal{N}(\\mu \\mid mean = 0, sd = 10)} +\n\\log{\\mathcal{U}(\\sigma | min = 0, max = 10)}\n\\right]\n\\]\nwhich we add to the likelihood to obtain the posterior distribution on the log scale\n\n# add the priors to the likelihood  on the log scales to obtain the\n# log of the posterior\npostHowel_gte18$grid &lt;- postHowel_gte18$grid |&gt;\n  mutate(prob =\n           LL + \n           dnorm(x = mu, mean = 178, sd = 20, log = TRUE) +\n           dunif(x = sigma, min = 0, max = 50, log = TRUE))\n\nand to convert the posterior back to the natural scale we exponentiate. The usage of max(the_grid$post) is explained in endnote 73. It is basically used as an approximation to what would be the denominator of the likelihood.\n\\[\n\\sum_{i=1}^n \\left[\n\\log{\\mathcal{N}(\\mu \\mid mean = 0, sd = 10)} +\n\\log{\\mathcal{U}(\\sigma | min = 0, max = 10)}\n\\right]\n\\]\n\\[\n\\exp{\\left[\\log{P(\\mu, \\sigma \\mid h)}\\right]} = P(\\mu, \\sigma \\mid h)\n\\]\n\n# convert back to real scale\n# attention: see endnote 73 on using max(prob)\npostHowel_gte18 &lt;- within(postHowel_gte18, {\n  grid$prob &lt;- with(grid, {exp(prob - max(prob))})\n})\n\nplot the results on a heatmap\n\nplotPost &lt;- list()\nplotPost$heat &lt;- ggplot(data = postHowel_gte18$grid, aes(x = mu, y = sigma, fill = prob)) +\n  geom_raster() +\n  scale_x_continuous(limits = c(153, 156)) +\n  scale_y_continuous(limits = c(6.5, 9)) +\n  scale_fill_paletteer_c(\"grDevices::Viridis\") +\n  coord_fixed() +\n  labs(title = \"The grid's posterior prob.\",\n       x = expression(mu), y = expression(sigma))\nplotPost$heat\n\nWarning: Removed 37129 rows containing missing values (`geom_raster()`).\n\n\n\n\n\n\n\n4.3.4 Sampling from the grid’s posterior\n\npostHowel_gte18$draws &lt;- postHowel_gte18$grid |&gt;\n  slice_sample(n = 1e4, weight_by = prob, replace = TRUE)\n\nand visualizing the density of \\(\\mu\\) and \\(\\sigma\\) together using ggExtra\n\nplotPost$marg &lt;- ggplot(data = postHowel_gte18$draws, \n                        mapping = aes(x = mu, y = sigma)) +\n  geom_point(color = \"mediumorchid\", size = 0.8) +\n  geom_jitter(color = \"mediumorchid\", size = 0.8) +\n  labs(title = expression(\"Distribution of\" ~ mu ~\"and\" ~ sigma ~ \" using a grid.\"),\n       x = expression(mu), y = expression(sigma))\nplotPost$marg &lt;- ggExtra::ggMarginal(plotPost$marg, \n                    xparams = list(colour = \"blue\", fill = \"lightblue\", \n                                   linewidth = 1),\n                    yparams = list(colour=\"darkgreen\", fill = \"lightgreen\", \n                                   linewidth = 1))\nplotPost$marg\n\n\n\n\n\n\n4.3.5 Finding the posterior distribution with quap and brm()\n\n4.3.5.1 using rethinking::map\nWe now fit the model using rethinking::quap()\n\nSee the overthinking box about list() vs alist() on p. 88 of chapter 4.\n\nThe model is\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu, \\sigma)\\\\\n\\mu &\\sim \\mathcal{N}(178, 20) \\\\\n\\sigma &\\sim \\mathcal{Uniform}(0, 50)\n\\end{align*}\n\\]\nand the fit is\n\nfit04_01quap &lt;- xfun::cache_rds(\n    {rethinking::quap(\n      data = dataHowel_gte18,\n      flist = alist(\n        height ~ dnorm(mu, sigma),\n        mu ~ dnorm(178, 28),\n        sigma ~ dunif(0, 50)),\n      start = list(\n        mu  = mean(dataHowel_gte18$height),\n        sigma = sd(dataHowel_gte18$height))\n      )},\n  file = \"ch04_fit04_01quap\")\n\nwhich gives us the summary\n\nprecis(fit04_01quap)\n\n            mean        sd       5.5%      94.5%\nmu    154.602156 0.4120367 153.943642 155.260671\nsigma   7.731328 0.2913854   7.265637   8.197018\n\n\nand the variance covariance matrix is\n\nvcov(fit04_01quap) |&gt;\n  round(digits = 3)\n\n        mu sigma\nmu    0.17 0.000\nsigma 0.00 0.085\n\n\nand the correlation matrix\n\ncov2cor(vcov(fit04_01quap))\n\n              mu      sigma\nmu    1.00000000 0.00092618\nsigma 0.00092618 1.00000000\n\n\n\n\n4.3.5.2 Using brms::brm\nThis borrows heavily from Kurz (2020)\nAs mentioned in chapter 8, it is best to use Half-Cauchy distribution for sigma as the tends to work better when using Half Cauchy for sigma when doing a Hamiltonian MCMC with brm().\nTherefore the model is\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu, \\sigma)\\\\\n\\mu &\\sim \\mathcal{N}(178, 20) \\\\\n\\sigma &\\sim \\mathcal{HalfCauchy}(0, 1)\n\\end{align*}\n\\]\n\nSee the overthinking box about half Cauchy distribution in chapter 8 on p. 260.\n\nThis process takes less than a second. It has been save to the rsd file b04_01.rds\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit04_01brm &lt;- xfun::cache_rds({\n  brms::brm(data = dataHowel_gte18,\n            formula = height ~ 1,\n            family = gaussian,\n            prior = c(prior(normal(178, 20), class = Intercept),\n                      prior(cauchy(0, 1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n            seed = 4)},\n  file = \"ch04_fit04_01brm\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.13 sec elapsed\n\n\n\nplot(fit04_01brm)\n\n\n\n\nwith the summary\n\nsummary(fit04_01brm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: dataHowel_gte18 (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.42   153.76   155.41 1.00     3454     2464\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.74      0.28     7.19     8.32 1.00     3956     2623\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nwhich can also be done with tidybayes::summarize_draws which comes from the posterior package.\n\n# normally we only use summarise_draws() but here we change it to match \n# the width of 0.89\ntidybayes::summarise_draws(fit04_01brm, mean, median, sd, mad, \n                           ~quantile2(.x, probs = c(0.055, 0.945))) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n# A tibble: 4 × 7\n  variable        mean   median    sd   mad     q5.5    q94.5\n  &lt;chr&gt;          &lt;num&gt;    &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 b_Intercept   155.     155.    0.42  0.42   154.     155.  \n2 sigma           7.74     7.73  0.28  0.28     7.31     8.22\n3 lprior         -9.16    -9.16  0.08  0.08    -9.29    -9.04\n4 lp__        -1228.   -1227.    1     0.72 -1229.   -1227.  \n\n\nand to plot the posteriors we need to know the names of the variables\n\ntidybayes::get_variables(fit04_01brm)\n\n [1] \"b_Intercept\"   \"sigma\"         \"lprior\"        \"lp__\"         \n [5] \"accept_stat__\" \"stepsize__\"    \"treedepth__\"   \"n_leapfrog__\" \n [9] \"divergent__\"   \"energy__\"     \n\n\nand we spread the data with one column per variable to be able to plot it. The tidybayes package is particularly useful for this. We will use it extensively from now on.\nIn particular, we can use tidybayes::spread_draws() to put variables in separate columns or tidybayes::gather_draws() to have them in long format.\nWe can visualize with ggdist. it could be done with tidybayes but since tidybayes reexport ggdist we use it directly.\n\npost04_01brm &lt;- fit04_01brm |&gt;\n    gather_draws(b_Intercept, sigma, ndraws = 500)\n\n\n# source: https://cran.r-project.org/web/packages/ggdist/vignettes/slabinterval.html\nplot04_01 &lt;- post04_01brm |&gt;\n    ggplot(aes(x = .value)) +\n    stat_halfeye(aes(fill = stat(level)), .width = c(0.89, 1)) +\n    scale_x_continuous(breaks = scales::breaks_extended(n = 7),\n                       labels = scales::label_number(accuracy = 0.1)) +\n    scale_y_continuous(labels = scales::label_percent()) +\n    scale_fill_paletteer_d(palette = \"ggsci::teal_material\",\n                           na.translate = FALSE) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Model 4.1: Posterior Distribution of Parameters\", x = NULL, y = NULL) +\n    facet_wrap(. ~ .variable, ncol = 1, scales = \"free\")\nplot04_01\n\nWarning: `stat(level)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(level)` instead.\n\n\n\n\n\n\n\n\n4.3.6 Sampling from a fit\n\n4.3.6.1 Using quap\nSince quap is a quadratic approximation, how do we simulate 2 variables, \\(\\mu\\) and \\(\\sigma\\)?\nSimply quap gives us the variance covariance. Therefore quap can be used to simulation the bivariate normal distribution of \\(\\mu\\) and \\(\\sigma\\)\n\nvcov(fit04_01quap)\n\n                mu        sigma\nmu    0.1697742465 0.0001111986\nsigma 0.0001111986 0.0849054742\n\n\nfrom which we can obtain the correlation matrix\n\ncov2cor(vcov(fit04_01quap)) |&gt;\n  round(digits = 3)\n\n         mu sigma\nmu    1.000 0.001\nsigma 0.001 1.000\n\n\nso to simulate using rethinking we simply use\n\npost04_01m &lt;- extract.samples(fit04_01quap, n = 1e4)\n\nwhich gives us a sample of size 10000 of the posterior distribution which can be summarized with the usual precis()\n\nprecis(post04_01m)\n\n            mean        sd       5.5%      94.5%    histogram\nmu    154.604070 0.4111663 153.945875 155.258115      ▁▁▅▇▂▁▁\nsigma   7.731244 0.2913491   7.266001   8.198884 ▁▁▁▂▅▇▇▃▁▁▁▁\n\n\n\n\n4.3.6.2 Using brm\nUsing brm however we are not given the variance covariance, it is only available for the intercept (first-level parameter)\n\nvcov(fit04_01brm)\n\n          Intercept\nIntercept 0.1750386\n\n\nSo you have to calculate the var-cov matrix by using a sample from the posterior distribution\n\npost04_01b &lt;- tidy_draws(fit04_01brm)\n# compute the cov\ncor(post04_01b[, c(\"b_Intercept\", \"sigma\")]) |&gt;\n  round(digits = 3)\n\n            b_Intercept sigma\nb_Intercept       1.000 0.001\nsigma             0.001 1.000\n\n\n\nSee comment from Kurz (2020) at end of section 4.3.6 to explain that McElreath uses mvnorm() from MASS to simulate using the varcov whereas with brms::tidy_draws() we do it directly.\n\nAlso Kurz (2020) has a nice discussion on how to create summary with histogram."
  },
  {
    "objectID": "ch04_linear.html#linear-predictions",
    "href": "ch04_linear.html#linear-predictions",
    "title": "4  Linear Models",
    "section": "4.4 Linear predictions",
    "text": "4.4 Linear predictions\n\n4.4.1 The linear model strategy\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\beta (x_i - \\bar{x}) \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta &\\sim \\mathcal{N}(0,10) \\\\\n\\sigma &\\sim \\mathcal{Uniform}(0, 50)\n\\end{align*}\n\\]\n\n4.4.1.1 Probability of the data\n\\[\nh_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\n\\]\n\n\n4.4.1.2 Linear model\n\\[\n\\mu_i = \\alpha + \\beta (x_i - \\bar{x})\n\\]\n\n\n4.4.1.3 Priors\n\\[\n\\begin{align*}\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta &\\sim \\mathcal{N}(0,10) \\\\\n\\sigma &\\sim \\mathcal{Uniform}(0, 50)\n\\end{align*}\n\\]\nThe goal is to simulate the heights from the model, using only the prior.\n\npriorHeights &lt;- list()\npriorHeights &lt;- within(priorHeights, {\n  n &lt;- 100L\n  set.seed(4)\n  sim &lt;- data.frame(\n    id = seq_len(n),\n    a = rnorm(n = n, mean = 178, sd = 20),\n    b = rnorm(n = n, mean = 0, sd = 10)) |&gt;\n    expand(nesting(id, a, b), weight = range(dataHowel_gte18$weight)) |&gt;\n    mutate(height = a + b * (weight - mean(dataHowel_gte18$weight)))\n})\n# glimpse(priorHeights$sim)\n\nand we plot if\n\nggplot(priorHeights$sim, aes(x = weight, y = height, group = id)) +\n  geom_line(alpha = 1/10) +\n  geom_hline(yintercept = c(0, 272), linetype = c(2, 1), size = 1/3) +\n  coord_cartesian(ylim = c(-100, 400)) +\n  labs(title = \"b ~ dnorm(0, 10)\")\n\n\n\n\n\n4.4.1.3.1 Adjusting the priors\nSince we know that the effect (\\(\\beta\\)) of the weight on height, i.e. the relation between the 2 should be positive and very large value unlikely we can use the log-normal as a prior on \\(beta\\).\nIn addition, sigma can also very often be better modeled with the exponential or HalfCauchy distribution. See section 9.5.3 in the text. We will use the exponential distribution for \\(\\sigma\\) in this work.\n\npriorHeights &lt;- within(priorHeights, {\n  lnorm &lt;- ggplot(data.frame(x = c(0, 5)), aes(x)) +\n    stat_function(geom = \"line\", fun = dlnorm, args = list(meanlog = 0, sdlog = 1), \n                color = \"slategray\", linewidth = 1.5) +\n    stat_function(geom = \"area\", fun = dlnorm, args = list(meanlog = 0, sdlog = 1), \n                fill = \"slategray1\") +\n    scale_y_continuous(labels = scales::label_percent()) +\n    labs(title = \"log-normal distribution\", x = expression(beta), y = \"density\")\n  exp &lt;- ggplot(data.frame(x = c(0, 5)), aes(x)) +\n    stat_function(geom = \"line\", fun = dexp, args = list(rate = 1), \n                color = \"seagreen\", linewidth = 1.5) +\n    stat_function(geom = \"area\", fun = dexp, args = list(rate = 1), \n                fill = \"seagreen1\") +\n    scale_y_continuous(labels = scales::label_percent()) +\n    labs(title = \"exponential distribution\", x = expression(sigma), y = \"density\")\n})\npriorHeights$lnorm + priorHeights$exp\n\n\n\n\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\beta (x_i - \\bar{x}) \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta &\\sim \\mathcal{LogNormal}(0,1) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\n\n\n\n\n4.4.2 Fitting the posterior distribution\nAs suggested by the discussion of prior just above, we use a log-normal prior for \\(\\beta\\)\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\beta (x_i - \\bar{x}) \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta &\\sim \\mathcal{LogNormal}(0,1) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\n\n4.4.2.1 Using quap\nWe add the centralized weight to the data\n\ndataHowel_gte18 &lt;- dataHowel_gte18 |&gt;\n  mutate(weight_c = as.numeric(scale(weight, center = TRUE, scale = FALSE)))\n\nthen get the fit using rethinking::quap\n\nGiving start values to quap seem to help it significantly and avoiding error, at least when using b ~ dlnorm(0, 1).\n\n\ntictoc::tic()\nfit04_03quap &lt;- xfun::cache_rds(\n    {rethinking::quap(\n      data = dataHowel_gte18,\n      flist = alist(\n      height ~ dnorm(mu, sigma),\n      mu &lt;- a + b * weight_c,\n      a ~ dnorm(178, 20),\n      b ~ dlnorm(0, 1),\n      sigma ~ dunif(0, 50)),\n    start = list(\n      a  = mean(dataHowel_gte18$height),\n      sigma = sd(dataHowel_gte18$height))\n    )},\n  file = \"ch04_fit04_03quap\")\ntictoc::toc()\n\n0 sec elapsed\n\n\n\nprecis(fit04_03quap)\n\n             mean         sd        5.5%       94.5%\na     154.6013679 0.27030764 154.1693641 155.0333718\nsigma   5.0718806 0.19115475   4.7663784   5.3773828\nb       0.9032807 0.04192363   0.8362786   0.9702828\n\n\n\n\n4.4.2.2 Using brm\nAgain, we use the exponential distribution as a prior of sigma to facilitate the iterations with brm. There are 2 equivalent ways to run this model. One uses the log-normal distribution of \\(\\beta\\), the other one uses the log transform of \\(\\beta\\) with the normal distribution. The two models are mathematically equivalent\n\n\n4.4.2.3 Using lognormal distribution\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\beta (x_i - \\bar{x}) \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta &\\sim \\mathcal{LogNormal}(0,1) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\nWhen using lognormal for a parameter of class b, you should specify lb and ub (lower bound and upper bound) to avoid error message and accelerate the computations with brm.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit04_03brm &lt;- xfun::cache_rds({\n  brms::brm(\n    data = dataHowel_gte18,\n    family = gaussian,\n    formula = height ~ 1 + weight_c,\n    prior = c(\n      prior(normal(178, 20), class = Intercept),\n      prior(lognormal(0, 1), class = b, lb = 0, ub = 3),\n      prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, cores = detectCores(), chains = detectCores(),\n    seed = 4)},\n  file = \"ch04_fit04_03brm\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.16 sec elapsed\n\n\n\nsummarize_draws(fit04_03brm) |&gt;\n  mutate(across(.cols= where(is.numeric), .fns = round, digits = 1))\n\n# A tibble: 5 × 10\n  variable      mean  median    sd   mad      q5     q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;        &lt;num&gt;   &lt;num&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 b_Interce…   155.    155.    0.3   0.3   154.    155       1    7975.    6010 \n2 b_weight_c     0.9     0.9   0     0       0.8     1       1    8445.    6252.\n3 sigma          5.1     5.1   0.2   0.2     4.8     5.4     1    8724.    6149.\n4 lprior       -10.3   -10.3   0.2   0.2   -10.7   -10       1    8914.    5907.\n5 lp__       -1082.  -1081.    1.2   1   -1084.  -1080.      1    4250     5606.\n\n\n\n\n\n4.4.3 Using the log tranformation\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\exp{(log\\_b)} (x_i - \\bar{x}) \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\nlog\\_b &\\sim \\mathcal{N}(0,1) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\n\nglimpse(dataHowel_gte18)\n\nRows: 352\nColumns: 5\n$ height   &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 1…\n$ weight   &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 3…\n$ age      &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 4…\n$ male     &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1…\n$ weight_c &lt;dbl&gt; 2.835121, -8.504679, -13.125648, 8.051429, -3.713614, 18.0021…\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit04_03brm_b &lt;- xfun::cache_rds({\n  brms::brm(\n  data = dataHowel_gte18,\n  family = gaussian,\n  formula = bf(height ~ a + exp(lb) * weight_c,\n               a ~ 1, lb ~ 1, nl = TRUE),\n  prior = c(\n    prior(normal(178, 20), class = b, nlpar = a),\n    prior(normal(0, 1), class = b, nlpar = lb),\n    prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = detectCores(), seed = 4)},\n  file = \"ch04_fit04_03brm_b\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.14 sec elapsed\n\n\n\nsummarize_draws(fit04_03brm_b) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n# A tibble: 5 × 10\n  variable           mean   median    sd   mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;             &lt;num&gt;    &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 b_a_Intercept    155.     155.    0.27  0.27   154.     155.       1    4611.\n2 b_lb_Intercept    -0.1     -0.1   0.05  0.05    -0.18    -0.03     1    4525.\n3 sigma              5.07     5.07  0.19  0.19     4.77     5.39     1    5388.\n4 lprior           -10.6    -10.6   0.19  0.19   -10.9    -10.3      1    5224.\n5 lp__           -1081.   -1081.    1.24  0.95 -1084.   -1080.       1    2134.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n\n\n\n4.4.4 Interpreting the posterior distribution\n\n4.4.4.1 Tables of marginal distributions\nUsing rethinking Important, the parameters are correlated here, to avoid this one must do centering of variables. The following uses centered variables.\n\nprecis(fit04_03quap, corr = TRUE)\n\n             mean         sd        5.5%       94.5%\na     154.6013679 0.27030764 154.1693641 155.0333718\nsigma   5.0718806 0.19115475   4.7663784   5.3773828\nb       0.9032807 0.04192363   0.8362786   0.9702828\n\n\n\nround(vcov(fit04_03quap), 3)\n\n          a sigma     b\na     0.073 0.000 0.000\nsigma 0.000 0.037 0.000\nb     0.000 0.000 0.002\n\n\nUsing brm\nNote: lp__ stands for unnormalized log posterior density.\n\n# normally we only use summarise_draws() but here we change it to match \n# the width of 0.89\ntidybayes::summarise_draws(fit04_03brm, mean, median, sd, mad, \n                           ~quantile2(.x, probs = c(0.055, 0.945)),\n                           default_convergence_measures(),\n                           default_mcse_measures()) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n# A tibble: 5 × 15\n  variable        mean   median    sd   mad     q5.5    q94.5  rhat ess_bulk\n  &lt;chr&gt;          &lt;num&gt;    &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 b_Intercept   155.     155.    0.27  0.27   154.     155.       1    7975.\n2 b_weight_c      0.9      0.9   0.04  0.04     0.84     0.97     1    8445.\n3 sigma           5.07     5.06  0.19  0.19     4.77     5.38     1    8724.\n4 lprior        -10.3    -10.3   0.19  0.19   -10.7    -10.0      1    8915.\n5 lp__        -1082.   -1081.    1.22  0.98 -1084.   -1080.       1    4250.\n# ℹ 6 more variables: ess_tail &lt;num&gt;, mcse_mean &lt;num&gt;, mcse_median &lt;num&gt;,\n#   mcse_sd &lt;num&gt;, mcse_q5 &lt;num&gt;, mcse_q95 &lt;num&gt;\n\n\nwe get the varcov matrix as follows\n\ntidy_draws(fit04_03brm) |&gt;\n  select(!matches(\"^[.]|__$\")) |&gt;\n  cov() |&gt;\n  round(digits = 3)\n\n            b_Intercept b_weight_c  sigma lprior\nb_Intercept       0.072      0.000  0.001  0.003\nb_weight_c        0.000      0.002  0.000 -0.002\nsigma             0.001      0.000  0.036 -0.036\nlprior            0.003     -0.002 -0.036  0.038\n\n\nand the correlation matrix\n\ntidy_draws(fit04_03brm) |&gt;\n  select(!matches(\"^[.]|__$\")) |&gt;\n  cor() |&gt;\n  round(digits = 3)\n\n            b_Intercept b_weight_c  sigma lprior\nb_Intercept       1.000     -0.007  0.024  0.058\nb_weight_c       -0.007      1.000 -0.026 -0.192\nsigma             0.024     -0.026  1.000 -0.973\nlprior            0.058     -0.192 -0.973  1.000\n\n\n\n\n4.4.4.2 Plotting posterior inference against data\nWith brms we use the ggmcmc package to illustrate the results from the markov chain\n\ntidybayes::get_variables(fit04_03brm)\n\n [1] \"b_Intercept\"   \"b_weight_c\"    \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\npost04_03brm &lt;- list()\npost04_03brm &lt;- within(post04_03brm, {\n  long &lt;- fit04_03brm |&gt;\n    tidybayes::gather_draws(b_Intercept, b_weight_c, sigma)\n})\n\nwith the histogram\n\nplot04_03brm &lt;- list()\nplot04_03brm$hist &lt;- ggplot(post04_03brm$long, aes(x = .value)) +\n  geom_histogram(aes(fill = .variable)) +\n  scale_fill_paletteer_d(palette = \"futurevisions::atomic_orange\") +\n  theme(legend.position = \"none\") +\n  facet_wrap(. ~ .variable, ncol = 1, scales = \"free\")\nplot04_03brm$hist\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nand density plots by chains\n\nplot04_03brm$dens &lt;- ggplot(post04_03brm$long, \n                            aes(x = .value, color = as.factor(.chain))) +\n  geom_density() +\n  scale_color_paletteer_d(palette = \"futurevisions::atomic_clock\") +\n  labs(x = NULL, color = \"chain\") +\n  facet_wrap(. ~ .variable, ncol = 1, scales = \"free\")\nplot04_03brm$dens\n\n\n\n\nand the paired plots with ggally\n\npost04_03brm &lt;- within(post04_03brm, {\n  wide &lt;- fit04_03brm |&gt;\n    tidybayes::spread_draws(b_Intercept, b_weight_c, sigma)\n})\n\n\nplot04_03brm &lt;- within(plot04_03brm, {\n  fun_diag &lt;- function(data, mapping, ...){\n    ggplot(data = data, mapping = mapping) +\n      geom_density(linewidth = 1)\n  }\n  fun_lower &lt;- function(data, mapping) {\n    ggplot(data = data, mapping = mapping) +\n      stat_density2d(linewidth = 1/3)\n  }\n  pairs &lt;- GGally::ggpairs(\n    data = post04_03brm$wide, \n    mapping = aes(color = as.factor(.chain)),\n    columns = c(\"b_Intercept\", \"b_weight_c\", \"sigma\"),\n    diag = list(continuous = fun_diag),\n    lower = list(continuous = fun_lower)) +\n    scale_color_paletteer_d(palette = \"futurevisions::atomic_clock\", direction = -1) +\n    # scale_fill_paletteer_d(palette = \"futurevisions::atomic_clock\") +\n    labs(title = \"Parameters Comparisons by Chain\")\n})\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nplot04_03brm$pairs\n\n\n\n\nand the correlation matrix\n\nplot04_03brm$corr &lt;- post04_03brm$wide |&gt;\n  select(b_Intercept, b_weight_c, sigma) |&gt;\n  GGally::ggcorr(color = \"darkgreen\",\n                 nbreaks = 10, label = TRUE, label_round = 2,\n                 label_color = \"midnightblue\") +\n  scale_fill_paletteer_d(palette = \"futurevisions::venus\") +\n  theme(legend.position = \"none\") +\n  labs(title = \"Correlations between parameters\")\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\nplot04_03brm$corr\n\n\n\n\nand for added extra, the trace plot\n\nplot04_03brm$trace &lt;- post04_03brm$long |&gt;\n  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line() +\n  scale_color_paletteer_d(palette = \"futurevisions::atomic_clock\", direction = 1) +\n  theme(legend.position = \"none\") +\n  labs(x = NULL, y = NULL) +\n  facet_wrap(. ~ .variable, ncol = 1, scales = \"free\")\nplot04_03brm$trace"
  },
  {
    "objectID": "ch04_linear.html#curves-from-lines",
    "href": "ch04_linear.html#curves-from-lines",
    "title": "4  Linear Models",
    "section": "4.5 Curves from lines",
    "text": "4.5 Curves from lines\n\n4.5.1 Polynomial regression\n\ndata(\"Howell1\")\ndataHowel &lt;- Howell1 |&gt;\n  # use as.vector() to keep the attribute\n  mutate(weight_s = scale(as.vector(weight)),\n         weight_s2 = weight_s ^ 2)\nrm(Howell1)\n\n\nplotHowel &lt;- list()\nplotHowel &lt;- within(plotHowel, {\n  colr &lt;- unclass(paletteer::paletteer_d(\"futurevisions::titan\"))\n  basic &lt;- dataHowel |&gt;\n    (\\(.) {\n      ggplot(., aes(x = weight_s, y = height, color = age)) +\n        scale_x_continuous(breaks = scales::breaks_extended(n=7),\n                     labels = function(x) {\n                       x &lt;- x * sd(.$weight) + mean(.$weight)\n                       label_number(accuracy = 1)(x)\n                     }) +\n        scale_color_gradientn(colors = colr) +\n        geom_point(shape = 20, size = 2, alpha = 2/3) +\n        theme(legend.position = c(0.1, 0.8)) +\n        labs(title = \"Census data for the Dobe area !Kung San\",\n       subtitle = sprintf(\"%d individuals\", nrow(.)))\n      })()\n})\nplotHowel$basic\n\n\n\n\nand the model used is\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\beta_1 \\cdot weight\\_s_i + \\beta_2 \\cdot weight\\_s^2_i \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta_1 &\\sim \\mathcal{LogNormal}(0,1) \\\\\n\\beta_2 &\\sim \\mathcal{N}(0,1) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\nhttps://discourse.mc-stan.org/t/error-with-gamma-prior/16420\n\n\n\n\n\n\nWarning\n\n\n\nThe following code gives a warning about setting lower boundaries. It started to show with R 4.2. Paul Buerkner advises to ignore it. See advice\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit04_05brm &lt;- xfun::cache_rds({\n  brm(data = dataHowel,\n      family = gaussian,\n      height ~ 1 + weight_s + weight_s2,\n      prior = c(prior(normal(178, 20), class = Intercept),\n                prior(lognormal(0, 1), class = b, coef = \"weight_s\"),\n                prior(normal(0, 1), class = b, coef = \"weight_s2\"),\n                prior(exponential(1), class = sigma)),\n      iter = 4000, warmup = 2000, chains = 4, cores = detectCores(),\n      seed = 4)},\n  file = \"ch04_fit04_05brm\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.18 sec elapsed\n\n\n\nsummarize_draws(fit04_05brm) |&gt;\n  filter(!grepl(pattern = \"__$\", x = variable)) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n# A tibble: 5 × 10\n  variable      mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;        &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 b_Intercept 146.   146.    0.37  0.37 145.   147.       1    6914.    5892.\n2 b_weight_s   21.7   21.7   0.29  0.29  21.2   22.2      1    6999.    5915.\n3 b_weight_s2  -7.8   -7.8   0.27  0.28  -8.25  -7.35     1    6755.    6110.\n4 sigma         5.78   5.77  0.18  0.18   5.49   6.08     1    7514.    5821.\n5 lprior      -51.8  -51.7   2.1   2.11 -55.2  -48.4      1    6861.    6303.\n\n\nand to obtain a simplified dataframe we use\n\nbrms::fixef(fit04_05brm) |&gt;\n  round(digits = 2)\n\n          Estimate Est.Error   Q2.5  Q97.5\nIntercept   146.05      0.37 145.33 146.78\nweight_s     21.73      0.29  21.16  22.31\nweight_s2    -7.80      0.27  -8.33  -7.26\n\n\n\ntidybayes::get_variables(fit04_05brm)\n\n [1] \"b_Intercept\"   \"b_weight_s\"    \"b_weight_s2\"   \"sigma\"        \n [5] \"lprior\"        \"lp__\"          \"accept_stat__\" \"stepsize__\"   \n [9] \"treedepth__\"   \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\npost04_05brm &lt;- xfun::cache_rds({\n  fit04_05brm |&gt; \n    tidybayes::gather_draws(b_Intercept, b_weight_s, b_weight_s, sigma)},\n  file = \"ch04_post04_05brm\")\n\n\nplot04_05brm &lt;- list()\nplot04_05brm$dens &lt;- \n  post04_05brm |&gt;\n  ggplot(aes(x = .value, color = as.factor(.chain))) +\n  geom_density() +\n  scale_color_paletteer_d(palette = \"futurevisions::mars\", direction = -1) +\n  theme(legend.position = \"none\") +\n  labs(x = NULL, color = \"chain\") +\n  facet_wrap(. ~ .variable, ncol = 1, scales = \"free\")\n\nand\n\nplot04_05brm$trace &lt;- \n  post04_05brm |&gt;\n  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line() +\n  scale_color_paletteer_d(palette = \"futurevisions::mars\", direction = -1) +\n  theme(legend.position = \"none\") +\n  labs(x = NULL, y = NULL) +\n  facet_wrap(. ~ .variable, ncol = 1, scales = \"free\")\n\n\nplot04_05brm$dens + plot04_05brm$trace\n\n\n\n\nAnd we look at the fitted and predicted values to understand and interpret the result.\nWhat is the difference between fitted and predict? fitted A nice explanation is given by Greg Snow\n\nThe fitted function returns the y-hat values associated with the data used to fit the model. The predict function returns predictions for a new set of predictor variables. If you don’t specify a new set of predictor variables then it will use the original data by default giving the same results as fitted for some models (especially the linear ones), but if you want to predict for a new set of values then you need predict. The predict function often also has options for which type of prediction to return, the linear predictor, the prediction transformed to the response scale, the most likely category, the contribution of each term in the model, etc.\n\nTherefore, if we give the same data to fitted or predict will will obtain sensibly the same results, the difference being caused by the random seed. However, in Bayesian stats, fitted will only provide \\(\\mu_i\\) and its variation whereas predict will give \\(h_i\\) which is \\(h_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\\)\nWe can see it clearly here as fitd_quad gives ans estimate about the same as for predict since they both report the same \\mu_i, but predict has a wider interval since it uses \\(\\sigma\\)\n\npred04_05brm &lt;- list()\npred04_05brm &lt;- data.frame(\n  weight_s = seq_range(dataHowel$weight_s, n = 30L)) |&gt;\n  mutate(weight_s2 = weight_s^2) |&gt;\n  add_predicted_draws(fit04_05brm, ndraws = 500) |&gt;\n  median_qi()\n\nlpred04_05brm &lt;- data.frame(\n  weight_s = seq_range(dataHowel$weight_s, n = 30L)) |&gt;\n  mutate(weight_s2 = weight_s^2) |&gt;\n  add_linpred_draws(fit04_05brm, ndraws = 500) |&gt;\n  median_qi()\n\nand we can now create the plot.\n\nplot04_05brm$model &lt;- plotHowel$basic +\n  geom_ribbon(data = pred04_05brm,\n              aes(x = weight_s, ymin = .lower, ymax = .upper),\n              inherit.aes = FALSE, fill = \"lightcyan\", alpha = 1) +\n  geom_smooth(data = lpred04_05brm,\n              aes(x=weight_s, y = .linpred, ymin = .lower, ymax = .upper),\n              inherit.aes = FALSE, stat = \"identity\",\n              fill = \"lightcyan3\", color = \"royalblue\", alpha = 1, size = 1/2) +\n  geom_point(shape = 20, size = 2, alpha = 2/3)\nplot04_05brm$model\n\n\n\n\n\n\n4.5.2 Splines\n\ndata(\"cherry_blossoms\")\ndataCherry &lt;- cherry_blossoms\nrm(cherry_blossoms)\ndataCherry |&gt; skimr::skim()\n\n\nData summary\n\n\nName\ndataCherry\n\n\nNumber of rows\n1215\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n1408.00\n350.88\n801.00\n1104.50\n1408.00\n1711.50\n2015.00\n▇▇▇▇▇\n\n\ndoy\n388\n0.68\n104.54\n6.41\n86.00\n100.00\n105.00\n109.00\n124.00\n▁▅▇▅▁\n\n\ntemp\n91\n0.93\n6.14\n0.66\n4.67\n5.70\n6.10\n6.53\n8.30\n▃▇▇▂▁\n\n\ntemp_upper\n91\n0.93\n7.19\n0.99\n5.45\n6.48\n7.04\n7.72\n12.10\n▇▇▂▁▁\n\n\ntemp_lower\n91\n0.93\n5.10\n0.85\n0.75\n4.61\n5.14\n5.54\n7.74\n▁▁▆▇▁\n\n\n\n\n# data without NA\ndataCherry_nona &lt;- dataCherry |&gt;\n  drop_na(doy)\n\n\n4.5.2.1 Knots, degree and basis functions\nThe knots used here are based on quantiles, other ways are possible,\n\nknots &lt;- quantile(dataCherry_nona$year, probs = seq(from = 0, to = 1, length.out = 15))\nknots\n\n       0% 7.142857% 14.28571% 21.42857% 28.57143% 35.71429% 42.85714%       50% \n      812      1036      1174      1269      1377      1454      1518      1583 \n57.14286% 64.28571% 71.42857% 78.57143% 85.71429% 92.85714%      100% \n     1650      1714      1774      1833      1893      1956      2015 \n\n\n\ncolr &lt;- unclass(paletteer::paletteer_d(\"futurevisions::cancri\"))\nggplot(dataCherry_nona, aes(x = year, y = doy, color = temp)) +\n  geom_vline(xintercept = knots, color = \"slateblue\", alpha = 1/2) +\n  geom_point(shape = 20, size = 2, alpha = 2/3) +\n  scale_x_continuous(breaks = knots, labels = scales::label_number(big.mark = \"\")) +\n  scale_color_gradientn(colors = colr) +\n  theme(legend.position = c(0.05, 0.8),\n        axis.text.x = element_text(size = rel(0.9))) +\n  labs(title = \"Cherry Blossom in Japan\",\n       subtitle = sprintf(\"%d observations with %d knots\", nrow(dataCherry_nona), length(knots)))\n\n\n\n\nthe code knots[-c(1, nknots)] is required because bs places knots at the boundaries by default, so we have to remove them.\n\nB &lt;- splines::bs(x = dataCherry_nona$year, \n                 knots = knots[-c(1, length(knots))], \n                 degree = 3, intercept = TRUE)\n# str(B)\n\nand we plot the basis functions\n\n# this data.frame will be reused below with the posteriors\ndf_bias &lt;- B |&gt;\n  as.data.frame() %&gt;%\n  setNames(sprintf(\"B%02d\", seq_len(ncol(.)))) |&gt;\n  mutate(year = dataCherry_nona$year) |&gt;\n  pivot_longer(cols = -year, names_to = \"bias_func\", values_to = \"bias\")\n# str(df_bias)\n\nclrs &lt;- paletteer::paletteer_c(\"pals::jet\", n = length(unique(df_bias$bias_func)))\nggplot(df_bias, aes(x = year, y = bias, color = bias_func)) +\n  geom_vline(xintercept = knots, color = \"grey60\", linetype = \"longdash\", alpha = 1/2) +\n  geom_line() +\n  scale_x_continuous(breaks = knots, labels = scales::label_number(big.mark = \"\")) +\n  scale_color_manual(values = clrs) +\n  theme(legend.position = \"none\") +\n  labs(\"The bias functions\")\n\n\n\n\n\n\n4.5.2.2 Model and fit\n\\[\n\\begin{align*}\ndoy_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\,u_i &= \\alpha + \\sum_{k=1}^Kw_kB_{k, i} \\\\\n\\alpha &\\sim \\mathcal{N}(100, 10) \\\\\nw_j &\\sim \\mathcal{N}(0, 10) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nWe first append the matrix to the data in one column. See Kurz (2020) on this data structure.\n\ndataCherry_nonaB &lt;- dataCherry_nona |&gt;\n  mutate(B = B)\n# the last column is a matrix column, with same nb of rows as the other\n# columns but with a column including 17 subcolumns (!)\n\nand the fit\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit04_08brm &lt;- xfun::cache_rds({\n  brm(data = dataCherry_nonaB,\n      family = gaussian,\n      doy ~ 1 + B,\n      prior = c(prior(normal(100, 10), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(exponential(1), class = sigma)),\n      cores = detectCores(), seed = 4)},\n  file = \"ch04_fit04_08brm\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.21 sec elapsed\n\n\n\nsummarize_draws(fit04_08brm) |&gt;\n  filter(!grepl(pattern = \"__$\", x = variable)) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 1))\n\n# A tibble: 20 × 10\n   variable     mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n   &lt;chr&gt;       &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n 1 b_Intercept 104.   104.    2.4   2.4  99.3 108.      1     657.    1083.\n 2 b_B1         -3.1   -3.1   3.9   3.9  -9.5   3.2     1    1472     2103.\n 3 b_B2         -1     -1.1   3.9   4    -7.6   5.4     1    1512.    2289.\n 4 b_B3         -1.2   -1.2   3.7   3.6  -7.2   5       1    1122.    2198.\n 5 b_B4          4.7    4.7   2.9   3    -0.2   9.4     1     950.    1800.\n 6 b_B5         -1     -1     3     3    -5.8   4       1     860     1448.\n 7 b_B6          4.2    4.1   3     2.9  -0.7   9       1     946.    1843.\n 8 b_B7         -5.5   -5.5   2.9   2.8 -10.1  -0.7     1     844.    1264.\n 9 b_B8          7.7    7.6   2.9   2.9   3    12.5     1     864.    1804.\n10 b_B9         -1.2   -1.2   2.9   2.9  -5.9   3.7     1     978.    1688 \n11 b_B10         2.8    2.8   3     3    -1.9   7.7     1     890.    1897.\n12 b_B11         4.5    4.5   2.9   2.9  -0.4   9.3     1     906.    1686.\n13 b_B12        -0.3   -0.3   2.9   3    -5.1   4.6     1     910.    1429.\n14 b_B13         5.4    5.4   2.9   2.9   0.5  10.2     1     883.    1521 \n15 b_B14         0.6    0.6   3     3.1  -4.5   5.5     1     938.    1691.\n16 b_B15        -1     -1     3.3   3.4  -6.3   4.4     1    1079.    1880.\n17 b_B16        -7.1   -7.1   3.4   3.4 -12.9  -1.4     1    1124.    2086.\n18 b_B17        -7.8   -7.8   3.3   3.3 -13.2  -2.5     1    1151     1906.\n19 sigma         5.9    5.9   0.1   0.1   5.7   6.2     1    4602.    2860.\n20 lprior      -66.5  -66.3   0.8   0.6 -68   -65.5     1    1472.    1510.\n\n\n\n\n4.5.2.3 Plot\n\nget_variables(fit04_08brm)\n\n [1] \"b_Intercept\"   \"b_B1\"          \"b_B2\"          \"b_B3\"         \n [5] \"b_B4\"          \"b_B5\"          \"b_B6\"          \"b_B7\"         \n [9] \"b_B8\"          \"b_B9\"          \"b_B10\"         \"b_B11\"        \n[13] \"b_B12\"         \"b_B13\"         \"b_B14\"         \"b_B15\"        \n[17] \"b_B16\"         \"b_B17\"         \"sigma\"         \"lprior\"       \n[21] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n[25] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\n# Source: https://github.com/mjskay/tisdybayes/issues/38\npost04_08brm &lt;- tidybayes::gather_draws(fit04_08brm, !!sym(\"^b_B.+\"), regex = TRUE) |&gt;\n  mutate(.variable = as.integer(sub(\"^b_B\", replacement = \"\", x = .variable)),\n         .variable = sprintf(\"B%02d\", .variable)) |&gt;\n  rename(\"bias_func\" = .variable) |&gt;\n  group_by(bias_func) |&gt;\n  summarise(weight = mean(.value)) |&gt;\n  full_join(y = df_bias, by = \"bias_func\")\n# glimpse(df)\n\nplot04_08brm &lt;- list()\nplot04_08brm &lt;- within(plot04_08brm, {\n  clrs &lt;- paletteer::paletteer_c(\"pals::jet\", n = length(unique(post04_08brm$bias_func)))\n  bias &lt;- ggplot(post04_08brm, aes(x = year, y = bias * weight, color = bias_func)) +\n    geom_vline(xintercept = knots, color = \"grey60\", linetype = \"longdash\", alpha = 1/2) +\n    geom_line(size = 1) +\n    scale_x_continuous(breaks = knots, labels = scales::label_number(big.mark = \"\")) +\n    scale_color_manual(values = clrs) +\n    theme(legend.position = \"none\") +\n    labs(title = \"fitted bias functions\")\n})\nplot04_08brm$bias\n\n\n\n\nthe fitted values\n\nlpred04_08brm &lt;- dataCherry_nonaB |&gt; \n    add_linpred_draws(fit04_08brm) |&gt;\n    ungroup() |&gt;\n    select(-B) |&gt;\n    group_by(year) |&gt;\n    mean_qi(.linpred)\n\nand the plot\n\nplot04_08brm &lt;- within(plot04_08brm, {\n  clrs &lt;- unclass(paletteer::paletteer_d(\"futurevisions::cancri\"))\n  p &lt;- ggplot(lpred04_08brm, aes(x = year, y = .linpred)) +\n    geom_vline(xintercept = knots[-c(1, length(knots))], color = \"slateblue\", alpha = 1/2) +\n    geom_point(dataCherry_nonaB, mapping = aes(x = year, y = doy, color = temp),\n             inherit.aes = FALSE) +\n    geom_lineribbon(aes(x = year, y = .linpred, ymin = .lower, ymax = .upper),\n                  color = \"blueviolet\", fill = \"cornflowerblue\", alpha = 1/2) +\n    scale_x_continuous(breaks = knots, labels = scales::label_number(big.mark = \"\")) +\n    scale_color_gradientn(colors = clrs) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Figure 4.12\",x = \"year\", y = \"doy\")\n})\nplot04_08brm$p\n\n\n\n\n\n\n\n4.5.3 Smooth functions for a smooth world\nSee Kurz (2020) for much more details on this topic."
  },
  {
    "objectID": "ch04_linear.html#summary",
    "href": "ch04_linear.html#summary",
    "title": "4  Linear Models",
    "section": "4.6 Summary",
    "text": "4.6 Summary\nThis was an important chapter. Most of the plots and basic coding tools are exemplified here. It is an important reference chapter. The brms package will be exclusively used from now on.\n\n\n\n\nKurz, Solomon. 2020. Statistical Rethinking with Brms. 2nd ed. https://bookdown.org/content/4857/."
  },
  {
    "objectID": "ch05_multivariate.html#spurious-association",
    "href": "ch05_multivariate.html#spurious-association",
    "title": "5  Multivariate Linear Models",
    "section": "5.1 Spurious association",
    "text": "5.1 Spurious association\nGet the data and standardize the variables\n\ndata(\"WaffleDivorce\")\ndataWaffle &lt;- WaffleDivorce |&gt;\n  # standardize the variables\n  mutate(A = scale(as.vector(MedianAgeMarriage)),\n         M = scale(as.vector(Marriage)),\n         D = scale(as.vector(Divorce)))\ndataWaffle |&gt;\n  select(MedianAgeMarriage, Marriage, Divorce, A, M, D, South) |&gt;\n  skim() |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 1))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = where(is.numeric), .fns = round, digits = 1)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\nData summary\n\n\nName\nselect(…)\n\n\nNumber of rows\n50\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nMedianAgeMarriage\n0\n1\n26.1\n1.2\n23.2\n25.3\n25.9\n26.8\n29.7\n▂▇▇▂▁\n\n\nMarriage\n0\n1\n20.1\n3.8\n13.5\n17.1\n19.7\n22.1\n30.7\n▅▇▆▂▁\n\n\nDivorce\n0\n1\n9.7\n1.8\n6.1\n8.3\n9.8\n10.9\n13.5\n▂▇▇▅▃\n\n\nA\n0\n1\n0.0\n1.0\n-2.3\n-0.6\n-0.1\n0.6\n2.9\n▂▆▇▂▁\n\n\nM\n0\n1\n0.0\n1.0\n-1.7\n-0.8\n-0.1\n0.5\n2.8\n▅▇▆▂▁\n\n\nD\n0\n1\n0.0\n1.0\n-2.0\n-0.8\n0.0\n0.7\n2.1\n▂▇▇▅▃\n\n\nSouth\n0\n1\n0.3\n0.5\n0.0\n0.0\n0.0\n1.0\n1.0\n▇▁▁▁▃\n\n\n\n\n\nand plot the data\n\nplot_waffles &lt;- function(data, x_var = \"WaffleHouses\", y_var = \"Divorce\",\n                         color_var = \"South\",\n                         titles = list(title = \"Waffle Houses\",\n                                     x = \"Waffle Houses per million\",\n                                     y = \"Divorce rate\",\n                                     color = \"South\")) {\n  ggplot(data, aes(x = .data[[x_var]], y = .data[[y_var]])) +\n    geom_point(aes(color = as.logical(.data[[color_var]]))) +\n    scale_color_paletteer_d(\"fishualize::Bodianus_rufus\", direction = -1) + \n    labs(title = titles$title, x = titles$x, y = titles$y, color = titles$color)\n}\n\n\nplotWaffle &lt;- lapply(X = c(\"MedianAgeMarriage\", \"Marriage\", \"WaffleHouses\"),\n            FUN = function(x) {\n              plot_waffles(dataWaffle, x_var = x) +\n                stat_smooth(method = \"lm\", fullrange = TRUE,\n                            fill = \"darkolivegreen\", color = \"darkgreen\") +\n                labs(x = x)\n            })\nwrap_plots(plotWaffle, guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nGGally::ggscatmat(dataWaffle, columns = c(\"A\", \"M\", \"D\"))\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nWarning: The dot-dot notation (`..scaled..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(scaled)` instead.\nℹ The deprecated feature was likely used in the GGally package.\n  Please report the issue at &lt;https://github.com/ggobi/ggally/issues&gt;.\n\n\n\n\n\nThe model for regressing the divorce rate \\(D\\) on the median age \\(A\\) is\n\\[\n\\begin{align*}\nD_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_A \\cdot A_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_A &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand the fit, take note of the argument sample_prior = TRUE which is important for the rest of the exercise\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit05_01 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataWaffle,\n    formula = D ~ 1 + A,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 0.2), class = Intercept),\n      prior(normal(0, 0.5), class = b),\n      prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4,\n    sample_prior = TRUE,\n    core = detectCores(), seed = 5)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  },\n  file = \"ch05_fit05_01\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.11 sec elapsed\n\n\nand we can investigate the priors as follows\n\nprior05_01 &lt;- brms::prior_draws(fit05_01)\n\nget the fitted \\(\\mu_i\\) by using a sequence of median age marriage of length 30 from the min(MedianAgeMarriage) to max(MedianAgeMarriage).\n\nlpred05_01 &lt;- data.frame(\n    A = seq_range(dataWaffle$A, n = 30, pretty = TRUE)) |&gt;\n    add_linpred_draws(fit05_01, ndraws = 500) |&gt;\n    mean_qi(.width = 0.89)\n\nand plot them\n\nplot05_01 &lt;- list()\nplot05_01 &lt;- within(plot05_01, {\n  age &lt;- ggplot(data = dataWaffle, mapping = aes(x = A, y = D)) +\n  geom_smooth(data = lpred05_01,\n              aes(y = .linpred, ymin = .lower, ymax = .upper),\n              stat = \"identity\",\n              fill = \"olivedrab1\", color = \"olivedrab4\", alpha = 1, size = 1/2) +\n  geom_point(aes(color = as.logical(South))) +\n  scale_x_continuous(breaks = scales::breaks_extended(n = 7),\n                     labels = function(x) {\n                       center &lt;- mean(dataWaffle$MedianAgeMarriage)\n                       scale &lt;- sd(dataWaffle$MedianAgeMarriage)\n                       scales::label_number(accuracy = 0.1)(x * scale + center)\n                     }) +\n  scale_y_continuous(breaks = scales::breaks_extended(n = 7),\n                     labels = function(x) {\n                       center &lt;- mean(dataWaffle$Divorce) \n                       scale &lt;- sd(dataWaffle$Divorce)\n                       scales::label_number(accuracy = 0.1)(x * scale + center)\n                     }) +\n  scale_color_paletteer_d(\"calecopal::kelp1\", direction = -1) +\n  theme(legend.position = c(0.85, 0.85),\n        title = element_text(color = \"midnightblue\")) +\n  labs(title = \"Divorce rate vs Median Marriage age\",\n       color = \"South\",\n       x = \"Median age of marriage\", y = \"Divorce rate\")\n})\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# plot05_01\n\nThe model for regressing the divorce rate \\(D\\) on the marriage rate \\(M\\) is\n\\[\n\\begin{align*}\nD_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_M \\cdot M_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_A &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit05_02 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataWaffle,\n    formula = D ~ 1 + M,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 0.2), class = Intercept),\n      prior(normal(0, 0.5), class = b),\n      prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4,\n    sample_prior = TRUE,\n    core = detectCores(), seed = 5)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  },\n  file = \"ch05_fit05_02\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.14 sec elapsed\n\n\n\nposterior_summary(fit05_02)\n\n                     Estimate  Est.Error         Q2.5       Q97.5\nb_Intercept      1.418806e-03 0.11272147  -0.22161720   0.2202850\nb_M              3.504744e-01 0.13212237   0.07844829   0.6042600\nsigma            9.479699e-01 0.09600916   0.77926104   1.1520339\nprior_Intercept  8.586518e-04 0.20068513  -0.38689646   0.3962355\nprior_b          1.248018e-02 0.50641854  -0.95523318   1.0106563\nprior_sigma      9.985915e-01 1.00077523   0.02319797   3.7120347\nlprior          -9.226424e-01 0.31150694  -1.67568849  -0.4986527\nlp__            -6.901415e+01 1.24450230 -72.22582345 -67.5839137\n\n\n\nlpred05_02 &lt;- list()\nlpred05_02 &lt;- data.frame(\n  M = seq_range(dataWaffle$M, n = 30, pretty = TRUE)) |&gt;\n  add_linpred_draws(fit05_02, ndraws = 500) |&gt;\n  mean_qi(.width = 0.89)\n\nand plot them\n\nplot05_02 &lt;- list()\nplot05_02 &lt;- within(plot05_02, {\n  marriage &lt;- ggplot(data = dataWaffle, mapping = aes(x = M, y = D)) +\n  geom_smooth(data = lpred05_02,\n              aes(y = .linpred, ymin = .lower, ymax = .upper),\n              stat = \"identity\",\n              fill = \"springgreen1\", color = \"springgreen4\", alpha = 1, size = 1/2) +\n  geom_point(aes(color = as.logical(South))) +\n  scale_x_continuous(breaks = scales::breaks_extended(n = 7),\n                     labels = function(x) {\n                       center &lt;- mean(dataWaffle$Marriage)\n                       scale &lt;- sd(dataWaffle$Marriage)\n                       scales::label_number(accuracy = 0.1)(x * scale + center)\n                     }) +\n  scale_y_continuous(breaks = scales::breaks_extended(n = 7),\n                     labels = function(x) {\n                       center &lt;- mean(dataWaffle$Divorce) \n                       scale &lt;- sd(dataWaffle$Divorce)\n                       scales::label_number(accuracy = 0.1)(x * scale + center)\n                     }) +\n  scale_color_paletteer_d(\"calecopal::kelp1\", direction = -1) +\n  theme(legend.position = c(0.85, 0.85),\n        title = element_text(color = \"midnightblue\")) +\n  labs(title = \"Divorce rate vs Marriage rate\",\n       color = \"South\",\n       x = \"Marriage rate\", y = \"Divorce rate\")\n})\n# plot05_02$marriage\n\n\nplot05_01$age + plot05_02$marriage +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n5.1.1 Think before you regress\n\ndagWaffle &lt;- list()\ndagWaffle &lt;- within(dagWaffle, {\n  coord &lt;- data.frame(\n    name = c(\"A\", \"D\", \"M\"),\n    x = c(1, 2, 3),\n    y = c(2, 1, 2))\n  \n  dag1 &lt;- ggdag::dagify(M ~ A,\n                      D ~ A + M,\n                      coords = coord)\n  \n  plot1 &lt;- ggplot(dag1, aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(aes(color = name), size = 10) +\n    geom_dag_text(color = \"midnightblue\") +\n    geom_dag_edges(edge_color = \"midnightblue\") +\n    scale_color_paletteer_d(\"calecopal::kelp1\") +\n    scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n    scale_y_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n    ggthemes::theme_solid(fill = \"snow2\") +\n    theme(legend.position = \"none\")\n\n  dag2 &lt;- ggdag::dagify(M ~ A,\n                      D ~ A,\n                      coords = coord)\n  \n  plot2 &lt;- ggplot(dag2, aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(aes(color = name), size = 10) +\n    geom_dag_text(color = \"midnightblue\") +\n    geom_dag_edges(edge_color = \"midnightblue\") +\n    scale_color_paletteer_d(\"calecopal::kelp1\") +\n    scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n    scale_y_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n    ggthemes::theme_solid(fill = \"snow2\") +\n    theme(legend.position = \"none\")})\n\n\nwith(dagWaffle, {plot1 | plot2}) +\n  plot_annotation(\n    title = \"Testable Implications of the Waffle Data\"\n  )\n\n\n\n\n\n\n5.1.2 Testable implications\n\ndag &lt;- \"dag{ D &lt;- A -&gt; M }\"\ndagitty::dagitty(dag) |&gt;\n  dagitty::impliedConditionalIndependencies()\n\nD _||_ M | A\n\n\n\ndag &lt;- \"dag{ D &lt;- A -&gt; M -&gt; D }\"\ndagitty::dagitty(dag) |&gt;\n  dagitty::impliedConditionalIndependencies()\nmessage(\"This returns NULL because there are no conditional independencies\")\n\nThis returns NULL because there are no conditional independencies\n\n\n\n\n5.1.3 Multiple regression notation\nThe model with median age and marriage rate, both standardized.\n\nThe \\(+\\) in the model can be interpreted as the divorce rate is a function of the marriage rate OR the median age of marriage\n\n\\[\n\\begin{align*}\nD_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_M \\cdot M_i + \\beta_A \\cdot A_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\beta_2 &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(0, 1)\n\\end{align*}\n\\]\n\n\n5.1.4 Approximating the posterior\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit05_03 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataWaffle,\n    formula = D ~ 1 + M + A,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 0.2), class = Intercept),\n      prior(normal(0, 0.5), class = b, coef = \"M\"),\n      prior(normal(0, 0.5), class = b, coef = \"A\"),\n      prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4,\n    core = detectCores(), seed = 5)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  },\n  file = \"ch05_fit05_03\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.15 sec elapsed\n\n\n\nposterior_summary(fit05_03)\n\n                 Estimate  Est.Error        Q2.5       Q97.5\nb_Intercept  -0.002879751 0.10047152  -0.2003863   0.1896808\nb_M          -0.061176350 0.16111769  -0.3720397   0.2543411\nb_A          -0.606779058 0.16149095  -0.9255685  -0.2939154\nsigma         0.827157443 0.08754153   0.6811746   1.0243357\nlprior       -1.562391660 0.48423505  -2.6847351  -0.8670715\nlp__        -62.895506465 1.48908896 -66.7400580 -61.0721726\n\n\n\ntidybayes::get_variables(fit05_03)\n\n [1] \"b_Intercept\"   \"b_M\"           \"b_A\"           \"sigma\"        \n [5] \"lprior\"        \"lp__\"          \"accept_stat__\" \"stepsize__\"   \n [9] \"treedepth__\"   \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\nfit05_03 |&gt;\n  gather_draws(b_Intercept, b_M, b_A, ndraws = 500) |&gt;\n  ggplot(aes(x = .value, y = .variable)) +\n  stat_pointinterval(point_interval = mean_qi,\n                     .width = 0.89,\n                     fatten_point = 3,\n                     size = 10,\n                     color = \"tan4\") +\n  labs(title = deparse1(fit05_03$formula$formula),\n       x = NULL, y = NULL)\n\n\n\n\n\n\n5.1.5 Plotting multivariate posteriors\n3 main plots are used\n\nPredictor residual plots\nPosterior prediction plots\nCounterfactual plots\n\n\n5.1.5.1 Predictor residual plots\n\n5.1.5.1.1 Marriage rate residuals\nWe compute marriage rate in terms of median age of marriage which is the model\n\\[\n\\begin{align*}\nM_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot A_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nfit this model\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit05_04m &lt;- xfun::cache_rds({brm(\n  data = dataWaffle,\n  formula = M ~ 1 + A,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 100, chains = 4, core = detectCores(),\n  seed = 5)\n  },\n  file = \"ch05_fit05_04m\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.17 sec elapsed\n\n\n\nsummary(fit05_04m)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: M ~ 1 + A \n   Data: dataWaffle (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 100; thin = 1;\n         total post-warmup draws = 7600\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.09    -0.18     0.18 1.00     9909     5930\nA            -0.69      0.10    -0.89    -0.50 1.00     7018     5473\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.71      0.07     0.58     0.87 1.00     6719     5532\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nlpred05_04m &lt;- dataWaffle |&gt;\n  select(Loc, M, A) |&gt;\n  add_linpred_draws(fit05_04m, ndraws = 500) |&gt;\n  mean_qi()\n\nplot05_04 &lt;- list()\nplot05_04 &lt;- within(plot05_04, {\n  marriage &lt;- lpred05_04m |&gt;\n    ggplot(aes(x = A, y = M)) +\n    geom_point() +\n    geom_segment(aes(xend = A, yend = .linpred), size = 0.5, color = \"green\") +\n    geom_line(aes(x = A, y = .linpred), color = \"purple\") +\n    geom_text(aes(label = Loc), size = 3, color = \"darkblue\") +\n    labs(x = \"Median age of marriage (standardized)\",\n         y = \"Marriage rate (standardized)\")\n})\n\n\n\n5.1.5.1.2 Age of Marriage residuals\nWe compute age of marriage in terms of rate of marriage which is the model\n\\[\n\\begin{align*}\nA_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot M_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nfit this model\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit05_04a &lt;- xfun::cache_rds({brm(\n  data = dataWaffle,\n  formula = A ~ 1 + M,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 100, chains = 4, core = detectCores(),\n  seed = 5)\n  },\n  file = \"ch05_fit05_04a\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.19 sec elapsed\n\n\n\nlpred05_04a &lt;- dataWaffle |&gt;\n  select(Loc, M, A) |&gt;\n  add_linpred_draws(fit05_04a, ndraws = 500) |&gt;\n  mean_qi()\n\nplot05_04 &lt;- within(plot05_04, {\n  age &lt;- lpred05_04a |&gt;\n    ggplot(aes(x = M, y = A)) +\n    geom_point() +\n    geom_segment(aes(xend = M, yend = .linpred), size = 0.5, color = \"orchid\") +\n    geom_line(aes(x = M, y = .linpred), color = \"darkgoldenrod\") +\n    geom_text(aes(label = Loc), size = 3, color = \"darkblue\") +\n    labs(x = \"Marriage rate (standardized)\",\n         y = \"Median age of marriage (standardized)\")\n  })\n\nwhich gives us the 2 top plots of figure 5.4\n\nwrap_plots(plot05_04[c(\"marriage\", \"age\")])\n\n\n\n\n\n\n5.1.5.1.3 Using residuals as predictors\nNow we use the residuals of the marriage rate to represent marriage free of any influence by the median age of marriage. We use the residual_draws() function to extract the residuals.\n\nres05_04m &lt;- dataWaffle |&gt;\n  select(Loc, D, M, A) |&gt;\n  add_residual_draws(fit05_04m, ndraws = 500) |&gt;\n  ungroup() |&gt;\n  select(D, .residual) |&gt;\n  group_by(D) |&gt;\n  mean_qi() |&gt;\n  select(D, resM = .residual)\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit05_04dm &lt;- xfun::cache_rds({brm(\n  data = res05_04m,\n  formula = D ~ 1 + resM,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.5), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 100, chains = 4, core = detectCores(),\n  seed = 5)\n  },\n  file = \"ch05_fit05_04dm\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.2 sec elapsed\n\n\n\nlpred_05_04dm &lt;- res05_04m |&gt;\n  add_linpred_draws(fit05_04dm, ndraws = 500) |&gt;\n  median_qi(.width = 0.89) |&gt;\n  identity()\n\nplot05_04 &lt;- within(plot05_04, {\n  marriage_res &lt;- lpred_05_04dm |&gt;\n    ggplot(mapping = aes(x = resM, y = D)) +\n    geom_point() +\n    geom_smooth(mapping = aes(y = .linpred, ymin = .lower, ymax = .upper),\n                stat = \"identity\", fill = \"powderblue\", color = \"darkblue\") +\n    labs(x = \"Marriage rate (standardized)\",\n         y = \"Divorce rate (standardized)\")\n  })\n\n\nres05_04a &lt;- dataWaffle |&gt;\n  select(Loc, D, M, A) |&gt;\n  add_residual_draws(fit05_04a, ndraws = 500) |&gt;\n  ungroup() |&gt;\n  select(D, .residual) |&gt;\n  group_by(D) |&gt;\n  mean_qi() |&gt;\n  select(D, resA = .residual)\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit05_04da &lt;- xfun::cache_rds({brm(\n  data = res05_04a,\n  formula = D ~ 1 + resA,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.5), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 100, chains = 4, core = detectCores(),\n  seed = 5)\n  },\n  file = \"ch05_fit05_04da\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.2 sec elapsed\n\n\n\nlpred_05_04da &lt;- res05_04a |&gt;\n  add_linpred_draws(fit05_04da, ndraws = 500) |&gt;\n  median_qi(.width = 0.89) |&gt;\n  identity()\n\nplot05_04 &lt;- within(plot05_04, {\n  age_res &lt;- lpred_05_04da |&gt;\n    ggplot(mapping = aes(x = resA, y = D)) +\n    geom_point() +\n    geom_smooth(mapping = aes(y = .linpred, ymin = .lower, ymax = .upper),\n                stat = \"identity\", fill = \"powderblue\", color = \"darkblue\") +\n    labs(x = \"Median age of marriage (standardized)\",\n         y = \"Divorce rate (standardized)\")\n  })\n\nwhich gives the bottom part of figure 5.1\n\nwrap_plots(plot05_04[c(\"marriage_res\", \"age_res\")]) +\n  plot_annotation(\"Fig 5.4: Divorce vs Residual marriage rate and median age of marriage\")\n\n\n\n\nWe conclude that the marriage rate seems to have little impact on the divorce rate once we remove the effect of the median age of marriage.\n\n\n\n5.1.5.2 Posterior prediction plots\nThe textbook mentions posterior predictions. So we use add_epred_draws() to get the expected predictions.\n\nepred05_03 &lt;- dataWaffle |&gt;\n  add_epred_draws(fit05_03, ndraws = 500) |&gt;\n  mean_qi()\nepred05_03\n\n# A tibble: 50 × 23\n   Location      Loc   Population MedianAgeMarriage Marriage Marriage.SE Divorce\n   &lt;fct&gt;         &lt;fct&gt;      &lt;dbl&gt;             &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Alabama       AL          4.78              25.3     20.2        1.27    12.7\n 2 Alaska        AK          0.71              25.2     26          2.93    12.5\n 3 Arizona       AZ          6.33              25.8     20.3        0.98    10.8\n 4 Arkansas      AR          2.92              24.3     26.4        1.7     13.5\n 5 California    CA         37.2               26.8     19.1        0.39     8  \n 6 Colorado      CO          5.03              25.7     23.5        1.24    11.6\n 7 Connecticut   CT          3.57              27.6     17.1        1.06     6.7\n 8 Delaware      DE          0.9               26.6     23.1        2.89     8.9\n 9 District of … DC          0.6               29.7     17.7        2.53     6.3\n10 Florida       FL         18.8               26.4     17          0.58     8.5\n# ℹ 40 more rows\n# ℹ 16 more variables: Divorce.SE &lt;dbl&gt;, WaffleHouses &lt;int&gt;, South &lt;int&gt;,\n#   Slaves1860 &lt;int&gt;, Population1860 &lt;int&gt;, PropSlaves1860 &lt;dbl&gt;, A &lt;dbl[,1]&gt;,\n#   M &lt;dbl[,1]&gt;, D &lt;dbl[,1]&gt;, .row &lt;int&gt;, .epred &lt;dbl&gt;, .lower &lt;dbl&gt;,\n#   .upper &lt;dbl&gt;, .width &lt;dbl&gt;, .point &lt;chr&gt;, .interval &lt;chr&gt;\n\nggplot(data = epred05_03, aes(x = D, y = .epred)) +\n  geom_abline(slope = 1, linetype = \"dashed\", color = \"darkorchid\") +\n  geom_point(color = \"firebrick4\") +\n  geom_linerange(aes(ymin = .lower, ymax = .upper), color = \"firebrick3\") +\n  geom_text(aes(label = Loc), size = 3, color = \"midnightblue\") +\n  labs(title = \"Figure 5.5: Posterior Predictive (expected) Plot: Divorce rate\",\n       x = \"observed divorce\",\n       y = \"predicted divorce\")\n\n\n\n\n\n\n5.1.5.3 Counterfactual plots\nWe are using the same DAG as above\n\ndagWaffle$plot1\n\n\n\n\nIMPORTANT: The full model implies 2 effects, \\(A\\) on \\(M\\) and \\(D\\) and \\(A\\) on \\(M\\). In other words 2 structural equations are involved or expressed differently, 2 formulas in brms.\n\nd_model &lt;- brms::bf(D ~ 1 + A + M)\nm_model &lt;- brms::bf(M ~ 1 + A)\n\nThe set_rescor(FALSE) indicates that we do not want brms to add the residual correlation between \\(D\\) and \\(M\\).\nAlso the argument resp is used to identify the response.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit05_03_A &lt;- xfun::cache_rds({brm(\n  data = dataWaffle, \n  family = gaussian,\n  d_model + m_model + set_rescor(FALSE),\n      prior = c(prior(normal(0, 0.2), class = Intercept, resp = D),\n                prior(normal(0, 0.5), class = b, resp = D),\n                prior(exponential(1), class = sigma, resp = D),\n                prior(normal(0, 0.2), class = Intercept, resp = M),\n                prior(normal(0, 0.5), class = b, resp = M),\n                prior(exponential(1), class = sigma, resp = M)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 5)\n  }, file = \"ch05_fit05_03_A\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.22 sec elapsed\n\n\n\nsummary(fit05_03_A)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: D ~ 1 + A + M \n         M ~ 1 + A \n   Data: dataWaffle (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nD_Intercept    -0.00      0.10    -0.20     0.20 1.00     5757     3111\nM_Intercept     0.00      0.09    -0.17     0.17 1.00     5386     2960\nD_A            -0.60      0.16    -0.91    -0.29 1.00     3381     2726\nD_M            -0.06      0.16    -0.36     0.26 1.00     3535     2749\nM_A            -0.69      0.10    -0.88    -0.49 1.00     5972     2664\n\nFamily Specific Parameters: \n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_D     0.83      0.09     0.68     1.02 1.00     5459     3301\nsigma_M     0.71      0.07     0.58     0.88 1.00     5342     2919\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe manipulate \\(M\\) \\(M=0\\) and predicting \\(D\\) with \\(A\\) with this new counterfactual \\(M\\).\n\npred05_03_A &lt;- list()\npred05_03_A$doA &lt;- data.frame(\n  A = seq_range(dataWaffle$A, n = 30, pretty = TRUE),\n  M = 0) |&gt;\n  add_predicted_draws(fit05_03_A, ndraws = 500) |&gt;\n  mean_qi()\n\n\nglimpse(pred05_03_A$doA)\n\nRows: 56\nColumns: 10\n$ A           &lt;dbl&gt; -2.4, -2.4, -2.2, -2.2, -2.0, -2.0, -1.8, -1.8, -1.6, -1.6…\n$ M           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ .row        &lt;int&gt; 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, …\n$ .category   &lt;fct&gt; D, M, D, M, D, M, D, M, D, M, D, M, D, M, D, M, D, M, D, M…\n$ .prediction &lt;dbl&gt; 1.48702703, 1.72097389, 1.37606354, 1.53466640, 1.20974873…\n$ .lower      &lt;dbl&gt; -0.38342018, 0.23006229, -0.44711690, 0.06568539, -0.51217…\n$ .upper      &lt;dbl&gt; 3.475603, 3.210165, 3.373526, 3.074758, 3.058260, 2.913136…\n$ .width      &lt;dbl&gt; 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95…\n$ .point      &lt;chr&gt; \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"m…\n$ .interval   &lt;chr&gt; \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\"…\n\n\n\nplot05_03_A &lt;- list()\nplot05_03_A$doAonD &lt;- pred05_03_A$doA |&gt;\n  filter(.category == \"D\") |&gt;\n  ggplot(aes(x = A, y = .prediction)) +\n  geom_smooth(aes(ymin = .lower, ymax = .upper), stat = \"identity\",\n              color = \"indianred4\", fill = \"indianred1\") +\n  labs(title = \"Total counterfactual effect of A on D\",\n       x = \"Manipulated A\", y = \"Counterfactual D\")\n# plot05_03_A$doAonD\n\n\nplot05_03_A$doAonM &lt;- pred05_03_A$doA |&gt;\n  filter(.category == \"M\") |&gt;\n  ggplot(aes(x = A, y = .prediction)) +\n  geom_smooth(aes(ymin = .lower, ymax = .upper), stat = \"identity\",\n              color = \"darkorange4\", fill = \"darkorange1\") +\n  labs(title = \"Total counterfactual effect of A on M\",\n       x = \"Manipulated A\", y = \"Counterfactual M\")\n# plot05_03_A$doAonM\n\n\nplot05_03_A$doAonD + plot05_03_A$doAonM\n\n\n\n\nFigure 5.6\n\n\n\n\nThe counterfactual effect of manipulating average rate effect \\(M\\) on divorce rate \\(D\\) is done as follows\n\npred05_03_A$doM &lt;- data.frame(\n  M = seq_range(dataWaffle$M, n = 30, pretty = TRUE),\n  A = 0) |&gt;\n  add_predicted_draws(fit05_03_A, ndraws = 500) |&gt;\n  mean_qi()\n# glimpse(pred05_03_A$doM)\n\n\nplot05_03_A$doMonD &lt;- pred05_03_A$doM |&gt;\n  filter(.category == \"D\") |&gt;\n  ggplot(aes(x = M, y = .prediction)) +\n  geom_smooth(aes(ymin = .lower, ymax = .upper), stat = \"identity\",\n              color = \"khaki4\", fill = \"khaki1\") +\n  labs(title = \"Total counterfactual effect of M on D\",\n       x = \"Manipulated M\", y = \"Counterfactual D\")\nplot05_03_A$doMonD\n\n\n\n\nFigure 5.7"
  },
  {
    "objectID": "ch05_multivariate.html#masked-relationship",
    "href": "ch05_multivariate.html#masked-relationship",
    "title": "5  Multivariate Linear Models",
    "section": "5.2 Masked relationship",
    "text": "5.2 Masked relationship\nLoad data and look at the pair plot. We use GGally::pairs() which gives better information and formatting options.\nBut first, as mentioned on p. 136, we need to remove missing values which cause problems when plotting and in modeling.\n\ndata(milk)\ndataMilk &lt;- milk |&gt;\n  as.data.frame() |&gt;\n  tidyr::drop_na() |&gt;\n  mutate(K = as.vector(scale(kcal.per.g)),\n         N = as.vector(scale(neocortex.perc)),\n         M = as.vector(scale(log(mass))))\nskimr::skim(dataMilk)\n\n\nData summary\n\n\nName\ndataMilk\n\n\nNumber of rows\n17\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nclade\n0\n1\nFALSE\n4\nNew: 7, Ape: 6, Old: 3, Str: 1\n\n\nspecies\n0\n1\nFALSE\n17\nA p: 1, Alo: 1, Cal: 1, Cal: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nkcal.per.g\n0\n1\n0.66\n0.17\n0.46\n0.49\n0.62\n0.80\n0.97\n▇▁▃▂▃\n\n\nperc.fat\n0\n1\n36.06\n14.71\n3.93\n27.18\n37.78\n50.49\n55.51\n▂▅▅▇▇\n\n\nperc.protein\n0\n1\n16.26\n5.60\n7.37\n11.68\n15.80\n20.85\n25.30\n▆▇▇▃▇\n\n\nperc.lactose\n0\n1\n47.68\n13.59\n27.09\n37.80\n46.88\n55.20\n70.77\n▇▇▃▇▆\n\n\nmass\n0\n1\n16.64\n23.58\n0.12\n1.55\n5.25\n33.11\n79.43\n▇▁▂▁▁\n\n\nneocortex.perc\n0\n1\n67.58\n5.97\n55.16\n64.54\n68.85\n71.26\n76.30\n▃▃▆▇▆\n\n\nK\n0\n1\n0.00\n1.00\n-1.14\n-0.97\n-0.22\n0.82\n1.81\n▇▁▃▂▃\n\n\nN\n0\n1\n0.00\n1.00\n-2.08\n-0.51\n0.21\n0.62\n1.46\n▃▃▆▇▆\n\n\nM\n0\n1\n0.00\n1.00\n-1.87\n-0.55\n0.08\n1.03\n1.49\n▂▃▇▁▇\n\n\n\n\n# it should give us a dataframe with 17 rows\nstopifnot(nrow(dataMilk) == 17)\n# glimpse(dataMilk)\n\n\nGGally::ggpairs(dataMilk, columns = c(\"K\", \"N\", \"M\")) +\n  ggthemes::theme_fivethirtyeight()\n\n\n\n\n\n5.2.1 Model 5.5\n\\[\n\\begin{align*}\nK &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_N \\cdot N_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_N &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand the fit is\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit05_05 &lt;- xfun::cache_rds({brm(\n  data = dataMilk,\n  formula = K ~ 1 + N,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sigma)\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n  seed = 5)\n  }, file = \"ch05_fit05_05\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.25 sec elapsed\n\n\n\nprint(fit05_05, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: K ~ 1 + N \n   Data: dataMilk (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept   -0.001     0.154   -0.297    0.298 1.002     3748     2970\nN            0.119     0.233   -0.348    0.572 1.000     3479     2764\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    1.039     0.192    0.749    1.487 1.000     3448     2054\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand show the coefficient plot\n\ntidybayes::get_variables(fit05_05)\n\n [1] \"b_Intercept\"   \"b_N\"           \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\npost05_05 &lt;- fit05_05 |&gt;\n  tidybayes::gather_draws(b_Intercept, b_N, sigma)\n\n\n# source: https://cran.r-project.org/web/packages/ggdist/vignettes/slabinterval.html\nplot05_05 &lt;- post05_05 |&gt;\n    ggplot(aes(x = .value, y = .variable)) +\n    stat_halfeye(aes(fill = after_stat(level)), .width = c(0.89, 1)) +\n    scale_x_continuous(breaks = scales::breaks_extended(n = 7),\n                       labels = scales::label_number(accuracy = 0.1)) +\n    scale_fill_paletteer_d(palette = \"calecopal::dudleya\",\n                           na.translate = FALSE) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Model 5.5: Posterior Distribution of Parameters\", x = NULL, y = NULL)\nplot05_05\n\n\n\n\nfit for different confidence intervals\n\nlpred05_05 &lt;- dataMilk |&gt;\n  add_linpred_draws(fit05_05, ndraws = 500)\n# glimpse(lpred05_05)\n\n\nplot05_05 &lt;- lpred05_05 |&gt;\n  ggplot(aes(x = N, y = .linpred)) +\n  stat_lineribbon(color = \"purple\") +\n  scale_fill_paletteer_d(\"ggthemes::Classic_Cyclic\") +\n  coord_cartesian(xlim = c(-2, 1.5), ylim = c(-2, 2)) +\n  theme(legend.position = \"none\") +\n  labs(x = \"neocortex.perc (standardized)\", y = \"kcal.per.g (standardized)\")\n# plot05_05\n\n\n\n5.2.2 Model 5.6\n\\[\n\\begin{align*}\nK &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_M \\cdot M_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_N &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand the fit is\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit05_06 &lt;- xfun::cache_rds({brm(\n  data = dataMilk,\n  formula = K ~ 1 + M,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n  seed = 5)\n  }, file = \"ch05_fit05_06\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.25 sec elapsed\n\n\n\nprint(fit05_06, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: K ~ 1 + M \n   Data: dataMilk (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept    0.001     0.155   -0.301    0.301 1.001     3730     2997\nM           -0.280     0.227   -0.719    0.164 1.001     3431     2721\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.994     0.190    0.702    1.439 1.000     3272     2669\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand the plot for the kilo vs mass\n\ntidybayes::get_variables(fit05_06)\n\n [1] \"b_Intercept\"   \"b_M\"           \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\nlpred05_06 &lt;- dataMilk |&gt;\n  select(K, M) |&gt;\n  add_linpred_draws(fit05_06, ndraws = 500) |&gt;\n  mean_qi(.width = 0.89)\nglimpse(lpred05_06)\n\nRows: 17\nColumns: 9\n$ K         &lt;dbl&gt; -1.1431349, -1.0852977, -1.0274605, -0.9696234, -0.9696234, …\n$ M         &lt;dbl&gt; -1.16624144, 0.08221084, 1.14221538, -0.43015430, 1.48762036…\n$ .row      &lt;int&gt; 7, 2, 15, 1, 14, 13, 16, 3, 12, 9, 8, 17, 6, 11, 4, 5, 10\n$ .linpred  &lt;dbl&gt; 0.31200026, -0.02892643, -0.31839192, 0.11098997, -0.4127149…\n$ .lower    &lt;dbl&gt; -0.1868543, -0.2938923, -0.7768433, -0.2033202, -0.9751731, …\n$ .upper    &lt;dbl&gt; 0.7779483, 0.2175400, 0.1550807, 0.3832155, 0.1870197, 0.150…\n$ .width    &lt;dbl&gt; 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, …\n$ .point    &lt;chr&gt; \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mea…\n$ .interval &lt;chr&gt; \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", …\n\n\n\nplot05_06 &lt;- ggplot(dataMilk, aes(x = M, y = K)) +\n  geom_point() +\n  geom_smooth(lpred05_06, \n              mapping = aes(x = M, y = .linpred, ymin = .lower, ymax = .upper),\n              stat = \"identity\", inherit.aes = FALSE,\n              color = \"darkslategray4\", fill = \"darkslategray1\") +\n  coord_cartesian(xlim = c(-1.9, 1.9), ylim = c(-2, 2)) +\n  theme(legend.position = c(0.8, 0.2)) +\n  labs(x = \"mass (log)\", y = \"kcal.per.g (standardized)\")\n# plot05_06\n\n\n\n5.2.3 Model 5.7\nwe now add neocortex and log mass together to see their mutual effect,\n\\[\n\\begin{align*}\nK_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_N \\cdot N_i + \\beta_M \\cdot log(M_i) \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_N &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\beta_M &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand the fit is\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit05_07 &lt;- xfun::cache_rds({brm(\n  data = dataMilk,\n  formula = K ~ 1 + N + M,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(cauchy(0, 0.5), class = sigma)\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n  seed = 5)\n  }, file = \"ch05_fit05_07\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.25 sec elapsed\n\n\n\nprint(fit05_07, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: K ~ 1 + N + M \n   Data: dataMilk (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept    0.001     0.139   -0.277    0.267 1.001     3021     2364\nN            0.577     0.263    0.030    1.057 1.002     2108     2531\nM           -0.680     0.262   -1.162   -0.131 1.002     2085     2112\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.789     0.164    0.541    1.192 1.001     2204     2234\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nget the counterfactual data holding \\(M = 0\\)\n\nlpred05_07 &lt;- list()\nlpred05_07 &lt;- within(lpred05_07, {\n  doM &lt;- dataMilk |&gt;\n    select(K, M, N) |&gt;\n    mutate(M = 0) |&gt;\n    add_linpred_draws(fit05_07, ndraws = 500) |&gt;\n    mean_qi(.width = 0.89)\n  \n  doN &lt;- dataMilk |&gt;\n    select(K, M, N) |&gt;\n    mutate(N = 0) |&gt;\n    add_linpred_draws(fit05_07, ndraws = 500) |&gt;\n    mean_qi(.width = 0.89)\n})\nglimpse(lpred05_07)\n\nList of 2\n $ doN: tibble [17 × 10] (S3: tbl_df/tbl/data.frame)\n  ..$ K        : num [1:17] -1.14 -1.09 -1.03 -0.97 -0.97 ...\n  ..$ M        : num [1:17] -1.1662 0.0822 1.1422 -0.4302 1.4876 ...\n  ..$ N        : num [1:17] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ .row     : int [1:17] 7 2 15 1 14 13 16 3 12 9 ...\n  ..$ .linpred : num [1:17] 0.8 -0.056 -0.783 0.295 -1.02 ...\n  ..$ .lower   : num [1:17] 0.24898 -0.2651 -1.29512 0.00118 -1.63963 ...\n  ..$ .upper   : num [1:17] 1.321 0.166 -0.294 0.56 -0.391 ...\n  ..$ .width   : num [1:17] 0.89 0.89 0.89 0.89 0.89 0.89 0.89 0.89 0.89 0.89 ...\n  ..$ .point   : chr [1:17] \"mean\" \"mean\" \"mean\" \"mean\" ...\n  ..$ .interval: chr [1:17] \"qi\" \"qi\" \"qi\" \"qi\" ...\n $ doM: tibble [17 × 10] (S3: tbl_df/tbl/data.frame)\n  ..$ K        : num [1:17] -1.14 -1.09 -1.03 -0.97 -0.97 ...\n  ..$ M        : num [1:17] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ N        : num [1:17] -0.986 -0.509 0.446 -2.08 0.842 ...\n  ..$ .row     : int [1:17] 7 2 15 1 14 13 16 3 12 9 ...\n  ..$ .linpred : num [1:17] -0.572 -0.293 0.264 -1.21 0.495 ...\n  ..$ .lower   : num [1:17] -1.0113 -0.6039 -0.0195 -2.0447 0.0824 ...\n  ..$ .upper   : num [1:17] -0.062 0.00445 0.52351 -0.2712 0.87229 ...\n  ..$ .width   : num [1:17] 0.89 0.89 0.89 0.89 0.89 0.89 0.89 0.89 0.89 0.89 ...\n  ..$ .point   : chr [1:17] \"mean\" \"mean\" \"mean\" \"mean\" ...\n  ..$ .interval: chr [1:17] \"qi\" \"qi\" \"qi\" \"qi\" ...\n\n\n\nplot05_07 &lt;- list()\nplot05_07 &lt;- within(plot05_07, {\n  doM &lt;- ggplot(dataMilk, aes(x = N, y = K)) +\n    geom_point() +\n    geom_smooth(lpred05_07$doM, \n                mapping = aes(x = N, y = .linpred, ymin = .lower, ymax = .upper),\n                stat = \"identity\", inherit.aes = FALSE,\n              color = \"chartreuse4\", fill = \"chartreuse1\") +\n    coord_cartesian(xlim = c(-1.9, 1.9), ylim = c(-2, 2)) +\n    theme(legend.position = c(0.8, 0.2)) +\n    labs(title = \"Counterfactual holding M = 0\",\n         x = \"neocortex.perc (standardized)\", y = \"kcal.per.g (standardized)\")\n  \n  doN &lt;- ggplot(dataMilk, aes(x = M, y = K)) +\n  geom_point() +\n  geom_smooth(lpred05_07$doN, \n              mapping = aes(x = M, y = .linpred, ymin = .lower, ymax = .upper),\n              stat = \"identity\", inherit.aes = FALSE,\n              color = \"burlywood4\", fill = \"burlywood1\") +\n  coord_cartesian(xlim = c(-1.9, 1.9), ylim = c(-2, 2)) +\n  theme(legend.position = c(0.8, 0.2)) +\n  labs(title = \"Counterfactual holding N = 0\",\n       x = \"mass (log)\", y = \"kcal.per.g (standardized)\")\n})\n\n\n((plot05_05 | plot05_06) / (plot05_07$doM | plot05_07$doN)) +\n  plot_annotation(title = \"Milk Energy and Neocortex Among Primates\")\n\n\n\n\nFigure 5.9"
  },
  {
    "objectID": "ch05_multivariate.html#categorical-variables",
    "href": "ch05_multivariate.html#categorical-variables",
    "title": "5  Multivariate Linear Models",
    "section": "5.3 Categorical variables",
    "text": "5.3 Categorical variables\n\n5.3.1 Binary categories\nLoad data\n\ndata(\"Howell1\")\ndataHowel &lt;- Howell1 |&gt;\n  mutate(sex = factor(male))\n# glimpse(dataHowel)\n\nthe model\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{sex[i]} \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta_m &\\sim \\mathcal{N}(0, 10) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit05_08 &lt;- xfun::cache_rds({brm(\n  data = dataHowel,\n  formula = height ~ 0 + sex,\n  family = gaussian,\n  prior = c(\n    prior(normal(178, 20), class = b),\n    prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n  seed = 5)\n  }, file = \"ch05_fit05_08\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.3 sec elapsed\n\n\n\nsummary(fit05_08)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 0 + sex \n   Data: dataHowel (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsex0   134.90      1.57   131.83   137.92 1.00     4095     3000\nsex1   142.56      1.66   139.33   145.86 1.00     4365     3015\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    26.77      0.79    25.27    28.34 1.00     4864     2451\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nRead the important comment in section 5.3.1 when using \\(\\alpha\\). \\(\\alpha\\) now represents the average of women and the male heights is more variable because it relates to 2 parameters instead of 1.\n\n\n\n5.3.2 Many categories\n\nSee the overthinking box in section 5.3.2 on how to reparametrize. Very nice.\n\nLoad data, standardize and make sure clade is a factor.\n\nWith brms there is no need to create the contrasts. We simply make sure that *clade* is a factor.brms` will create the contrasts by itself.\n\n\ndata(\"milk\")\ndataMilk &lt;- milk |&gt; \n  mutate(K = as.vector(scale(kcal.per.g)))\n# skimr::skim(d)\nstopifnot(is.factor(dataMilk$clade))\n\nthe model\n\\[\n\\begin{align*}\nK_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{clade[i]} \\\\\n\\alpha_j &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit05_09 &lt;- xfun::cache_rds({brm(\n  data = dataMilk,\n  formula = K ~ 0 + clade,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n  seed = 5)\n  }, file = \"ch05_fit05_09\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.3 sec elapsed\n\n\n\nsummary(fit05_09)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: K ~ 0 + clade \n   Data: dataMilk (Number of observations: 29) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ncladeApe               -0.46      0.24    -0.92     0.02 1.00     4302     2750\ncladeNewWorldMonkey     0.35      0.25    -0.15     0.83 1.00     4573     2474\ncladeOldWorldMonkey     0.64      0.28     0.08     1.15 1.00     5032     3030\ncladeStrepsirrhine     -0.55      0.30    -1.12     0.05 1.00     4299     2852\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.80      0.12     0.61     1.07 1.00     3575     2858\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\ntidybayes::get_variables(fit05_09)\n\n [1] \"b_cladeApe\"            \"b_cladeNewWorldMonkey\" \"b_cladeOldWorldMonkey\"\n [4] \"b_cladeStrepsirrhine\"  \"sigma\"                 \"lprior\"               \n [7] \"lp__\"                  \"accept_stat__\"         \"stepsize__\"           \n[10] \"treedepth__\"           \"n_leapfrog__\"          \"divergent__\"          \n[13] \"energy__\"             \n\n\nand plot the result\n\ntidybayes::gather_draws(fit05_09, b_cladeApe, b_cladeNewWorldMonkey, \n                        b_cladeOldWorldMonkey, b_cladeStrepsirrhine) |&gt;\n  mean_hdi(.width = 0.89) |&gt;\n  ggplot(aes(x = .value, xmin = .lower, xmax = .upper, y = .variable)) +\n  geom_vline(xintercept = 0, colour = \"darkgoldenrod\") +\n  geom_pointinterval(fatten_point = 2.5, color = \"yellowgreen\", size = 5) +\n  theme(panel.grid.major.y = element_line(linetype = \"dotted\", linewidth = 1)) +\n  labs(title = \"Expected kcal by clade with 89% CI\", x = \"kcal (std)\", y = NULL)"
  },
  {
    "objectID": "ch05_multivariate.html#other-ways-to-modify-categorical-variables",
    "href": "ch05_multivariate.html#other-ways-to-modify-categorical-variables",
    "title": "5  Multivariate Linear Models",
    "section": "5.4 Other ways to modify categorical variables",
    "text": "5.4 Other ways to modify categorical variables\nSee Kurz (2020)"
  },
  {
    "objectID": "ch05_multivariate.html#summary",
    "href": "ch05_multivariate.html#summary",
    "title": "5  Multivariate Linear Models",
    "section": "5.5 Summary",
    "text": "5.5 Summary\n\n\n\n\nKurz, Solomon. 2020. Statistical Rethinking with Brms. 2nd ed. https://bookdown.org/content/4857/."
  },
  {
    "objectID": "ch06_scm.html#multicollinearity",
    "href": "ch06_scm.html#multicollinearity",
    "title": "6  Structural Causal Models",
    "section": "6.1 Multicollinearity",
    "text": "6.1 Multicollinearity\nMulticollinearity means a very strong association between 2 or more predictor variables.\n\n6.1.1 Multicollinear legs\nCreate the data\n\nset.seed(6)\ndataLegs &lt;- \n  tibble(height   = rnorm(100, mean = 10, sd = 2),\n         leg_prop = runif(100, min = 0.4, max = 0.5)) |&gt; \n  mutate(leg_left  = leg_prop * height + rnorm(100, mean = 0, sd = 0.02),\n         leg_right = leg_prop * height + rnorm(100, mean = 0, sd = 0.02))\n\nwhich has the following correlations\n\ndataLegs |&gt;\n   cor() |&gt;\n   ggcorrplot::ggcorrplot(type = \"lower\", lab = TRUE, digits = 2,\n                          show.legend = FALSE,\n                          title = \"Correlations between variables\") +\n  scale_fill_paletteer_c(\"pals::warmcool\", direction = -1)\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"110 secs.\"))\nfit06_01 &lt;- xfun::cache_rds({brm(data = dataLegs, \n      family = gaussian,\n      height ~ 1 + leg_left + leg_right,\n      prior = c(prior(normal(10, 100), class = Intercept),\n                prior(normal(2, 10), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 6)},\n  file = \"ch06_fit06_01\")\ntictoc::toc()\n\nrun time of 110 secs., use the cache.: 0.15 sec elapsed\n\n\n\nposterior_summary(fit06_01) |&gt; round(digits = 3)\n\n            Estimate Est.Error     Q2.5    Q97.5\nb_Intercept    0.814     0.302    0.208    1.397\nb_leg_left     1.319     2.211   -3.170    5.728\nb_leg_right    0.711     2.223   -3.714    5.230\nsigma          0.633     0.046    0.550    0.727\nlprior       -12.660     0.088  -12.898  -12.543\nlp__        -108.762     1.417 -112.231 -106.954\n\n\n\ntidybayes::get_variables(fit06_01)\n\n [1] \"b_Intercept\"   \"b_leg_left\"    \"b_leg_right\"   \"sigma\"        \n [5] \"lprior\"        \"lp__\"          \"accept_stat__\" \"stepsize__\"   \n [9] \"treedepth__\"   \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\ntidybayes::gather_draws(fit06_01, b_Intercept, b_leg_left, b_leg_right, sigma) |&gt;\n  mean_hdi() |&gt;\n  ggplot(aes(x = .value, xmin = .lower, xmax = .upper, y = .variable, color = .variable)) +\n  geom_pointinterval() +\n  ggrepel::geom_text_repel(aes(label = round(.value, 2))) +\n  scale_color_paletteer_d(\"Manu::Kereru\") +\n  labs(title = \"Leg model\",\n       x = \"value\", y = NULL, color = NULL)\n\n\n\n\n\n\n6.1.2 Multicollinear milk\n\ndata(milk)\ndataMilk &lt;- milk\ndataMilk &lt;- dataMilk |&gt;\n  mutate(K = as.vector(scale(kcal.per.g)),\n         `F` = as.vector(scale(perc.fat)),\n         L = as.vector(scale(perc.lactose)))\n\n\nplotMilk &lt;- list()\nplotMilk &lt;- within(plotMilk, {\n  fun_diag &lt;- function(data, mapping, ...){\n    ggplot(data = data, mapping = mapping) +\n      geom_density(linewidth = 1, color = \"pink\")}\n  fun_upper &lt;- function(data, mapping) {\n    ggplot(data = data, mapping = mapping) +\n      geom_text(size = 1, color = \"blue\")}\n  fun_lower &lt;- function(data, mapping) {\n    ggplot(data = data, mapping = mapping) +\n      stat_density2d(linewidth = 1/3, color = \"blue\") +\n      geom_point(size = 1, color = \"blue\") +\n      geom_smooth(method = \"loess\")}\n  plot &lt;- GGally::ggpairs(\n    data = dataMilk,\n    columns = c(\"K\", \"F\", \"L\"),\n    title = \"Milk example\",\n    diag = list(continuous = fun_diag),\n    lower = list(continuous = fun_lower))\n})\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nplotMilk$plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit06_03 &lt;- xfun::cache_rds({\n  # k regressed on f\n  brm(data = dataMilk, \n      family = gaussian,\n      K ~ 1 + `F`,\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 6)},\n  file = \"ch06_fit06_03\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.16 sec elapsed\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"90 secs.\"))\nfit06_04 &lt;- xfun::cache_rds({\n  # k regressed on f\n  brm(data = dataMilk, \n      family = gaussian,\n      K ~ 1 + L,\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 6)},\n  file = \"ch06_fit06_04\")\ntictoc::toc()\n\nrun time of 90 secs., use the cache.: 0.14 sec elapsed\n\n\nand the coefficients are\n\nposterior_summary(fit06_03) |&gt; round(digits = 3)\n\n            Estimate Est.Error    Q2.5   Q97.5\nb_Intercept    0.001     0.084  -0.166   0.165\nb_F            0.856     0.089   0.673   1.027\nsigma          0.488     0.069   0.374   0.643\nlprior        -1.593     0.336  -2.311  -1.005\nlp__         -22.091     1.276 -25.371 -20.639\n\n\n\nposterior_summary(fit06_04) |&gt; round(digits = 2)\n\n            Estimate Est.Error   Q2.5  Q97.5\nb_Intercept     0.00      0.07  -0.14   0.14\nb_L            -0.90      0.08  -1.06  -0.75\nsigma           0.41      0.06   0.32   0.55\nlprior         -1.65      0.30  -2.30  -1.12\nlp__          -17.38      1.25 -20.63 -15.91\n\n\nand the multivariate which shows that each variable has now a much larger variance caused by the colinearity.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit06_05 &lt;- xfun::cache_rds({\n  brm(data = dataMilk, \n      family = gaussian,\n      K ~ 1 + `F` + L,\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 6)},\n  file = \"ch06_fit06_05\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.15 sec elapsed"
  },
  {
    "objectID": "ch06_scm.html#post-treatment-bias",
    "href": "ch06_scm.html#post-treatment-bias",
    "title": "6  Structural Causal Models",
    "section": "6.2 Post-treatment bias",
    "text": "6.2 Post-treatment bias\n\n# how many plants would you like?\nset.seed(7)\ndataPlants &lt;- tibble(\n  h0        = rnorm(100, mean = 10, sd = 2), \n  treatment = rep(0:1, each = 100 / 2),\n  fungus    = rbinom(100, size = 1, prob = .5 - treatment * 0.4),\n  h1        = h0 + rnorm(100, mean = 5 - 3 * fungus, sd = 1))\nskimr::skim(dataPlants)\n\n\nData summary\n\n\nName\ndataPlants\n\n\nNumber of rows\n100\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nh0\n0\n1\n10.28\n1.92\n6.43\n8.88\n10.21\n11.44\n15.43\n▃▇▇▃▂\n\n\ntreatment\n0\n1\n0.50\n0.50\n0.00\n0.00\n0.50\n1.00\n1.00\n▇▁▁▁▇\n\n\nfungus\n0\n1\n0.30\n0.46\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nh1\n0\n1\n14.38\n2.41\n7.93\n13.12\n14.69\n15.87\n20.53\n▁▃▇▇▁\n\n\n\n\n\n\n6.2.1 A prior is born\n\nIf we center our prior for \\(p\\) on 1, that implies an expectation of no change in height. That is less than we know. But we would allow \\(p\\) to be less than 1, in case the experiment ges wrong. We also want to ensure \\(p&gt;0\\).\n\nTherefore we use \\(p\\) with log-normal distribution\n\\[\n\\begin{align*}\nh_{1,i} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= h_{0,i} \\times p \\\\\np &\\sim \\mathcal{LogNormal}(0, 0.25) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"90 secs.\"))\nfit06_06 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataPlants, \n    family = gaussian,\n    h1 ~ 0 + h0,\n    prior = c(prior(lognormal(0, 0.25), class = b, lb = 0),\n              prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 6)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  out},\n  file = \"ch06_fit06_06\")\ntictoc::toc()\n\nrun time of 90 secs., use the cache.: 0.2 sec elapsed\n\n\n\nbrms::posterior_summary(fit06_06) |&gt; round(digits = 2)\n\n       Estimate Est.Error    Q2.5   Q97.5\nb_h0       1.38      0.02    1.35    1.42\nsigma      1.74      0.13    1.51    2.01\nlprior    -2.44      0.15   -2.76   -2.18\nlp__    -199.13      0.99 -201.92 -198.13\n\n\nSo the increase is 1.38 relative to \\(h_0\\).\nNow including the treatment and fungus we have\n\\[\n\\begin{align*}\nh_{1,i} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= h_{0,i} \\times p \\\\\np &\\sim \\alpha + \\beta_1 treatment_i + \\beta_2 fungus_i \\\\\n\\alpha &\\sim \\mathcal{LogNormal}(0, 0.25) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\beta_2 &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit06_07 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataPlants, \n    family = gaussian,\n    bf(h1 ~ h0 * (a + t * treatment + f * fungus),\n       a + t + f ~ 1, \n       nl = TRUE),\n    prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0),\n                prior(normal(0, 0.5), nlpar = t),\n                prior(normal(0, 0.5), nlpar = f),\n                prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 6)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  out},\n  file = \"ch06_fit06_07\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.12 sec elapsed\n\n\n\nbrms::posterior_summary(fit06_07) |&gt; round(digits = 2)\n\n              Estimate Est.Error    Q2.5   Q97.5\nb_a_Intercept     1.42      0.02    1.38    1.47\nb_t_Intercept     0.04      0.03   -0.01    0.10\nb_f_Intercept    -0.20      0.03   -0.27   -0.14\nsigma             1.33      0.10    1.16    1.53\nlprior           -3.10      0.20   -3.51   -2.72\nlp__           -173.07      1.40 -176.65 -171.34\n\n\nNow the effect of the treatment is almost non existent.\n\n\n6.2.2 Blocked by consequence\nThe problem is that the fungus is part of a chain between the treatment and the growth.\n\nggdag::dagify(h1 ~ h0, h1 ~ `F`, `F` ~ `T`) |&gt;\n    ggdag::ggdag(layout = \"sugiyama\", node_size = 8, text_col = \"yellow\") +\n    ggdag::theme_dag_blank(\n      panel.background = element_rect(fill = \"snow2\", color = \"snow2\"))\n\n\n\n\nso now we redo the model but without the fungus effect.\n\\[\n\\begin{align*}\nh_{1,i} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= h_{0,i} \\times p \\\\\np &\\sim \\alpha + \\beta_1 treatment_i \\\\\n\\alpha &\\sim \\mathcal{LogNormal}(0, 0.25) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit06_08 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataPlants, \n    family = gaussian,\n    bf(h1 ~ h0 * (a + t * treatment),\n      a + t ~ 1, nl = TRUE),\n    prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0),\n                prior(normal(0, 0.5), nlpar = t),\n                prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 6)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch06_fit06_08\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.16 sec elapsed\n\n\n\nposterior_summary(fit06_08) |&gt; round(digits = 3)\n\n              Estimate Est.Error     Q2.5    Q97.5\nb_a_Intercept    1.312     0.021    1.271    1.354\nb_t_Intercept    0.150     0.030    0.091    0.208\nsigma            1.571     0.113    1.368    1.811\nlprior          -2.349     0.159   -2.692   -2.064\nlp__          -188.581     1.245 -191.920 -187.156\n\n\nand we now see more treatment effect."
  },
  {
    "objectID": "ch06_scm.html#collider-bias",
    "href": "ch06_scm.html#collider-bias",
    "title": "6  Structural Causal Models",
    "section": "6.3 Collider bias",
    "text": "6.3 Collider bias\n\n6.3.1 Collider of false sorrow\n\ndataHappy &lt;- rethinking::sim_happiness(seed = 1977, N_years = 1000)\n# select age &gt; 17 and rescale to [0, 1] and create indexed factor\n# creating factor makes it easer with brms\ndataHappy_gt17 &lt;- dataHappy |&gt;\n  filter(age &gt; 17) |&gt;\n  mutate(A = scales::rescale(age, to = c(0, 1)),\n         mid = factor(married + 1, labels = c(\"single\", \"married\")))\n# glimpse(dataHappy_gt17)\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit06_09 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataHappy_gt17, \n    family = gaussian,\n    happiness ~ 0 + mid + A,\n    prior = c(prior(normal(0, 1), class = b, coef = midmarried),\n                prior(normal(0, 1), class = b, coef = midsingle),\n                prior(normal(0, 2), class = b, coef = A),\n                prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 6)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch06_fit06_09\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.17 sec elapsed\n\n\n\nposterior_summary(fit06_09)\n\n                  Estimate  Est.Error          Q2.5         Q97.5\nb_midsingle     -0.2360886 0.06276554    -0.3585190    -0.1139693\nb_midmarried     1.2576981 0.08489173     1.0925441     1.4227535\nb_A             -0.7470218 0.11251484    -0.9648072    -0.5258873\nsigma            0.9922772 0.02270052     0.9486625     1.0377774\nlprior          -5.3379200 0.11727085    -5.5848227    -5.1304462\nlp__         -1360.3803066 1.44212127 -1364.0379724 -1358.6465647\n\n\nThe fit finds that the effect of age on happiness is negative\nnow lets do it without the marriage factor\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"90 secs.\"))\nfit06_10 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataHappy_gt17,\n    family = gaussian,\n    happiness ~ 1 + A,\n    prior = c(prior(normal(0, 1), class = Intercept),\n                prior(normal(0, 2), class = b),\n                prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 6)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch06_fit06_10\")\ntictoc::toc()\n\nrun time of 90 secs., use the cache.: 0.17 sec elapsed\n\n\n\nposterior_summary(fit06_10)\n\n                 Estimate  Est.Error          Q2.5         Q97.5\nb_Intercept -7.860119e-04 0.07794572    -0.1488609     0.1563472\nb_A          2.175651e-03 0.13191281    -0.2591360     0.2579227\nsigma        1.216523e+00 0.02774757     1.1638998     1.2719130\nlprior      -3.750499e+00 0.02806359    -3.8070398    -3.6975066\nlp__        -1.553357e+03 1.19787955 -1556.4143153 -1551.9710955\n\n\nNow the age has no effect on happiness! When we include marriage, we include a spurious association.\n\n\n6.3.2 The haunted DAG"
  },
  {
    "objectID": "ch06_scm.html#confronting-counfounding",
    "href": "ch06_scm.html#confronting-counfounding",
    "title": "6  Structural Causal Models",
    "section": "6.4 Confronting counfounding",
    "text": "6.4 Confronting counfounding\nSee Overthinking box in section 6.4.3. Confounding occurs when\n\\[\nPr(Y \\mid X) \\neq Pr(Y \\mid do(X))\n\\]"
  },
  {
    "objectID": "ch06_scm.html#summary",
    "href": "ch06_scm.html#summary",
    "title": "6  Structural Causal Models",
    "section": "6.5 Summary",
    "text": "6.5 Summary"
  },
  {
    "objectID": "ch07_information.html#the-problem-with-parameters",
    "href": "ch07_information.html#the-problem-with-parameters",
    "title": "7  Ulysses’ Compass",
    "section": "7.1 The problem with parameters",
    "text": "7.1 The problem with parameters\n\\(R^2\\) is not the right way to do it.\n\\[\n\\begin{align*}\nR^2 &= \\frac{var(outcome) - var(residuals)}{var(outcome)} =\n1 - \\frac{var(residuals)}{var(outcome)} \\\\\n&= 1- \\frac{SSR}{SST}\n\\end{align*}\n\\]\n\n7.1.1 More parameters always improve fit OVERFITTING\nGet the data and standardize it. In section 7.1.1 McElreath (2020) explains that we rescale brain size instead of standardizing it because we want to preserve zero as a reference point.\n\ndataBrains &lt;- tibble(\n    species = c(\"afarensis\", \"africanus\", \"habilis\", \"boisei\",\n                \"rudolfensis\", \"ergaster\", \"sapiens\"),\n    brain = c(438, 452, 612, 521, 752, 871, 1350),\n    mass = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)) |&gt;\n    mutate(B = scales::rescale(brain),\n           M = as.vector(scale(mass)))\n# dataBrains\n\nplot the raw data\n\n# we do a plot with fancy background\nplotBrains &lt;- list()\nplotBrains &lt;- within(plotBrains, {\n  colr &lt;- data.frame(colr = seq_range(dataBrains$mass, n = 100))\n  p &lt;- ggplot(dataBrains, aes(x = mass, y = brain, label = species)) +\n    geom_segment(data = colr, aes(x = colr, xend = colr, y = -Inf, yend = Inf,\n                                  color = colr),\n                 inherit.aes = FALSE, size = 3) +\n    geom_point(color = \"gold\", size = 3) +\n    ggrepel::geom_text_repel(color = \"yellow\") +\n    scale_color_paletteer_c(\"scico::berlin\") +\n    theme_minimal() +\n    theme(panel.grid = element_blank(),\n          legend.position = \"none\") +\n    labs(title = \"Average brain volume vs body mass for 6 hominin species\",\n         x = \"body mass in kg\", y = \"brain volume in cc\")\n})\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplotBrains$p\n\n\n\n\nSee hadley for very nice discussion on how to process several models in one dataframe.\n\n\n7.1.2 Too few parameters hurts UNDERFITTING\nSee Rethinking box in section 7.1.2 which explains the Bias-variance trade-off.\nBias relates to underfitting and variance to over-fitting."
  },
  {
    "objectID": "ch07_information.html#entropy-and-accuracy",
    "href": "ch07_information.html#entropy-and-accuracy",
    "title": "7  Ulysses’ Compass",
    "section": "7.2 Entropy and accuracy",
    "text": "7.2 Entropy and accuracy\n\n7.2.1 Firing the weatherperson\n\n# the emoji used in his section\nweather &lt;- list()\nweather &lt;- within(weather, {\n  sun &lt;- emo::ji(\"sun\")\n  rain &lt;- emo::ji(\"cloud_with_rain\")\n  umbrella &lt;- emo::ji(\"closed_umbrella\")\n  \n  df1 &lt;- data.frame(\n    day = 1:10,\n    predicted = rep(c(1, 0.6), times = c(3, 7)),\n    observed = rep(c(rain, sun), times = c(3, 7))) |&gt;\n    t() |&gt;\n    as.data.frame() |&gt;\n    tibble::rownames_to_column()\n})\n\nThe currently employed weather person has the following data\n\nweather$df1 |&gt; gt::gt(rowname_col = \"rowname\") |&gt;\n    gt::tab_options(column_labels.hidden = TRUE)\n\n\n\n\n\n  \n  \n    day\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n    predicted\n1.0\n1.0\n1.0\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n    observed\n🌧\n🌧\n🌧\n☀️\n☀️\n☀️\n☀️\n☀️\n☀️\n☀️\n  \n  \n  \n\n\n\n\nThe new weather person has this data\n\nweather &lt;- within(weather, {\n  df2 &lt;- data.frame(\n    day = 1:10,\n    predicted = 0,\n    observed = rep(c(rain, sun), times = c(3, 7))\n    ) |&gt;\n    t() |&gt;\n    as.data.frame() |&gt;\n    tibble::rownames_to_column()\n})\nweather$df2 |&gt; gt::gt(rowname_col = \"rowname\") |&gt;\n    gt::tab_options(column_labels.hidden = TRUE)\n\n\n\n\n\n  \n  \n    day\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n    predicted\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n    observed\n🌧\n🌧\n🌧\n☀️\n☀️\n☀️\n☀️\n☀️\n☀️\n☀️\n  \n  \n  \n\n\n\n\nwould be, i.e. the expected nb of correct predictions,\n\n3 * 1 + 7 * 0.4\n\n[1] 5.8\n\n\nwhich gives a frequency per day (probability) of\n\n(3 * 1 + 7 * 0.4) / 10\n\n[1] 0.58\n\n\n\n\n7.2.2 Information and uncertainty\n\\[\nH(p)=-E(\\log{p_i})=-\\sum_{i=1}^n{p_i \\cdot \\log{p_i}}\n\\]\nthe entropy for the above is\n\n# define a function to compute entropy\nentr &lt;- \\(x) {-sum(x * log(x))}\nentr(c(0.3, 0.7))\n\n[1] 0.6108643\n\n\nbut in Abu Dhabi it is\n\nentr(c(0.01, 0.99))\n\n[1] 0.05600153\n\n\n\n\n7.2.3 From entropy to accuracy\n\\[\nD_{KL}(p,q) = H(p,q) - H(p) = -\\sum{p_i \\cdot (\\log{p_i} - \\log{q_i})} =\n-\\sum{p_i \\cdot \\frac {\\log{p_i}} {\\log{q_i}}}\n\\]\n\n\n7.2.4 Estimating divergence\nThe whole point here is the if we have 2 models, with 2 different probability distributions \\(q\\) and \\(r\\)\nthen their respective divergence is\n\\[\nD_{KL}(p,q) = H(p,q) - H(p) = E(\\log{q}) - E(\\log{p})\n\\]\nand\n\\[\nD_{KL}(p,r) = H(p,r) - H(p) = E(\\log{r}) - E(\\log{p})\n\\]\nand therefore their relative divergence between each other is\n\\[\n\\begin{align*}\nD_{KL}(p,q) - D_{KL}(p,r) &= [H(p,q) - H(p)] - [H(p,r) - H(p)] \\\\\n&= H(p,q) - H(p,r)  \\\\ &= E(\\log{q}) - E(\\log{r})\n\\end{align*}\n\\]\nand the relative value of the \\(D_{KL}(p,q)\\) and \\(D_{KL}(p,r)\\) is approximated with their deviance\n\\[\nD(q) = -2 \\sum_i{\\log{q_i}} \\\\\nD(r) = -2 \\sum_i{\\log{r_i}}\n\\]\nor, even more simply we could use the total score\n\\[\nS(q) = \\sum_i{\\log{q_i}} \\\\\nS(r) = \\sum_i{\\log{r_i}}\n\\]\n\nImportant: Since the deviance / total score represente the relative distance from the target, it does not mean anything by itself. It means something only when comparing models with each other.\n\nThe Bayesian version of the log-probability score is called the log-pointwise-predictive-density (lppd) and is defined as\nThe log-pointwise-predictive-density\n\\[\nlppd(y, \\Theta) = \\sum_{i=1}^N{\\log{Pr(y_i)}} =\n\\sum_{i=1}^N{\\log{\\frac{1}{S}\\sum_{s=1}^SPr(y_i \\mid \\Theta)}}\n\\]\n\n\n7.2.5 Scoring the right data\nNote the the total score, or deviance used just previously suffers from the same flaw as \\(R^2\\). That is, the more parameters (i.e. complexity), the better fit we obtain regardless of the relevance of such a complexity."
  },
  {
    "objectID": "ch07_information.html#golem-taming-regularization",
    "href": "ch07_information.html#golem-taming-regularization",
    "title": "7  Ulysses’ Compass",
    "section": "7.3 Golem taming: regularization",
    "text": "7.3 Golem taming: regularization\n\\[\n\\begin{align*}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 100) \\\\\n\\beta &\\sim \\mathcal{N}(0, 1) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch07_information.html#predicting-predictive-accuracy",
    "href": "ch07_information.html#predicting-predictive-accuracy",
    "title": "7  Ulysses’ Compass",
    "section": "7.4 Predicting predictive accuracy",
    "text": "7.4 Predicting predictive accuracy\nThere are 2 families of strategies to evaluate models\n\nCross-validation\nInformation citeria\n\n\n7.4.1 Cross-validation\nThe method suggested here is to use Leave-one-out cross validation (LOOCV) coupled with Pareto-smoothed importance sampling cross-validation (PSIS) to approximate the LOOCV’s score.\nThe best feature of PSIS is that it provides feed back about its own reliability.\n\n\n7.4.2 Information criteria\nThe difference in deviance between in-sample and out-of-sample is always \\(2 p\\) where p is the number of parameters.\nThe is the basis for the \\(AIC\\), the Akaike Information Criteria\n\\[\nAIC = D_{train} + 2 \\cdot p = -2\\cdot lppd + 2 \\cdot p\n\\]\nAIC is used when\n\nPriors are flat or overwhelmed by the likelihood\nThe posterior distribution is approximately multivariate Gaussian\nThe sample size \\(N\\) is much greater than the number of parameters \\(k\\)\n\n\n7.4.2.1 DIC (Deviance Information criteria)\nIt assumes a posterior distribution is approximately multivariate Gaussian like AIC which means it can be very wrong if the distribution is skewed.\n\n\n7.4.2.2 WAIC (Widely Applicable Information Criteria)\nThe penalty term is based on \\(V(y_i)\\) which is the variance in log-likelihood for the observation \\(i\\) in the sample. (See section 6.4.1 of the first edition of McElreath)\n\\[\np_{WAIC} = \\sum_{i=1}^N{V(y_i)} = \\sum_{i=1}^N{var_{\\theta} \\log{p(y_i \\mid \\theta)}}\n\\]\n\\[\nWAIC = -2 (lppd - p_{WAIC})\n\\]\nThe penalty term is also called the effective number of parameters which is really not the right mathematical way of writing it. See discussion in section 7.4.2.\n\n\n\n7.4.3 Comparing CV, PSIS and WAIC\nPSIS and WAIC perform very similarly in the context of prdinary linear models.\nEstimation aside, PSIS has the distinct advantage of warning the user when it is unreliable."
  },
  {
    "objectID": "ch07_information.html#model-comparison",
    "href": "ch07_information.html#model-comparison",
    "title": "7  Ulysses’ Compass",
    "section": "7.5 Model comparison",
    "text": "7.5 Model comparison\n\nfn &lt;- list.files(path=here::here(\"cache\"), pattern=\"ch06_fit06_06_.*[.]rds$\")\nstopifnot(length(fn) == 1)\nfit06_06 &lt;- readRDS(here::here(\"cache\", fn))\nfn &lt;- list.files(path=here::here(\"cache\"), pattern=\"ch06_fit06_07_.*[.]rds$\")\nstopifnot(length(fn) == 1)\nfit06_07 &lt;- readRDS(here::here(\"cache\", fn))\nfn &lt;- list.files(path=here::here(\"cache\"), pattern=\"ch06_fit06_08_.*[.]rds$\")\nstopifnot(length(fn) == 1)\nfit06_08 &lt;- readRDS(here::here(\"cache\", fn))\n\n\n7.5.1 Model mis-selection\n\nfit06_07_waic &lt;- loo::waic(fit06_07)\n# can also use this function if you need to manipulate the data\nfit06_07_waic$estimates |&gt; round(digits = 4)\n\n           Estimate      SE\nelpd_waic -172.5276  6.5710\np_waic       3.6630  0.7004\nwaic       345.0553 13.1421\n\n\nThe waic is \\(-2 * elpd\\) which is not what Kurtz says in his version of the textbook. I think he is mistaken. McElreath on the other is consistent with his previous definitions.\n\nnear(fit06_07_waic$estimates[\"elpd_waic\", \"Estimate\"] * -2,\n     fit06_07_waic$estimates[\"waic\", \"Estimate\"])\n\n[1] TRUE\n\n\nand comparing the 3 models\n\nw &lt;- loo::loo_compare(fit06_06, fit06_07, fit06_08, criterion = \"waic\")\n\nand for more details\n\nprint(w, simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  \nfit06_07    0.0       0.0  -172.5       6.6          3.7    0.7     345.1\nfit06_08  -15.8       4.8  -188.4       6.3          2.7    0.5     376.7\nfit06_06  -25.8       6.3  -198.4       5.5          1.6    0.3     396.7\n         se_waic\nfit06_07   13.1 \nfit06_08   12.7 \nfit06_06   11.1 \n\n\nKurtz make some mistake in describing this data. Be careful when reading him. McElreath is more precise.\nTo get the corresponding WAIC we simply mutliply by -2\n\ncbind(waic_diff = w[, \"elpd_diff\"] * -2, waic_se = w[, \"se_diff\"] * 2)\n\n         waic_diff   waic_se\nfit06_07   0.00000  0.000000\nfit06_08  31.68303  9.630005\nfit06_06  51.69095 12.562092\n\n\nand we can also compare using loo which gives the same results.\n\nl &lt;- loo::loo_compare(fit06_06, fit06_07, fit06_08, criterion = \"loo\")\nprint(l, simplify = FALSE)\n\n         elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit06_07    0.0       0.0  -172.5      6.6         3.7    0.7    345.1   13.2  \nfit06_08  -15.8       4.8  -188.4      6.4         2.7    0.5    376.8   12.7  \nfit06_06  -25.8       6.3  -198.4      5.5         1.6    0.3    396.8   11.1  \n\n\n\nw &lt;- loo::loo_compare(fit06_06, fit06_07, fit06_08, criterion = \"waic\") |&gt; \n    data.frame() |&gt;\n    tibble::rownames_to_column(\"model_name\") |&gt;\n    mutate(model_name = forcats::fct_reorder(model_name, waic, .desc = TRUE))\nggplot(w, aes(x = waic, y = model_name, xmin = waic - se_waic, xmax = waic + se_waic)) +\n    geom_pointrange(shape = 19, linewidth = 2, fatten = 8, color = \"mediumseagreen\") +\n    ggrepel::geom_text_repel(aes(label = round(waic, 0))) +\n    labs(title = \"WAIC plot\", x = \"waic\", y = NULL)\n\n\n\n\nA last point about model comparison is that comparing the pointwise weights. See the useful comments by McElreath at the end of section 7.5.1.\n\\[\nw_i = \\frac{exp(-0.5 \\Delta_i)}{\\sum_j exp(-0.5 \\Delta_j)}\n\\]\nwhich can be obtained with brms::model_weights\n\nbrms::model_weights(fit06_06, fit06_07, fit06_08) |&gt;\n    round(digits = 4)\n\nfit06_06 fit06_07 fit06_08 \n  0.0213   0.9787   0.0000 \n\n\n\n\n7.5.2 Outliers and other illusions\n\ndata(\"WaffleDivorce\")\ndataWaffle &lt;- WaffleDivorce |&gt;\n    mutate(D = scale(as.vector(Divorce)),\n           M = scale(as.vector(Marriage)),\n           A = scale(as.vector(MedianAgeMarriage)))\n\n\nfn &lt;- list.files(path=here::here(\"cache\"), pattern=\"ch05_fit05_01_.*[.]rds$\")\nstopifnot(length(fn) == 1)\nfit05_01 &lt;- readRDS(here::here(\"cache\", fn))\nfn &lt;- list.files(path=here::here(\"cache\"), pattern=\"ch05_fit05_02_.*[.]rds$\")\nstopifnot(length(fn) == 1)\nfit05_02 &lt;- readRDS(here::here(\"cache\", fn))\nfn &lt;- list.files(path=here::here(\"cache\"), pattern=\"ch05_fit05_03_f.*[.]rds$\")\nstopifnot(length(fn) == 1)\nfit05_03 &lt;- readRDS(here::here(\"cache\", fn))\n\n\nl &lt;- loo::loo_compare(fit05_01, fit05_02, fit05_03, criterion = \"loo\")\nprint(l, simplify = FALSE)\n\n         elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic\nfit05_01   0.0       0.0   -62.1      6.3         2.9   1.6    124.2  12.7   \nfit05_03  -1.7       0.4   -63.8      6.3         4.7   1.8    127.5  12.7   \nfit05_02  -6.9       4.6   -69.0      5.0         2.4   0.9    138.0  10.0   \n\n\n\ndp &lt;- tibble(pareto_k = fit05_03$criteria$loo$diagnostics$pareto_k,\n       p_waic   = fit05_03$criteria$waic$pointwise[, \"p_waic\"],\n       Loc      = dataWaffle$Loc,\n       South = dataWaffle$South)\nggplot(dp, aes(x = pareto_k, y = p_waic, color = Loc == \"ID\")) +\n    geom_vline(xintercept = .5, linetype = 2, linewidth = 1, \n               color = \"magenta\") +\n    geom_point(aes(shape = Loc == \"ID\")) +\n    geom_text(data = . %&gt;% filter(p_waic &gt; 0.5),\n              aes(x = pareto_k - 0.03, label = Loc),\n              hjust = 1) +\n    scale_color_manual(values = c(\"darkgreen\", \"violetred\")) +\n    scale_shape_manual(values = c(19, 19)) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Gaussian model (fit05_03)\",\n         subtitle = deparse1(fit05_03$formula$formula),\n         caption = \"Different than McElreath but ok with Kurtz (same conclusions for both)\")\n\n\n\n\nTherefore we have ID which has too much influence. The solution is to use robust regression, a wonderful solution described by McElreath at the end of section 7.5.2.\nSee Kurtz on how to do it with brms as follows. We use \\(\\nu = 2\\), same as McElreath.\nMake sure you read McElreath and Kurtz on the t-distribution. A few things are important to remember. e.g. the parameter \\(\\sigma\\) is not the standard deviation in t-distribution.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb5.3t &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataWaffle, \n      family = student,\n      formula = bf(D ~ 1 + M + A, nu = 2),\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 1000, warmup = 500, chains = 4, cores = detectCores(),\n      seed = 5)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch07_b05_03t\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.21 sec elapsed\n\n\n\nsummary(b5.3t)\n\n Family: student \n  Links: mu = identity; sigma = identity; nu = identity \nFormula: D ~ 1 + M + A \n         nu = 2\n   Data: dataWaffle (Number of observations: 50) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.03      0.10    -0.16     0.21 1.00     1935     1379\nM             0.05      0.20    -0.32     0.48 1.00     1519     1207\nA            -0.70      0.15    -0.99    -0.40 1.00     1794     1304\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.58      0.09     0.42     0.76 1.00     1720     1339\nnu        2.00      0.00     2.00     2.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand now we have a more robust results, i.e. where the influence from ID and ME is lessened.\n\ndp &lt;- tibble(pareto_k = b5.3t$criteria$loo$diagnostics$pareto_k,\n       p_waic   = b5.3t$criteria$waic$pointwise[, \"p_waic\"],\n       Loc      = dataWaffle$Loc,\n       South = dataWaffle$South)\nggplot(dp, aes(x = pareto_k, y = p_waic, color = Loc == \"ID\")) +\n    geom_vline(xintercept = .5, linetype = 2, linewidth = 1, \n               color = \"magenta\") +\n    geom_point(aes(shape = Loc == \"ID\")) +\n    geom_text(data = . %&gt;% filter(Loc %in% c(\"ID\", \"ME\")),\n              aes(x = pareto_k - 0.01, label = Loc),\n              hjust = 1) +\n    scale_color_manual(values = c(\"darkgreen\", \"violetred\")) +\n    scale_shape_manual(values = c(19, 19)) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Student-t model (b5.3)\",\n         subtitle = \"using brms (see Kurtz)\",\n         caption = \"See Kurtz comments\")"
  },
  {
    "objectID": "ch07_information.html#summary",
    "href": "ch07_information.html#summary",
    "title": "7  Ulysses’ Compass",
    "section": "7.6 Summary",
    "text": "7.6 Summary\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://www.taylorandfrancis.com."
  },
  {
    "objectID": "ch08_interactions.html#building-an-interaction",
    "href": "ch08_interactions.html#building-an-interaction",
    "title": "8  Conditional Manatees",
    "section": "8.1 Building an interaction",
    "text": "8.1 Building an interaction\nLoad the data, log transform the gdp measure, remove incomplete cases and create a character column for Africa or Not Africa.\n\ndata(rugged)\ndataRugged &lt;- rugged |&gt;\n  filter(complete.cases(rgdppc_2000)) |&gt;\n  mutate(log_gdp = log(rgdppc_2000),\n         is_africa = if_else(cont_africa == 1, \"Africa\", \"Not Africa\"),\n         is_africa = as.factor(is_africa))\nrm(rugged)\n# NOTE: Make sure as.vector() is outside of scale().\n#       Otherwise it keeps the vector as an array and causes all sort of little\n#       problems.  In particular, a very obscure, fine error message\n#       in doing brms fit for b8.2.\ndataRugged_nona &lt;- dataRugged |&gt;\n  drop_na(rgdppc_2000) |&gt;\n  mutate(log_gdp_s = log_gdp / mean(log_gdp),\n         rugged_s = scales::rescale(rugged),\n         rugged_sc = as.vector(scale(rugged_s, center = TRUE, scale = FALSE)))\ndataRugged_nona |&gt;\n  select(log_gdp, log_gdp_s, rugged, rugged_s, rugged_sc) |&gt;\n  skim() |&gt;\n  select(-n_missing, -complete_rate) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = where(is.numeric), .fns = round, digits = 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\nData summary\n\n\nName\nselect(…)\n\n\nNumber of rows\n170\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlog_gdp\n8.52\n1.17\n6.15\n7.54\n8.58\n9.48\n10.96\n▃▆▇▆▃\n\n\nlog_gdp_s\n1.00\n0.14\n0.72\n0.89\n1.01\n1.11\n1.29\n▃▆▇▆▃\n\n\nrugged\n1.33\n1.17\n0.00\n0.44\n0.98\n1.96\n6.20\n▇▅▁▁▁\n\n\nrugged_s\n0.21\n0.19\n0.00\n0.07\n0.16\n0.32\n1.00\n▇▅▁▁▁\n\n\nrugged_sc\n0.00\n0.19\n-0.21\n-0.14\n-0.06\n0.10\n0.79\n▇▅▁▁▁\n\n\n\n\n\nand we use the following DAG, see overthinking box in introduction of section 8.1 for another possible DAG.\n\ndagRugged &lt;- list()\ndagRugged &lt;- within(dagRugged, {\n  coords &lt;- tibble(name = c(\"C\", \"G\", \"R\", \"U\"),\n                     x = c(3, 2, 1, 2),\n                     y = c(2, 2, 2, 1))\n  dag &lt;- dagify(G ~ C + R + U,\n                R ~ U,\n                latent = \"U\",\n                outcome = \"G\",\n                coords = coords)\n  p &lt;- dag |&gt;\n    ggdag_status(aes(color = status), as_factor = TRUE, node_size = 14,\n                 text_size = 4, text_col = \"midnightblue\") +\n    scale_color_paletteer_d(\"khroma::light\", \n                          na.value = \"honeydew3\",\n                          direction = 1) +\n    theme_dag() +\n    theme(legend.position = c(0.8, 0.2)) +\n    labs(title = \"African nations\", subtitle = \"Section 8.1\")\n})\ndagRugged$p\n\n\n\n\n\nplotRugged &lt;- list()\nplotRugged &lt;- within(plotRugged, {\n  Africa &lt;- dataRugged_nona |&gt;\n    filter(grepl(\"^africa$\", x = is_africa, ignore.case = TRUE)) |&gt;\n    ggplot(aes(x = rugged_s, y = log_gdp_s)) +\n    geom_smooth(method = \"lm\", formula = y ~ x, fill = \"lightblue\", color = \"royalblue\") +\n    geom_point(color = \"burlywood4\") +\n    labs(title = \"African nations\", x = \"ruggedness (rescale)\", \n         y = \"log GDP (prop of mean)\")\n  \n  notAfrica &lt;- dataRugged_nona |&gt;\n    filter(!grepl(\"^africa$\", x = is_africa, ignore.case = TRUE)) |&gt;\n    ggplot(aes(x = rugged_s, y = log_gdp_s)) +\n    geom_smooth(method = \"lm\", formula = y ~ x, fill = \"burlywood1\", color = \"burlywood4\") +\n    geom_point(color = \"royalblue\") +\n    labs(title = \"Non-African nations\", x = \"ruggedness (rescale)\",\n         y = \"log GDP (prop of mean)\")\n  title &lt;- \"Figure 8.2. Separate linear regressions inside and outside of Africa\"\n})\nwrap_plots(plotRugged[c(\"Africa\", \"notAfrica\")]) + \n  plot_annotation(title = plotRugged$title)\n\n\n\n\n\n8.1.1 Making a rugged model\nand split the data into countries from Africa and not.\n\n# lst &lt;- d |&gt;\n#   split(d$is_africa)\n# str(lst)\n\nand now creating a simple univariate model\n\\[\n\\begin{align*}\n\\log{(log\\_gdp\\_s_i)} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot rugged\\_sc_i \\\\\n\\alpha &\\sim \\mathcal{N}(1, 1) \\\\\n\\beta &\\sim \\mathcal{N}(0, 1) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nNow fit the model. Get the prior samples by using sample_prior = TRUE.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit08_01a &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataRugged_nona,\n    family = gaussian,\n    log_gdp_s ~ 1 + rugged_sc,\n    prior = c(\n      prior(normal(1, 1), class = Intercept),\n      prior(normal(0, 1), class = b),\n      prior(exponential(1), class = sigma)),\n    sample_prior = TRUE,\n    iter = 1000, warmup = 500, chains = 4, cores = detectCores(),\n    seed = 809)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_fit08_01a\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.12 sec elapsed\n\n\n\n# tictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\n# b8.1a &lt;- xfun::cache_rds({\n#   out &lt;- brm(\n#     data = dd,\n#     family = gaussian,\n#     log_gdp_s ~ 1 + rugged_sc,\n#     prior = c(\n#       prior(normal(1, 1), class = Intercept),\n#       prior(normal(0, 1), class = b),\n#       prior(exponential(1), class = sigma)),\n#     sample_prior = TRUE,\n#     iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n#     seed = 8)\n#   out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n#   file = \"ch08_b08_01a\")\n# tictoc::toc()\n\n\nposterior_summary(fit08_01a) |&gt;\n  round(digits = 3)\n\n                Estimate Est.Error   Q2.5  Q97.5\nb_Intercept        1.000     0.010  0.979  1.020\nb_rugged_sc        0.001     0.055 -0.105  0.108\nsigma              0.138     0.007  0.124  0.153\nprior_Intercept    1.050     0.988 -0.866  2.973\nprior_b            0.016     0.990 -1.957  1.984\nprior_sigma        1.030     1.032  0.031  3.798\nlprior            -1.978     0.008 -1.993 -1.963\nlp__              91.845     1.152 88.832 93.182\n\n\nthe estimates are described in section 8.1.1 of McElreath but he seems to have\n\nprior08_01a &lt;- list()\nprior08_01a &lt;- within(prior08_01a, {\n  draws &lt;- prior_draws(fit08_01a)\n  df &lt;- draws |&gt;\n    slice_sample(n = 50) |&gt;\n    tibble::rownames_to_column(var = \"id\") |&gt;\n    expand(nesting(id, Intercept, b), rugged_sc = c(-2, 2)) |&gt;\n    mutate(log_gdp_s = Intercept + b * rugged_sc,\n         rugged_s  = rugged_sc + mean(dataRugged_nona$rugged_s))\n  est &lt;- c(\"fixed\" = min(dataRugged_nona$log_gdp_s), \"b\" = diff(range(dataRugged_nona$log_gdp_s)))\n  \n  p &lt;- ggplot(df, aes(x = rugged_s, y = log_gdp_s, group = id)) +\n    geom_line(color = \"orchid\") +\n    geom_hline(yintercept = range(dataRugged_nona$log_gdp_s), \n               size = 1, linetype = 2, color = \"royalblue\") +\n    geom_abline(intercept = est[\"fixed\"], slope = est[\"b\"], color = \"purple\", size = 1) +\n    coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) +\n    labs(\n      subtitle = \"Intercept ~ dnorm(1, 1)\\nb ~ dnorm(0, 1)\",\n      x = \"ruggedness (rescaled)\",\n      y = \"log GDP (prop of mean)\")\n})\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# prior08_01a$p\n\nNow using the prior where we want the intercept to be around 1 with extremes from 0.8 to 1.2 (i.e. a mean of 1 and sd of 0.1) and the slope to have extremes about \\(\\pm 0.6\\), that is a mean of 0 with sd of 0.3 (i.e. 2 sd with sd = 3 from a mean of 0).\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit08_01b &lt;- xfun::cache_rds({\n  out &lt;- update(\n    fit08_01a,\n    newdata = dataRugged_nona,\n    prior = c(\n      prior(normal(1, 0.1), class = Intercept),\n      prior(normal(0, 0.3), class = b),\n      prior(exponential(1), class = sigma)),\n    sample_prior = TRUE,\n    seed = 809)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_fit08_01b\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.12 sec elapsed\n\n\n\nposterior_summary(fit08_01b) |&gt;\n  round(digits = 3)\n\n                Estimate Est.Error   Q2.5  Q97.5\nb_Intercept        1.000     0.011  0.980  1.023\nb_rugged_sc        0.002     0.055 -0.108  0.111\nsigma              0.138     0.008  0.124  0.155\nprior_Intercept    0.998     0.099  0.801  1.196\nprior_b            0.008     0.299 -0.558  0.593\nprior_sigma        0.993     0.998  0.027  3.781\nlprior             1.508     0.028  1.435  1.539\nlp__              95.237     1.297 91.928 96.698\n\n\n\nprior08_01b &lt;- list()\nprior08_01b &lt;- within(prior08_01b, {\n  draws &lt;- prior_draws(fit08_01b) |&gt;\n    mutate(.draw = seq_len(n())) |&gt;\n    relocate(.draw)\n  df &lt;- draws |&gt;\n    slice_sample(n = 50) |&gt;\n    expand(nesting(.draw, Intercept, b), rugged_sc = c(-2, 2)) |&gt;\n    mutate(log_gdp_s = Intercept + b * rugged_sc,\n         rugged_s  = rugged_sc + mean(dataRugged_nona$rugged_s))\n  est &lt;- c(\"fixed\" = min(dataRugged_nona$log_gdp_s), \"b\" = diff(range(dataRugged_nona$log_gdp_s)))\n  \n  p &lt;- ggplot(df, aes(x = rugged_s, y = log_gdp_s, group = .draw)) +\n    geom_line(color = \"orchid\") +\n    geom_hline(yintercept = range(dataRugged_nona$log_gdp_s), \n               size = 1, linetype = 2, color = \"royalblue\") +\n    geom_abline(intercept = est[\"fixed\"], slope = est[\"b\"], color = \"purple\", size = 1) +\n    coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) +\n    labs(\n      subtitle = \"Intercept ~ dnorm(1, 0.1)\\nb ~ dnorm(0, 0.3)\",\n      x = \"ruggedness (rescaled)\",\n      y = \"log GDP (prop of mean)\")\n})\n# prior08_01b$p\n\n\nprior08_01a$p + prior08_01b$p + \n  plot_annotation(\n    title = \"Figure 8.3. Simulating different priors to evaluate their fit\")\n\n\n\n\n\n\n8.1.2 Adding an indicator variable isn’t enough\nWe add the cid variable to identify the continent.\n\ndataRugged_nona &lt;- dataRugged_nona |&gt;\n  mutate(cid = as.factor(if_else(cont_africa == 1, \"1\", \"2\")))\n\nand fitting the data to the following model\n\\[\n\\begin{align*}\n\\log{(log\\_gdp\\_s_i)} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha[cid] + \\beta \\cdot rugged\\_sc_i \\\\\n\\alpha &\\sim \\mathcal{N}(1, 0.1) \\\\\n\\beta &\\sim \\mathcal{N}(0, 0.3) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit08_02 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataRugged_nona,\n    family = gaussian,\n    log_gdp_s ~ 0 + cid + rugged_sc,\n    prior = c(\n      prior(normal(1, 0.1), class = b, coef = cid1),\n      prior(normal(1, 0.1), class = b, coef = cid2),\n      prior(normal(0, 0.3), class = b, coef = rugged_sc),\n      prior(exponential(1), class = sigma)),\n    sample_prior = TRUE,\n    iter = 1000, warmup = 500, chains = 4, cores = detectCores(),\n    seed = 811)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_fit08_02\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.44 sec elapsed\n\n\n\nposterior_summary(fit08_02) |&gt;\n  round(digits = 3)\n\n                  Estimate Est.Error    Q2.5   Q97.5\nb_cid1               0.881     0.017   0.848   0.914\nb_cid2               1.049     0.011   1.027   1.070\nb_rugged_sc         -0.045     0.047  -0.136   0.046\nsigma                0.114     0.006   0.103   0.128\nprior_b_cid1         1.003     0.098   0.811   1.207\nprior_b_cid2         1.001     0.101   0.811   1.204\nprior_b_rugged_sc   -0.001     0.292  -0.579   0.571\nprior_sigma          0.988     0.983   0.025   3.671\nlprior               2.061     0.213   1.618   2.444\nlp__               128.095     1.530 124.266 129.975\n\n\nand measuring the models’ performance\n\nloo::loo_compare(fit08_01b, fit08_02, criterion = \"waic\") |&gt; \n  print(simplify = FALSE)\n\n          elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  \nfit08_02     0.0       0.0   125.9       7.4          4.3    0.8    -251.9\nfit08_01b  -31.6       7.3    94.3       6.5          2.6    0.3    -188.7\n          se_waic\nfit08_02    14.8 \nfit08_01b   13.0 \n\n\nwith thew model weights\n\nbrms::model_weights(fit08_01b, fit08_02) |&gt;\n  round(digits = 2)\n\nWarning: Some Pareto k diagnostic values are slightly high. See help('pareto-k-diagnostic') for details.\n\n\nfit08_01b  fit08_02 \n     0.03      0.97 \n\n\nand create the fitted data used for the plot\n\ntidybayes::get_variables(fit08_02)\n\n [1] \"b_cid1\"            \"b_cid2\"            \"b_rugged_sc\"      \n [4] \"sigma\"             \"prior_b_cid1\"      \"prior_b_cid2\"     \n [7] \"prior_b_rugged_sc\" \"prior_sigma\"       \"lprior\"           \n[10] \"lp__\"              \"accept_stat__\"     \"stepsize__\"       \n[13] \"treedepth__\"       \"n_leapfrog__\"      \"divergent__\"      \n[16] \"energy__\"         \n\n\n\n# get the fitted values\nlpred08_02 &lt;- crossing(\n  cid = as.factor(1:2),\n  rugged_sc = seq(from = -0.2, to = 1.2, length.out = 30)) |&gt;\n  mutate(rugged_sc = as.vector(scale(rugged_sc))) |&gt;\n  add_linpred_draws(fit08_02, ndraws = 50) |&gt;\n  mean_qi() |&gt;\n  mutate(is_africa = if_else(cid == 1, \"Africa\", \"Not Africa\")) |&gt;\n  mutate(is_africa = as.factor(is_africa))\n# glimpse(lpred08_02)\n\n\nggplot(dataRugged_nona, aes(x = rugged_sc, y = log_gdp_s, fill = is_africa, color = is_africa)) +\n  geom_smooth(data = lpred08_02, aes(x = rugged_sc, y = .linpred, ymin = .lower, ymax = .upper),\n              stat = \"identity\",\n              alpha = 1/4, size = 1/2) +\n  geom_point(size = 1) +\n  scale_fill_manual(values = c(\"Africa\" = \"springgreen2\", \"Not Africa\" = \"violet\")) +\n  scale_color_manual(values = c(\"Africa\" = \"springgreen2\", \"Not Africa\" = \"violet\")) +\n  coord_cartesian(xlim = c(0, 1)) +\n  theme(legend.position = c(.80, .90),\n        legend.title = element_blank()) +\n  labs(title = \"Figure 8.4\",\n       subtitle = \"model b8.2\",\n       x = \"ruggedness (standardized)\",\n       y = \"log GDP (as proportion of mean)\")\n\n\n\n\n\n\n8.1.3 Adding an interaction does work\n\\[\n\\begin{align*}\n\\log{(log\\_gdp\\_s_i)} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{[cid]} + \\beta_{[cid]} \\cdot rugged\\_sc_i \\\\\n\\alpha &\\sim \\mathcal{N}(1, 0.1) \\\\\n\\beta &\\sim \\mathcal{N}(0, 0.3) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit08_03 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataRugged_nona,\n      family = gaussian,\n      formula = bf(log_gdp_s ~ 0 + a + b * rugged_sc,\n         a ~ 0 + cid,\n         b ~ 0 + cid,\n         nl = TRUE),\n      prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a),\n                prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a),\n                prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b),\n                prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b),\n                prior(exponential(1), class = sigma)),\n      iter = 1000, warmup = 500, chains = 4, cores = detectCores(),\n      seed = 821)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_fit08_03\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.18 sec elapsed\n\n\n\nposterior_summary(fit08_03) |&gt;\n  round(digits = 3)\n\n         Estimate Est.Error    Q2.5   Q97.5\nb_a_cid1    0.887     0.016   0.857   0.918\nb_a_cid2    1.051     0.010   1.031   1.070\nb_b_cid1    0.134     0.076  -0.020   0.279\nb_b_cid2   -0.143     0.057  -0.251  -0.033\nsigma       0.111     0.006   0.100   0.125\nlprior      2.180     0.226   1.702   2.593\nlp__      132.255     1.604 128.500 134.378\n\n\n\nloo::loo_compare(fit08_01b, fit08_02, fit08_03, criterion = \"waic\") |&gt; \n  print(simplify = FALSE)\n\n          elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  \nfit08_03     0.0       0.0   129.6       7.4          5.0    0.9    -259.2\nfit08_02    -3.6       3.3   125.9       7.4          4.3    0.8    -251.9\nfit08_01b  -35.2       7.5    94.3       6.5          2.6    0.3    -188.7\n          se_waic\nfit08_03    14.7 \nfit08_02    14.8 \nfit08_01b   13.0 \n\n\nwith thew model weights\n\nbrms::model_weights(fit08_01b, fit08_02, fit08_03) |&gt;\n  round(digits = 2)\n\nWarning: Some Pareto k diagnostic values are slightly high. See help('pareto-k-diagnostic') for details.\n\nWarning: Some Pareto k diagnostic values are slightly high. See help('pareto-k-diagnostic') for details.\n\n\nfit08_01b  fit08_02  fit08_03 \n     0.00      0.12      0.88 \n\n\n\n\n8.1.4 Plotting the interaction\n\nlpred08_03 &lt;- crossing(\n  cid = as.factor(1:2),\n  rugged_sc = seq(from = -0.2, to = 1.2, length.out = 30)) |&gt;\n  mutate(rugged_sc = as.vector(scale(rugged_sc))) |&gt;\n  add_linpred_draws(fit08_03, ndraws = 50) |&gt;\n  mean_qi() |&gt;\n  mutate(is_africa = if_else(cid == 1, \"Africa\", \"Not Africa\")) |&gt;\n  mutate(is_africa = as.factor(is_africa))\n# glimpse(lpred08_03)\n\n\nggplot(dataRugged_nona, aes(x = rugged_sc, y = log_gdp_s, fill = is_africa, color = is_africa)) +\n  geom_smooth(data = lpred08_03, aes(x = rugged_sc, y = .linpred, ymin = .lower, ymax = .upper),\n              stat = \"identity\",\n              alpha = 1/4, size = 1/2) +\n  geom_point(size = 1) +\n  scale_fill_manual(values = c(\"Africa\" = \"springgreen2\", \"Not Africa\" = \"violet\")) +\n  scale_color_manual(values = c(\"Africa\" = \"springgreen2\", \"Not Africa\" = \"violet\")) +\n  coord_cartesian(xlim = c(0, 1)) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Figure 8.5\",\n       subtitle = \"model b8.3\",\n       x = \"ruggedness (standardized)\",\n       y = \"log GDP (as proportion of mean)\") +\n  facet_wrap(~ is_africa)"
  },
  {
    "objectID": "ch08_interactions.html#symmetry-of-interactions",
    "href": "ch08_interactions.html#symmetry-of-interactions",
    "title": "8  Conditional Manatees",
    "section": "8.2 Symmetry of interactions",
    "text": "8.2 Symmetry of interactions"
  },
  {
    "objectID": "ch08_interactions.html#continuous-interactions",
    "href": "ch08_interactions.html#continuous-interactions",
    "title": "8  Conditional Manatees",
    "section": "8.3 Continuous interactions",
    "text": "8.3 Continuous interactions\n\n8.3.1 A winter flower\n\ndata(tulips, package = \"rethinking\")\ndataTulips &lt;- tulips |&gt;\n  mutate(blooms_r = scales::rescale(blooms),\n         water_c = as.vector(scale(water, scale = FALSE)),\n         shade_c = as.vector(scale(shade, scale = FALSE)))\nrm(tulips)\n\n\n\n8.3.2 The models\n\n8.3.2.1 Calibrating the priors\nOur preliminary model, as a first jest in terms of prior is\n\\[\n\\begin{align*}\nblooms\\_r_i = blooms - \\max(blooms)  &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_W \\cdot (water - \\overline{water})+ \\beta_S \\cdot (shade - \\overline{shade}) \\\\\n&= \\alpha + \\beta_W \\cdot water\\_c_i+ \\beta_S \\cdot shade\\_c_i\n\\alpha &\\sim \\mathcal{N}(0.5, 1) \\\\\n\\beta_W &\\sim \\mathcal{N}(0, 1) \\\\\n\\beta_S &\\sim \\mathcal{N}(0, 1) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nWhen looking at the data with skim() to evaluate the priors we obtain\n\nskim(dataTulips) |&gt;\n  select(-n_missing, -complete_rate) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n\nData summary\n\n\nName\ndataTulips\n\n\nNumber of rows\n27\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\nskim_variable\nordered\nn_unique\ntop_counts\n\n\n\n\nbed\nFALSE\n3\na: 9, b: 9, c: 9\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nwater\n2.00\n0.83\n1\n1.00\n2.00\n3.00\n3.00\n▇▁▇▁▇\n\n\nshade\n2.00\n0.83\n1\n1.00\n2.00\n3.00\n3.00\n▇▁▇▁▇\n\n\nblooms\n128.99\n92.68\n0\n71.12\n111.04\n190.30\n361.66\n▅▇▂▂▁\n\n\nblooms_r\n0.36\n0.26\n0\n0.20\n0.31\n0.53\n1.00\n▅▇▂▂▁\n\n\nwater_c\n0.00\n0.83\n-1\n-1.00\n0.00\n1.00\n1.00\n▇▁▇▁▇\n\n\nshade_c\n0.00\n0.83\n-1\n-1.00\n0.00\n1.00\n1.00\n▇▁▇▁▇\n\n\n\n\n\nWe see that blooms_r must be between 0 and 1. The prior used assign most probability outside of that range\n\nparam &lt;- c(\"mean\" = 0.5, \"sd\" = 1)\npnorm(q = -1, mean = param[\"mean\"], sd = param[\"sd\"]) + \n  pnorm(q = 1, mean = param[\"mean\"], sd = param[\"sd\"], lower.tail = FALSE)\n\n[1] 0.3753447\n\n\nlets say that we we want only 5% of the values outside the range (2.5% on each side) then, going with trial an error, the boundaries would be about\n\nparam &lt;- c(\"mean\" = 0.5, \"sd\" = 0.25)\npnorm(q = -1, mean = param[\"mean\"], sd = param[\"sd\"]) + \n  pnorm(q = 1, mean = param[\"mean\"], sd = param[\"sd\"], lower.tail = FALSE)\n\n[1] 0.02275013\n\n\nTherefore we will use\n\\[\n\\alpha \\sim \\mathcal{N}(0.5, 0.25)\n\\] and since the range for water_c and shade_c is -1 to 1 then we can use the same logic for both as follows\n\nparam &lt;- c(\"mean\" = 0, \"sd\" = 0.25)\npnorm(q = -1, mean = param[\"mean\"], sd = param[\"sd\"]) + \n  pnorm(q = 1, mean = param[\"mean\"], sd = param[\"sd\"], lower.tail = FALSE)\n\n[1] 6.334248e-05\n\n\nwhich means virtually almost all values will be between -1 and 1. When looking at the skim() summary we see that there are many extreme values so this prior covers this situation well.\nTherefore, our model with a little more informative priors is\n\\[\n\\begin{align*}\nblooms\\_r_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_W \\cdot water\\_c_i+ \\beta_S \\cdot shade\\_c_i \\\\\n\\alpha &\\sim \\mathcal{N}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\mathcal{N}(0, 0.25) \\\\\n\\beta_S &\\sim \\mathcal{N}(0, 0.25) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand we fit that model to the data\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit08_04 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataTulips,\n      family = gaussian,\n      formula = blooms_r ~ 1 + water_c + shade_c,\n      prior = c(prior(normal(0.5, 0.25), class = Intercept),\n                prior(normal(0, 0.25), class = b, coef = water_c),\n                prior(normal(0, 0.25), class = b, coef = shade_c),\n                prior(exponential(1), class = sigma)),\n      iter = 1000, warmup = 500, chains = 2, cores = detectCores(),\n      seed = 823)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_fit08_04\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.11 sec elapsed\n\n\n\nposterior_summary(fit08_04) |&gt;\n  round(digits = 3)\n\n            Estimate Est.Error   Q2.5  Q97.5\nb_Intercept    0.360     0.033  0.298  0.428\nb_water_c      0.205     0.040  0.125  0.280\nb_shade_c     -0.111     0.042 -0.190 -0.034\nsigma          0.176     0.026  0.133  0.231\nlprior         0.600     0.180  0.225  0.916\nlp__           8.108     1.396  4.684  9.918\n\n\n\n\n8.3.2.2 Adding an interaction\nUsing the notation \\(\\gamma_{W, i} = \\beta_W+\\beta_{WS} \\cdot shade\\_c_i\\) we get the new model with interactions\n\\[\n\\begin{align*}\nblooms\\_r_i & \\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i & =\n\\alpha + \\gamma_{W, i} \\cdot water\\_c_i+ \\beta_S \\cdot shade\\_c_i \\\\\n&= \\alpha + \\beta_W \\cdot water\\_c_i + \\beta_S \\cdot shade\\_c_i + \\beta_{WS} \\cdot shade\\_c_i \\cdot water\\_c_i\n\\\\\n\\alpha & \\sim \\mathcal{N}(0.5, 0.25) \\\\\n\\beta_W & \\sim \\mathcal{N}(0, 0.25) \\\\\n\\beta_S & \\sim \\mathcal{N}(0, 0.25) \\\\\n\\beta_{WS} & \\sim \\mathcal{N}(0, 0.25) \\\\\n\\sigma & \\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand now fitting the model with interaction\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit08_05 &lt;- xfun::cache_rds({\n  out &lt;- update(fit08_04,\n                newdata = dataTulips,\n                formula. = blooms_r ~ 1 + water_c + shade_c + water_c:shade_c,\n                prior = c(prior(normal(0.5, 0.25), class = Intercept),\n                  prior(normal(0, 0.25), class = b, coef = water_c),\n                  prior(normal(0, 0.25), class = b, coef = shade_c),\n                  prior(normal(0, 0.25), class = b, coef = water_c:shade_c),\n                  prior(exponential(1), class = sigma)))\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_fit08_05\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.14 sec elapsed\n\n\n\nposterior_summary(fit08_05) |&gt;\n  round(digits = 3)\n\n                  Estimate Est.Error   Q2.5  Q97.5\nb_Intercept          0.359     0.028  0.303  0.416\nb_water_c            0.206     0.035  0.134  0.271\nb_shade_c           -0.112     0.034 -0.174 -0.047\nb_water_c:shade_c   -0.143     0.040 -0.222 -0.060\nsigma                0.143     0.022  0.107  0.189\nlprior               0.929     0.171  0.556  1.231\nlp__                13.938     1.679  9.944 16.293\n\n\n\n\n\n8.3.3 Plotting continuous interactions\n\nplot08_04post &lt;- list()\nplot08_04post &lt;- within(plot08_04post, {\n  draws &lt;- data.frame(water_c = -1:1, shade_c = -1:1) |&gt;\n    expand(water_c, shade_c) |&gt;\n    add_linpred_draws(fit08_04, ndraws = 20) |&gt;\n    mutate(label = paste(\"shade_c =\", shade_c)) |&gt;\n    identity()\n  \n  intrvl &lt;- draws |&gt;\n    select(-label) |&gt;\n    mean_qi() |&gt;\n    mutate(.draw = 0,\n           label = paste(\"shade_c =\", shade_c))\n  \n  p &lt;- ggplot(draws, aes(x = water_c, y = .linpred, group = .draw)) +\n    geom_line(color = \"orchid\") +\n    geom_line(data = intrvl, aes(x = water_c, y = .linpred, group = .draw), \n              color = \"darkgreen\", linewidth = 1) +\n    geom_hline(\n      data = data.frame(y = c(0, 1)), aes(yintercept = y),\n      size = 1, linetype = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = c(-1, 1), ylim = c(-0.5, 1.5)) +\n    facet_wrap(. ~ label, nrow = 1) +\n  labs(\n    subtitle = \"Model 8.4 NO INTERACTIONS\\nIntercept ~ dnorm(0.5, 0.25)\\nb ~ dnorm(0, 0.25)\",\n    x = \"water (centered)\",\n    y = \"bloom (rescaled)\")\n})\nplot08_04post$p\n\n\n\n\n\nplot08_05post &lt;- list()\nplot08_05post &lt;- within(plot08_05post, {\n  draws &lt;- data.frame(water_c = -1:1, shade_c = -1:1) |&gt;\n    expand(water_c, shade_c) |&gt;\n    add_linpred_draws(fit08_05, ndraws = 20) |&gt;\n    mutate(label = paste(\"shade_c =\", shade_c))\n  \n  intrvl &lt;- draws |&gt;\n    select(-label) |&gt;\n    mean_qi() |&gt;\n    mutate(.draw = 0,\n           label = paste(\"shade_c =\", shade_c))\n  \n  p &lt;- ggplot(draws, aes(x = water_c, y = .linpred, group = .draw)) +\n    geom_line(color = \"orchid\") +\n    geom_line(data = intrvl, aes(x = water_c, y = .linpred, group = .draw), \n              color = \"darkgreen\", linewidth = 1) +\n    geom_hline(\n      data = data.frame(y = c(0, 1)), aes(yintercept = y),\n      size = 1, linetype = 2, color = \"royalblue\") +\n  # geom_abline(intercept = est[\"fixed\"], slope = est[\"b\"], color = \"purple\", size = 1) +\n  coord_cartesian(xlim = c(-1, 1), ylim = c(-0.5, 1.5)) +\n    facet_wrap(. ~ label, nrow = 1) +\n  labs(\n    subtitle = \"Model 8.5 WITH INTERACTIONS\\nIntercept ~ dnorm(0.5, 0.25)\\nb ~ dnorm(0, 0.25)\",\n    x = \"water (centered)\",\n    y = \"bloom (rescaled)\")\n})\n# plot08_05post$intrvl\nplot08_05post$p\n\n\n\n\n\nplot08_04post$p / plot08_05post$p +\n  plot_annotation(title = \"Tryptich plot of predicted bloom by level of shade (-1, 0, 1)\")\n\n\n\n\n\n\n8.3.4 Plotting prior predictions\n\n\n\n\n\n\nWarning\n\n\n\nThe plot in this section are different than what McElreath and Kurz have. Yet, McElreath”s are different than Kurz’s, so there doesn’t seem to be a consensus between the 2. I keep the plots below as I don’t see anything wrong with them.\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"20 secs.\"))\nfit08_04prior &lt;- xfun::cache_rds({\n  out &lt;- update(fit08_04,\n                sample_prior = \"only\")\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_fit08_04prior\")\ntictoc::toc()\n\nrun time of 20 secs., use the cache.: 0.14 sec elapsed\n\n\n\nposterior_summary(fit08_04prior)|&gt;\n  round(digits = 3)\n\n            Estimate Est.Error   Q2.5 Q97.5\nb_Intercept    0.496     0.251  0.018 0.976\nb_water_c     -0.015     0.251 -0.492 0.466\nb_shade_c     -0.002     0.237 -0.454 0.457\nsigma          0.985     0.963  0.018 3.452\nlprior        -1.040     1.499 -4.755 0.987\nlp__          -1.643     1.444 -5.345 0.151\n\n\n\nplot08_04prior &lt;- list()\nplot08_04prior &lt;- within(plot08_04prior, {\n  draws &lt;- data.frame(water_c = -1:1, shade_c = -1:1) |&gt;\n    expand(water_c, shade_c) |&gt;\n    add_linpred_draws(fit08_04prior, ndraws = 20) |&gt;\n    mutate(label = paste(\"shade_c =\", shade_c))\n  \n  intrvl &lt;- draws |&gt;\n    select(-label) |&gt;\n    mean_qi() |&gt;\n    mutate(.draw = 0,\n           label = paste(\"shade_c =\", shade_c))\n  \n  p &lt;- ggplot(draws, aes(x = water_c, y = .linpred, group = .draw)) +\n    geom_line(color = \"orchid\") +\n    geom_line(data = intrvl, aes(x = water_c, y = .linpred, group = .draw), \n              color = \"darkgreen\", linewidth = 1) +\n    geom_hline(\n      data = data.frame(y = c(0, 1)), aes(yintercept = y),\n      size = 1, linetype = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = c(-1, 1), ylim = c(-0.5, 1.5)) +\n    facet_wrap(. ~ label, nrow = 1) +\n  labs(\n    subtitle = \"Model 8.4 WITHOUT INTERACTIONS\\nIntercept ~ dnorm(0.5, 0.25)\\nb ~ dnorm(0, 0.25)\",\n    x = \"water (centered)\",\n    y = \"bloom (rescaled)\")\n})\n# plot08_04prior$intrvl\nplot08_04prior$p\n\n\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"20 secs.\"))\nfit08_05prior &lt;- xfun::cache_rds({\n  out &lt;- update(fit08_05,\n                sample_prior = \"only\")\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_fit08_05prior\")\ntictoc::toc()\n\nrun time of 20 secs., use the cache.: 0.17 sec elapsed\n\n\n\nposterior_summary(fit08_05prior)|&gt;\n  round(digits = 3)\n\n                  Estimate Est.Error   Q2.5 Q97.5\nb_Intercept          0.499     0.245  0.017 0.979\nb_water_c            0.004     0.256 -0.505 0.488\nb_shade_c           -0.009     0.255 -0.516 0.492\nb_water_c:shade_c    0.005     0.247 -0.475 0.472\nsigma                1.009     0.997  0.028 3.844\nlprior              -1.154     1.740 -5.299 1.236\nlp__                -1.702     1.597 -5.408 0.480\n\n\n\nplot08_05prior &lt;- list()\nplot08_05prior &lt;- within(plot08_05prior, {\n  draws &lt;- data.frame(water_c = -1:1, shade_c = -1:1) |&gt;\n    expand(water_c, shade_c) |&gt;\n    add_linpred_draws(fit08_05prior, ndraws = 20) |&gt;\n    mutate(label = paste(\"shade_c =\", shade_c))\n  \n  intrvl &lt;- draws |&gt;\n    select(-label) |&gt;\n    mean_qi() |&gt;\n    mutate(.draw = 0,\n           label = paste(\"shade_c =\", shade_c))\n  \n  p &lt;- ggplot(draws, aes(x = water_c, y = .linpred, group = .draw)) +\n    geom_line(color = \"orchid\") +\n    geom_line(data = intrvl, aes(x = water_c, y = .linpred, group = .draw), \n              color = \"darkgreen\", linewidth = 1) +\n    geom_hline(\n      data = data.frame(y = c(0, 1)), aes(yintercept = y),\n      size = 1, linetype = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = c(-1, 1), ylim = c(-0.5, 1.5)) +\n    facet_wrap(. ~ label, nrow = 1) +\n  labs(\n    subtitle = \"Model 8.5 WITH INTERACTIONS\\nIntercept ~ dnorm(0.5, 0.25)\\nb ~ dnorm(0, 0.25)\",\n    x = \"water (centered)\",\n    y = \"bloom (rescaled)\")\n})\n# plot08_05prior$intrvl\nplot08_05prior$p\n\n\n\n\n\nplot08_04prior$p / plot08_05prior$p +\n  plot_annotation(title = \"Tryptich plot of prior predicted bloom by level of shade (-1, 0, 1)\")"
  },
  {
    "objectID": "ch08_interactions.html#summary",
    "href": "ch08_interactions.html#summary",
    "title": "8  Conditional Manatees",
    "section": "8.4 Summary",
    "text": "8.4 Summary"
  },
  {
    "objectID": "ch09_mcmc.html#good-king-markov",
    "href": "ch09_mcmc.html#good-king-markov",
    "title": "9  Markov Chain Monte Carlo",
    "section": "9.1 Good King Markov",
    "text": "9.1 Good King Markov\nWe define the algorithm to simulate the King’s journey.\n\nplotKingMarkov &lt;- list()\nplotKingMarkov &lt;- within(plotKingMarkov, {\n  positions &lt;- integer(1e4)\n  current &lt;- 10\n  for (i in seq_len(length(positions))) {\n  \n  # Step 0: record current position\n  positions[i] &lt;- current\n  \n  # step 1: flip a coin\n  set.seed(9 * i)\n  coin &lt;- sample(x = c(-1, 1), size = 1)\n  # step 2: nominate the proposal island\n  #         we use modulo arithmetic to simulate a clock\n  #         constant 1 substracted and added to obtain 10 instead of 0\n  proposal &lt;- (positions[i] + coin - 1) %% 10 + 1\n  # step 3: count shells and stones\n  #         count of shells = proposal, count of stone = current\n  # step 4: prob of moving\n  prob_move &lt;- proposal / positions[i]\n  current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current)\n  }\n  # the itinerary dataframe\n  itinerary &lt;- data.frame(\n    week = seq_along(positions),\n    island = factor(positions, levels = 1:10, ordered = TRUE))\n  set.seed(907)\n  dp &lt;- itinerary |&gt;\n    arrange(week) |&gt;\n    slice_head(n = 250)\n  p1 &lt;- ggplot(data = dp, aes(x = week, y = island)) +\n    geom_point(aes(color = island), size = 1) +\n    scale_color_paletteer_d(palette = \"ggsci::category10_d3\") +\n    theme(legend.position = \"none\") +\n    labs(subtitle = sprintf(\"Itinerary for the first %d weeks\", nrow(dp)),\n       x = \"week #\", y = \"island\")\n  p2 &lt;- ggplot(data = itinerary, aes(x = island)) +\n    geom_bar(aes(fill = island), stat = \"count\") +\n    scale_fill_paletteer_d(palette = \"ggsci::category10_d3\") +\n    theme(legend.position = \"none\") +\n    labs(subtitle = sprintf(\"%d weeks\", nrow(itinerary)),\n        x = \"island\", y = \"nb of weeks\")\n})\nwrap_plots(plotKingMarkov[c(\"p1\", \"p2\")]) +\n  plot_annotation(title = \"Figure 9.2\", subtitle = \"Metropolis algorithm\")"
  },
  {
    "objectID": "ch09_mcmc.html#metropolis-algorithms",
    "href": "ch09_mcmc.html#metropolis-algorithms",
    "title": "9  Markov Chain Monte Carlo",
    "section": "9.2 Metropolis algorithms",
    "text": "9.2 Metropolis algorithms\n\n9.2.1 Gibbs sampling\nIt is a variant of the Metropolis-Hasting algorithm that is more efficient and uses pairs of conjugate prior and likelihood distributions.\nIt is the basis for the sofware BUGS (Bayesian inference using Gibbs Sampling) and JAGS (Just Another Gibbs Sampler)\n\n\n9.2.2 High-dimensional problems\nThe code for this section comes straight from the same section in Kurz (2020). Many thanks to Solomon Kurtz for this wonderful gift.\nThe core issue with high-demensional problems is that parameters will end up having high-correlations which causes the algorithm to get stuck.\nMcElreath explains it by fire explaining the problem of high correlations itself then how high-dimensionality leads unavoidably to high corrrelations.\n\n9.2.2.1 The problem of high correlations\nTo illustrate a bivariate distribution with strong negative autocorrelation of -0.9 is used\n\\[\n\\begin{align*}\n\\begin{bmatrix}\na_1 \\\\\na_2\n\\end{bmatrix}\n&\\sim\n\\mathcal{MVNormal}(\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix},\n\\Sigma\n) \\\\\n\\Sigma &= \\mathbf{SRS} \\\\\n\\mathbf{S} &=\n\\begin{bmatrix}\n0.22 & 0 \\\\\n0 & 0.22\n\\end{bmatrix} \\\\\n\\mathbf{R} &=\n\\begin{bmatrix}\n1 & -0.9 \\\\\n-0.9 & 1\n\\end{bmatrix}\n\\end{align*}\n\\]\nwe create the contour of \\(x\\) and \\(y\\) values and their bivariate density. See https://stackoverflow.com/questions/36221596/plot-multivariate-gaussian-contours-with-ggplot2 for reference.\n\nsimMetropolis &lt;- list()\nsimMetropolis &lt;- within(simMetropolis, {\n  # Create the Multivariate distribution matrix\n  mu &lt;- c(0, 0)\n  sd_a1 &lt;- 0.22\n  sd_a2 &lt;- 0.22\n  rho &lt;- -0.9\n  S &lt;- matrix(c(sd_a1, 0, 0, sd_a2), nrow = 2, byrow = TRUE)\n  R &lt;- matrix(c(1, rho, rho, 1), nrow = 2, byrow = TRUE)\n  Sigma &lt;- S %*% R %*% S\n  domain &lt;- data.frame(\n    x = seq(from = -1.6, to = 1.6, length.out = 200),\n    y = seq(from = -1.6, to = 1.6, length.out = 200)) |&gt;\n    tidyr::expand(x, y)\n  df &lt;- domain |&gt;\n    mutate(prob = mvtnorm::dmvnorm(x = as.matrix(domain), mean = mu, sigma = Sigma))\n})\n# simMetropolis$df\n\nthen create the basic contour map\n\nplotMetropolis &lt;- list()\nplotMetropolis &lt;- within(plotMetropolis, {\n  contour &lt;- ggplot(simMetropolis$df, aes(x = x, y = y, z = prob)) + \n  geom_contour(aes(color = after_stat(level)), breaks = 9^(-(10 * 1:25))) +\n  scale_color_paletteer_c(\"grDevices::Emrld\", direction = -1)\n})\n# plotMetropolis$contour\n\nDefine a function to implement the Metropolis algorithm. This is a copy from the same section in kurtz2020b.\n\nfunMetropolis &lt;- function(mu, Sigma, num_proposals,\n                       step_size,\n                       starting_point) {\n  \n  # Initialize vectors where we will keep track of relevant\n  candidate_x_history &lt;- rep(-Inf, num_proposals)\n  candidate_y_history &lt;- rep(-Inf, num_proposals)\n  did_move_history &lt;- rep(FALSE, num_proposals)\n  \n  # Prepare to begin the algorithm...\n  current_point &lt;- starting_point\n  \n  for(i in 1:num_proposals) {\n    \n    # \"Proposals are generated by adding random Gaussian noise\n    # to each parameter\"\n    \n    noise &lt;- rnorm(n = 2, mean = 0, sd = step_size)\n    candidate_point &lt;- current_point + noise\n    \n    # store coordinates of the proposal point\n    candidate_x_history[i] &lt;- candidate_point[1]\n    candidate_y_history[i] &lt;- candidate_point[2]\n    \n    # evaluate the density of our posterior at the proposal point\n    candidate_prob &lt;- mvtnorm::dmvnorm(candidate_point, mean = mu, sigma = Sigma)\n    \n    # evaluate the density of our posterior at the current point\n    current_prob &lt;- mvtnorm::dmvnorm(current_point, mean = mu, sigma = Sigma)\n    \n    # Decide whether or not we should move to the candidate point\n    acceptance_ratio &lt;- candidate_prob / current_prob\n    should_move &lt;- ifelse(runif(n = 1) &lt; acceptance_ratio, TRUE, FALSE)\n    \n    # Keep track of the decision\n    did_move_history[i] &lt;- should_move\n    \n    # Move if necessary\n    if(should_move) {\n      current_point &lt;- candidate_point\n    }\n  }\n  \n  # once the loop is complete, store the relevant results in a tibble\n  results &lt;- tibble::tibble(\n    candidate_x = candidate_x_history,\n    candidate_y = candidate_y_history,\n    accept = did_move_history\n  )\n  \n  # compute the \"acceptance rate\" by dividing the total number of \"moves\"\n  # by the total number of proposals\n  \n  number_of_moves &lt;- results %&gt;% dplyr::pull(accept) %&gt;% sum(.)\n  acceptance_rate &lt;- number_of_moves/num_proposals\n  \n  return(list(results = results, acceptance_rate = acceptance_rate))\n  \n}\n\nand run the algorithm with step size = 0.1\n\nsimMetropolis &lt;- within(simMetropolis, {\n  set.seed(9)\n  round_1 &lt;- funMetropolis(mu = mu, Sigma = Sigma, num_proposals = 50,\n                      step_size = 0.1,\n                      starting_point = c(-1,1))\n})\n# glimpse(simMetropolis$round_1)\n\n\nplotMetropolis &lt;- within(plotMetropolis, {\n  p1 &lt;- contour + \n    geom_point(data = simMetropolis$round_1$results,\n             aes(x = candidate_x, y = candidate_y, shape = accept, \n                 fill = accept), \n             inherit.aes = FALSE) +\n  scale_shape_manual(values = c(21, 21)) +\n  scale_fill_manual(values = c(\"FALSE\" = \"red\", \"TRUE\" = \"green\")) +\n  theme(legend.position = \"none\") + \n  labs(title = \"Round # 1\",\n       subtitle = paste(\"step size 0.1, accept rate\", \n                        simMetropolis$round_1$acceptance_rate),\n       x = \"a1\",\n       y = \"a2\")\n})\n# plotMetropolis$p1\n\nand for round # 2\n\nsimMetropolis &lt;- within(simMetropolis, {\n  set.seed(9)\n  round_2 &lt;- funMetropolis(mu = mu, Sigma = Sigma, num_proposals = 50,\n                      step_size = 0.25,\n                      starting_point = c(-1,1))\n})\n# glimpse(simMetropolis$round_2)\n\n\nplotMetropolis &lt;- within(plotMetropolis, {\n  p2 &lt;- contour + \n    geom_point(data = simMetropolis$round_2$results,\n             aes(x = candidate_x, y = candidate_y, shape = accept, \n                 fill = accept), \n             inherit.aes = FALSE) +\n  scale_shape_manual(values = c(21, 21)) +\n  scale_fill_manual(values = c(\"FALSE\" = \"red\", \"TRUE\" = \"green\")) +\n  theme(legend.position = \"none\") + \n  labs(title = \"Round # 2\",\n       subtitle = paste(\"step size 0.25, accept rate\", \n                        simMetropolis$round_2$acceptance_rate),\n       x = \"a1\",\n       y = \"a2\")\n})\n# plotMetropolis$p2\n\n\nwrap_plots(plotMetropolis[c(\"p1\", \"p2\")]) +\n  plot_annotation(\"Metropolis chain under high correlation\")\n\n\n\n\n\n\n9.2.2.2 Concentration of measure\nTo do on a rainy day"
  },
  {
    "objectID": "ch09_mcmc.html#hamiltonian-monte-carlo",
    "href": "ch09_mcmc.html#hamiltonian-monte-carlo",
    "title": "9  Markov Chain Monte Carlo",
    "section": "9.3 Hamiltonian Monte Carlo",
    "text": "9.3 Hamiltonian Monte Carlo\nTo do. Not critical to do the rest of the book."
  },
  {
    "objectID": "ch09_mcmc.html#easy-hmc-ulam-with-brmsbrm",
    "href": "ch09_mcmc.html#easy-hmc-ulam-with-brmsbrm",
    "title": "9  Markov Chain Monte Carlo",
    "section": "9.4 Easy HMC: ulam with brms::brm()",
    "text": "9.4 Easy HMC: ulam with brms::brm()\nSame data as in chapter 8.\n\ndata(rugged)\ndataRugged &lt;- rugged %&gt;%\n  filter(complete.cases(rgdppc_2000)) %&gt;%\n  mutate(log_gdp = log(rgdppc_2000),\n         is_africa = if_else(cont_africa == 1, \"Africa\", \"Not Africa\"),\n         is_africa = as.factor(is_africa))\nrm(rugged)\ndataRugged_nona &lt;- dataRugged %&gt;%\n  drop_na(rgdppc_2000) %&gt;%\n  mutate(log_gdp_s = log_gdp / mean(log_gdp),\n         rugged_s = scales::rescale(rugged),\n         # rugged_s = rugged / max(rugged),\n         rugged_sc = as.vector(scale(rugged_s, center = TRUE, scale = FALSE)))\ndataRugged_nona |&gt;\n  select(is_africa, rgdppc_2000, log_gdp, log_gdp_s, rugged, rugged_s, rugged_sc) |&gt;\n  skim() |&gt;\n  mutate(across(.cols=where(is.numeric), .fns = round, digits = 2))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = where(is.numeric), .fns = round, digits = 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\nData summary\n\n\nName\nselect(…)\n\n\nNumber of rows\n170\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nis_africa\n0\n1\nFALSE\n2\nNot: 121, Afr: 49\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrgdppc_2000\n0\n1\n9094.89\n9699.99\n466.65\n1880.83\n5314.74\n13100.15\n57792.09\n▇▂▂▁▁\n\n\nlog_gdp\n0\n1\n8.52\n1.17\n6.15\n7.54\n8.58\n9.48\n10.96\n▃▆▇▆▃\n\n\nlog_gdp_s\n0\n1\n1.00\n0.14\n0.72\n0.89\n1.01\n1.11\n1.29\n▃▆▇▆▃\n\n\nrugged\n0\n1\n1.33\n1.17\n0.00\n0.44\n0.98\n1.96\n6.20\n▇▅▁▁▁\n\n\nrugged_s\n0\n1\n0.21\n0.19\n0.00\n0.07\n0.16\n0.32\n1.00\n▇▅▁▁▁\n\n\nrugged_sc\n0\n1\n0.00\n0.19\n-0.21\n-0.14\n-0.06\n0.10\n0.79\n▇▅▁▁▁\n\n\n\n\n\n\n9.4.1 Preparation\n\n# dat_slim &lt;- dataRugged_nona %&gt;%\n#   select(log_gdp_s, rugged_s, rugged_sc, cid) %&gt;%\n#   list()\n# str(dat_slim)\n\n\n\n9.4.2 Sampling from the posterior\nThis is easy with tidybayes. The model is\n\\[\n\\begin{align*}\nlog\\_gdp\\_s_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu &= \\alpha_{cid[i]} + \\beta_{cid[i]} \\cdot rugged\\_sc \\\\\n\\alpha_{cid[i]} &\\sim \\mathcal{N}(1, 0.1) \\\\\n\\beta_{cid[i]} &\\sim \\mathcal{N}(1, 0.3) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\] As usual, we look at the default priors to have a starting point on how to write our prior with brms.\n\nget_prior(data = dataRugged_nona, \n          formula = log_gdp_s ~ 0 + (1 + rugged_sc|is_africa), \n          family = gaussian)\n\n                prior class      coef     group resp dpar nlpar lb ub\n               lkj(1)   cor                                          \n               lkj(1)   cor           is_africa                      \n student_t(3, 0, 2.5)    sd                                      0   \n student_t(3, 0, 2.5)    sd           is_africa                  0   \n student_t(3, 0, 2.5)    sd Intercept is_africa                  0   \n student_t(3, 0, 2.5)    sd rugged_sc is_africa                  0   \n student_t(3, 0, 2.5) sigma                                      0   \n       source\n      default\n (vectorized)\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"67 secs.\"))\nfit09_01a &lt;- xfun::cache_rds({\n  # sampling with 1 core only\n  brms::brm(\n    data = dataRugged_nona,\n    family = gaussian,\n    formula = bf(log_gdp_s ~ 0 + (1 + rugged_sc | is_africa)),\n    prior = c(prior(normal(0.5, 0.5), class = sd, coef = Intercept, group = is_africa),\n              prior(normal(0, 0.5), class = sd, coef = rugged_sc, group = is_africa),\n              prior(exponential(1), class = sigma)),\n    iter = 1000, warmup = 500, chains = 4, cores = 1, seed = 911)},\n  file = \"ch09_fit09_01a\")\ntictoc::toc()\n\nrun time of 67 secs., use the cache.: 0.09 sec elapsed\n\n\n\nprint(fit09_01a)\n\nWarning: There were 3 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_gdp_s ~ 0 + (1 + rugged_sc | is_africa) \n   Data: dataRugged_nona (Number of observations: 170) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nGroup-Level Effects: \n~is_africa (Number of levels: 2) \n                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                0.95      0.29     0.49     1.62 1.01      536\nsd(rugged_sc)                0.24      0.17     0.04     0.70 1.00      439\ncor(Intercept,rugged_sc)    -0.07      0.42    -0.79     0.73 1.00      858\n                         Tail_ESS\nsd(Intercept)                 831\nsd(rugged_sc)                 494\ncor(Intercept,rugged_sc)     1002\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.11      0.01     0.10     0.13 1.01     1312     1113\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote that we use mean_hdi() but there are many more of these functions in ggdist() (which are actually carried forward in tidybayes).\n\npost09_01a &lt;- gather_draws(fit09_01a, r_is_africa[is_africa, term]) |&gt;\n  ggdist::mean_hdi(.width = 0.89) |&gt;\n  identity()\npost09_01a\n\n# A tibble: 4 × 9\n  is_africa  term      .variable  .value  .lower  .upper .width .point .interval\n  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 Africa     Intercept r_is_afri…  0.882  0.855   0.906    0.89 mean   hdi      \n2 Africa     rugged_sc r_is_afri…  0.102 -0.0162  0.227    0.89 mean   hdi      \n3 Not.Africa Intercept r_is_afri…  1.05   1.03    1.07     0.89 mean   hdi      \n4 Not.Africa rugged_sc r_is_afri… -0.126 -0.219  -0.0313   0.89 mean   hdi      \n\n\n\n\n9.4.3 Sampling again, in parallel\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"20 secs.\"))\nfit09_01b &lt;- xfun::cache_rds({\n  update(fit09_01a, iter = 1000, warmup = 500, chains = 4, cores = 4, seed = 911)},\n  file = \"ch09_fit09_01b\")\ntictoc::toc()\n\nrun time of 20 secs., use the cache.: 0.13 sec elapsed\n\n\nand we can see that the result are pretty close to mcElreath”s\n\ngrepl(\"^r_is_africa.+\", x = rownames(posterior_summary(fit09_01b)))\n\n [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n\nrownames(posterior_summary(fit09_01b))\n\n [1] \"sd_is_africa__Intercept\"             \"sd_is_africa__rugged_sc\"            \n [3] \"cor_is_africa__Intercept__rugged_sc\" \"sigma\"                              \n [5] \"r_is_africa[Africa,Intercept]\"       \"r_is_africa[Not.Africa,Intercept]\"  \n [7] \"r_is_africa[Africa,rugged_sc]\"       \"r_is_africa[Not.Africa,rugged_sc]\"  \n [9] \"lprior\"                              \"lp__\"                               \n\nposterior_summary(fit09_01b) |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column() |&gt;\n  filter(grepl(\"^r_is_.+|^sd_is_.+\", x = rownames(posterior_summary(fit09_01b)))) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 3))\n\n                            rowname Estimate Est.Error   Q2.5  Q97.5\n1           sd_is_africa__Intercept    0.951     0.290  0.489  1.616\n2           sd_is_africa__rugged_sc    0.243     0.174  0.042  0.703\n3     r_is_africa[Africa,Intercept]    0.882     0.016  0.848  0.913\n4 r_is_africa[Not.Africa,Intercept]    1.051     0.010  1.031  1.071\n5     r_is_africa[Africa,rugged_sc]    0.102     0.078 -0.043  0.256\n6 r_is_africa[Not.Africa,rugged_sc]   -0.126     0.060 -0.244 -0.008\n\n\nand we can see the formula\n\nfit09_01b$formula\n\nlog_gdp_s ~ 0 + (1 + rugged_sc | is_africa) \n\n# and we can deparse it when plotting\ndeparse1(fit09_01b$formula$formula)\n\n[1] \"log_gdp_s ~ 0 + (1 + rugged_sc | is_africa)\"\n\n\nand obtain information on the model priors\n\nprior_summary(fit09_01b)\n\n                prior class      coef     group resp dpar nlpar lb ub\n lkj_corr_cholesky(1)     L                                          \n lkj_corr_cholesky(1)     L           is_africa                      \n student_t(3, 0, 2.5)    sd                                      0   \n student_t(3, 0, 2.5)    sd           is_africa                  0   \n     normal(0.5, 0.5)    sd Intercept is_africa                  0   \n       normal(0, 0.5)    sd rugged_sc is_africa                  0   \n       exponential(1) sigma                                      0   \n       source\n      default\n (vectorized)\n      default\n (vectorized)\n         user\n         user\n         user\n\n\n\n\n9.4.4 Visualization\nWe use GGally. It can also be done using the ggmcmc package with ggs_pairs. The bayesplot package also has many different plots. It is more sophisticated than ggmcmc with some vignettes explaining how to diagnose the chains.\nFor this we choose to used ggmcmc which has trank plot and provides the warmup.\nAlso note that Solomon Kurtz Kurz (2020) has a lot more options and recipes about all of these plots. It is worthwhile reading before losing time finding solutions on the web.\n\npost09_01b &lt;- fit09_01b |&gt;\n  spread_draws(r_is_africa[is_africa, term]) |&gt;\n  unite(col = \"param\", is_africa, term) |&gt;\n  pivot_wider(names_from = param, values_from = r_is_africa) |&gt;\n  identity()\n# glimpse(post09_01b)\n\n\nplot09_01b &lt;- list()\nplot09_01b &lt;- within(plot09_01b, {\n  cols &lt;- c(\"Africa_Intercept\", \"Africa_rugged_sc\", \"Not.Africa_Intercept\", \"Not.Africa_rugged_sc\")\n  pairs &lt;- post09_01b |&gt;\n    GGally::ggpairs(mapping = aes(color = as.factor(.chain)),\n                    columns = cols,\n                    title = \"Model b9.1b\") +\n    scale_color_paletteer_d(\"lisa::LeeKrasner\")\n})\nplot09_01b$pairs\n\n\n\n\nand the crosscorrelations which could also be done with ggmcmc but are a little mode flexible to use with GGally, in particular we can get in lower trianle format\n\nplot09_01b &lt;- within(plot09_01b, {\n  corr &lt;- GGally::ggcorr(post09_01b[, cols],\n                            color = \"darkgreen\",\n                            nbreaks = 13, label = TRUE, label_round = 2,\n                            label_color = \"midnightblue\") +\n  scale_fill_paletteer_d(palette = \"ggthemr::dust\") +\n  theme(legend.position = c(0.2, 0.8),\n        legend.title = element_blank(),\n        title = element_text(color = \"midnightblue\")) +\n  labs(title = \"Correlations between parameters\",\n       subtitle = \"Model b9.1b\")\n})\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\nplot09_01b$corr\n\n\n\n\n\n\n9.4.5 Checking the chain\nThe ggmcmc package always starts with the ggs() function which extract the posterior draws and organize them in a tibble. The ggmcmc is very useful with family of parameters. i.e.e parameter which are indexed. See the family argument in the ggs functions.\nSee more info on ggmcmc at ggmcmc\n\nggs09_01b &lt;- ggmcmc::ggs(fit09_01b, family = \"r_is_africa\")\n\nWarning in custom.sort(D$Parameter): NAs introduced by coercion\n\nWarning in custom.sort(D$Parameter): NAs introduced by coercion\n\n\n\n9.4.5.1 Trace plot\n\nggs09_01b |&gt;\n  ggs_traceplot() +\n  scale_color_paletteer_d(\"lisa::LeeKrasner\") +\n  ggthemes::theme_clean() +\n  theme(panel.grid = element_blank()) +\n  labs(title = \"Trace plot\",\n       subtitle = \"Model b9.1b\", color = \"chain\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n9.4.5.2 Trank plot\nThis is a special case, only done with bayesplot. See how it is done in section 9.4.5 of Kurz (2020).\nYou have to read section 9.5.3, Figure 9.9, to understand how to read a trank plot.\n\nplot09_01b_trank &lt;- list()\nplot09_01b_trank &lt;- within(plot09_01b_trank, {\n  post &lt;- posterior::as_draws_rvars(fit09_01b)\n  \n  p &lt;- post |&gt; bayesplot::mcmc_rank_overlay(regex_pars = \"is_africa\") +\n    coord_cartesian(ylim = c(10, 50)) +\n    scale_color_paletteer_d(\"lisa::LeeKrasner\")\n})\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\nplot09_01b_trank$p\n\n\n\n\n\n\n9.4.5.3 Running means\n\nggs09_01b |&gt; ggs_running() +\n  scale_color_paletteer_d(\"lisa::LeeKrasner\") +\n  labs(title = \"Running means\",\n       subtitle = \"Model b9.1b\", color = \"chain\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n\n9.4.6 Crosscorrelations\nI prefer the crosscorrelaion plot as done with GGally above, with lower triangle\n\nggs09_01b |&gt; ggs_crosscorrelation() +\n  scale_fill_paletteer_c(\"grDevices::Emrld\", direction = -1) +\n  theme_minimal() +\n  labs(title = \"Crosscorrelations\",\n       subtitle = \"Model b9.1b\", color = \"chain\")\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\n9.4.6.1 Autocorrelations\n\nggs09_01b |&gt; ggs_autocorrelation() +\n  scale_fill_paletteer_d(\"lisa::LeeKrasner\") +\n  scale_color_paletteer_d(\"lisa::LeeKrasner\", guide = \"none\") +\n  theme_minimal() + \n  labs(title = \"Autocorrelations\",\n       subtitle = \"Model b9.1b\", color = NULL, fill = \"chain\")\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n9.4.6.2 Overthinking: Raw Stan model code\nWe can use the brms::stancode() function to get the stan code from the brmsfit object\n\nbrms::stancode(fit09_01b)\n\n// generated with brms 2.18.0\nfunctions {\n /* compute correlated group-level effects\n  * Args:\n  *   z: matrix of unscaled group-level effects\n  *   SD: vector of standard deviation parameters\n  *   L: cholesky factor correlation matrix\n  * Returns:\n  *   matrix of scaled group-level effects\n  */\n  matrix scale_r_cor(matrix z, vector SD, matrix L) {\n    // r is stored in another dimension order than z\n    return transpose(diag_pre_multiply(SD, L) * z);\n  }\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  int&lt;lower=1&gt; J_1[N];  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  vector[N] Z_1_2;\n  int&lt;lower=1&gt; NC_1;  // number of group-level correlations\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real&lt;lower=0&gt; sigma;  // dispersion parameter\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  matrix[M_1, N_1] z_1;  // standardized group-level effects\n  cholesky_factor_corr[M_1] L_1;  // cholesky factor of correlation matrix\n}\ntransformed parameters {\n  matrix[N_1, M_1] r_1;  // actual group-level effects\n  // using vectors speeds up indexing in loops\n  vector[N_1] r_1_1;\n  vector[N_1] r_1_2;\n  real lprior = 0;  // prior contributions to the log posterior\n  // compute actual group-level effects\n  r_1 = scale_r_cor(z_1, sd_1, L_1);\n  r_1_1 = r_1[, 1];\n  r_1_2 = r_1[, 2];\n  lprior += exponential_lpdf(sigma | 1);\n  lprior += normal_lpdf(sd_1[1] | 0.5, 0.5)\n    - 1 * normal_lccdf(0 | 0.5, 0.5);\n  lprior += normal_lpdf(sd_1[2] | 0, 0.5)\n    - 1 * normal_lccdf(0 | 0, 0.5);\n  lprior += lkj_corr_cholesky_lpdf(L_1 | 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n];\n    }\n    target += normal_lpdf(Y | mu, sigma);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(to_vector(z_1));\n}\ngenerated quantities {\n  // compute group-level correlations\n  corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1);\n  vector&lt;lower=-1,upper=1&gt;[NC_1] cor_1;\n  // extract upper diagonal of correlation matrix\n  for (k in 1:M_1) {\n    for (j in 1:(k - 1)) {\n      cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];\n    }\n  }\n}"
  },
  {
    "objectID": "ch09_mcmc.html#care-and-feeding-of-your-markov-chain",
    "href": "ch09_mcmc.html#care-and-feeding-of-your-markov-chain",
    "title": "9  Markov Chain Monte Carlo",
    "section": "9.5 Care and feeding of your Markov chain",
    "text": "9.5 Care and feeding of your Markov chain\nThe brms defaults are iter = 2000 and warmup = 1000.\n\n9.5.1 How many samples do you need\nIn brms the n_eff mentioned by McElreath is bulk_ESS. The tail ESS discussed by McElreath is the tail_ESS value in brms.\n\n\n9.5.2 How many chains do you need\nFor more information on convergence statistics \\(\\mathcal{\\widehat{R}}\\) see section 9.5.2.1 in Kurz (2020).\n\n\n9.5.3 Taming a wild chain\nThe start argument in rethinking is replaced by inits in brms. The simple example of a wild chain is as follows. Note that brms can take data in the form of a list.\nI find much easier to understand the issues by looking at the autocorrelation and the mean plots created with ggmcmc::ggs_running() and ggmcmc::ggs_autocorrelation(). They will be used often in this work to visualize the Markov chain.\n\n9.5.3.1 Divergent transitions\nWe create a model that has poor priors\n\\[\n\\begin{align*}\ny_i &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha \\\\\n\\alpha &\\sim \\mathcal{N}(1, 1000) \\\\\n\\sigma &\\sim \\mathcal{Exp}(0.0001)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit09_02 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = list(y = c(-1, 1)), \n             family = gaussian,\n             y ~ 1,\n      prior = c(prior(normal(0, 1000), class = Intercept),\n                prior(exponential(0.0001), class = sigma)),\n      iter = 1000, warmup = 500, chains = 2, seed = 919)},\n  file = \"ch09_fit09_02\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.15 sec elapsed\n\n\n\nposterior_summary(fit09_02)\n\n             Estimate    Est.Error        Q2.5      Q97.5\nb_Intercept -12.49622  331.6546585 -820.745087  801.93867\nsigma       530.84090 1722.4140199    2.339842 2937.56044\nlprior      -17.14514    0.2820165  -17.837065  -17.03727\nlp__        -24.11428    2.0307353  -27.946036  -20.27860\n\n\nThe results are pretty bad, just like McElreath wanted them. The brms::nuts_params() provide much diagnostic informations. The type of information is in the Parameter column.\n\nnuts_params(fit09_02) %&gt;% \n  distinct(Parameter)\n\n      Parameter\n1 accept_stat__\n2    stepsize__\n3   treedepth__\n4  n_leapfrog__\n5   divergent__\n6      energy__\n\n\nIn the current case, the divergent transitions are the issue. For this we look at Parameter == \"divergent__\".\n\nnuts_params(fit09_02) %&gt;%\n  filter(Parameter == \"divergent__\") %&gt;%\n  count(Value)\n\n  Value   n\n1     0 896\n2     1 104\n\n\nand plotting the trace and trank\n\nggs09_02 &lt;- ggmcmc::ggs(fit09_02, family = c(\"b_|sigma\"))\n\nplot09_02 &lt;- list()\nplot09_02 &lt;- within(plot09_02, {\n  p1 &lt;- ggs09_02 |&gt;\n    ggs_traceplot() +\n    scale_color_paletteer_d(\"lisa::Pierre_AugusteRenoir\") +\n    theme_minimal()\n  \n  p2 &lt;-  as_draws_rvars(fit09_02) |&gt;\n    bayesplot::mcmc_rank_overlay(pars = vars(b_Intercept, sigma)) +\n    scale_color_paletteer_d(\"lisa::Pierre_AugusteRenoir\")\n})\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\nwrap_plots(plot09_02, nrow = 2) +\n  plot_annotation(title = \"These chains are not healthy\",\n  subtitle = \"Model b9.2\") &\n  theme(legend.position = \"none\")\n\n\n\n\nwe can also see that the parameter’s mean has a very hard time reaching a solution with different behaviors between the chains, i.e. the chains behave erratically.\n\nggs_running(ggs09_02) +\n  scale_color_paletteer_d(\"lisa::Pierre_AugusteRenoir\") +\n  labs(title = \"Running means\",\n       subtitle = \"Model b9.2\", color = \"chain\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\nand the autocorrelations are not as well behave as we normally see\n\nggs09_02 |&gt;\n  ggs_autocorrelation() +\n  scale_fill_paletteer_d(\"lisa::Pierre_AugusteRenoir\") +\n  scale_color_paletteer_d(\"lisa::Pierre_AugusteRenoir\", guide = FALSE) +\n  labs(title = \"Autocorrelations\",\n       subtitle = \"Model b9.2\", color = NULL, fill = \"chain\")\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\nWarning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in\nggplot2 3.3.4.\nℹ Please use \"none\" instead.\nℹ The deprecated feature was likely used in the ggmcmc package.\n  Please report the issue at &lt;https://github.com/xfim/ggmcmc/issues/&gt;.\n\n\n\n\n\n\n\n9.5.3.2 Convergent transitions\nNow lets give it little better priors to solve the issue. The model is as follows\n\\[\n\\begin{align*}\ny_i &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha \\\\\n\\alpha &\\sim \\mathcal{N}(1, 10) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit09_03 &lt;- xfun::cache_rds({\n  brm(data = list(y = c(-1, 1)),\n      family = gaussian,\n      y ~ 1,\n      prior = c(prior(normal(1, 10), class = Intercept),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 3, seed = 929)},\n  file = \"ch09_fit09_03\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.18 sec elapsed\n\n\n\nposterior_summary(fit09_03)\n\n               Estimate Est.Error       Q2.5     Q97.5\nb_Intercept  0.05561712 1.4105911  -2.571966  2.766473\nsigma        1.58870045 0.8633358   0.615426  3.839285\nlprior      -4.82462889 0.8811186  -7.154469 -3.837448\nlp__        -8.26246719 1.2490591 -11.577839 -7.094924\n\n\nand the results are more convincing\n\nggs09_03 &lt;- ggmcmc::ggs(fit09_03, family = c(\"b_|sigma\"))\nplot09_03 &lt;- list()\nplot09_03 &lt;- within(plot09_03, {\n  p1 &lt;- ggs_traceplot(ggs09_03) +\n    scale_color_paletteer_d(\"lisa::Pierre_AugusteRenoir\")\n  \n  p2 &lt;-  as_draws_rvars(fit09_03) |&gt;\n    bayesplot::mcmc_rank_overlay(pars = vars(b_Intercept:sigma)) +\n    scale_color_paletteer_d(\"lisa::Pierre_AugusteRenoir\")\n})\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\nwrap_plots(plot09_03, nrow = 2) +\n  plot_annotation(title = \"Better results even with weakly informative priors\",\n                  subtitle = \"Model b9.3\") &\n  theme(legend.position = \"none\")\n\n\n\n\nand we can see the parameter’s mean behaving similarly.\n\nggs_running(ggs09_03) +\n  scale_color_paletteer_d(\"lisa::Pierre_AugusteRenoir\") +\n  labs(title = \"Running means\",\n       subtitle = \"Model b9.3\", color = \"chain\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\nand the autocorrelations have certainly improved\n\nggs_autocorrelation(ggs09_03) +\n  scale_fill_paletteer_d(\"lisa::Pierre_AugusteRenoir\") +\n  scale_color_paletteer_d(\"lisa::Pierre_AugusteRenoir\", guide = FALSE) +\n  theme_minimal() + \n  labs(title = \"Autocorrelations\",\n       subtitle = \"Model b9.3\", color = NULL, fill = \"chain\")\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n\n9.5.4 Non-identifiable parameters\nThe data and model is not exactly what McElreath did but it illustrate the same thing. See Kurz (2020) for more details.\n\nset.seed(929)\ny &lt;- rnorm(100, mean = 0, sd = 1)\n\n\n9.5.4.1 Non-identifiable parameters with very wide priors\nthe model with unreasonable priors\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit09_04 &lt;- xfun::cache_rds({\n  brm(data = list(y  = y,\n                  a1 = 1,\n                  a2 = 1),\n      family = gaussian,\n      y ~ 0 + a1 + a2,\n      prior = c(prior(normal(0, 1000), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, seed = 937)},\n  file = \"ch09_fit09_04\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.17 sec elapsed\n\n\n\nposterior_summary(fit09_04)\n\n           Estimate    Est.Error          Q2.5       Q97.5\nb_a1   -592.6232247 474.96350302 -1334.6872017  215.420162\nb_a2    592.5211967 474.96359219  -215.5278585 1334.680843\nsigma     0.9645898   0.06455515     0.8500682    1.101628\nlprior  -17.1946532   0.55083978   -18.4296486  -16.545511\nlp__   -156.3444156   1.05485109  -158.7791240 -154.939693\n\n\n\nget_variables(fit09_04)\n\n [1] \"b_a1\"          \"b_a2\"          \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\nggs09_04 &lt;- ggmcmc::ggs(fit09_04, family = c(\"b_|sigma\"))\n\n\nplot09_04 &lt;- list()\nplot09_04 &lt;- within(plot09_04, {\n  p1 &lt;- ggs_traceplot(ggs09_04) +\n    scale_color_paletteer_d(\"lisa::JanvanEyck\")\n  \n  p2 &lt;- as_draws_rvars(fit09_04) |&gt;\n    bayesplot::mcmc_rank_overlay(pars = vars(b_a1:sigma)) +\n    scale_color_paletteer_d(\"lisa::JanvanEyck\")\n})\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\nwrap_plots(plot09_04, nrow = 2) +\n  plot_annotation(title = \"Non-identifiable parameters with uninformative priors\",\n                  subtitle = \"Model b9.4\") &\n  theme(legend.position = \"none\")\n\n\n\n\nwhere we can see that the autocorrelations are a major cause of problems!\n\nggs09_04 |&gt;\n  ggs_autocorrelation() +\n  scale_fill_paletteer_d(\"lisa::JanvanEyck\") +\n  scale_color_paletteer_d(\"lisa::JanvanEyck\", guide = FALSE) +\n  labs(title = \"Autocorrelations\",\n       subtitle = \"Model b9.4\", color = NULL, fill = \"chain\")\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\nwhich causes the means the be all over the place\n\nggs09_04 |&gt; ggs_running() +\n  scale_color_paletteer_d(\"lisa::JanvanEyck\") +\n  labs(title = \"Running means\",\n       subtitle = \"Model b9.4\", color = \"chain\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n9.5.4.2 Non-identifiable parameters with weakly informative priors\nand the model with weakly informative priors\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit09_05 &lt;- xfun::cache_rds({\n  brm(data = list(y  = y,\n                  a1 = 1,\n                  a2 = 1),\n      family = gaussian,\n      y ~ 0 + a1 + a2,\n      prior = c(prior(normal(0, 10), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, seed = 941)},\n  file = \"ch09_fit09_05\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.19 sec elapsed\n\n\n\nposterior_summary(fit09_05)\n\n            Estimate  Est.Error         Q2.5       Q97.5\nb_a1     -0.02944473 6.96602865  -13.5710600   13.743933\nb_a2     -0.07076685 6.96727685  -13.8096421   13.479939\nsigma     0.97098827 0.06889051    0.8500739    1.120296\nlprior   -7.89928607 0.68160641   -9.8670016   -7.341121\nlp__   -147.09959440 1.18535327 -150.1941445 -145.728743\n\n\nnow the plots make more sense\n\nggs09_05 &lt;- ggmcmc::ggs(fit09_05, family = c(\"b_|sigma\"))\n\n\nplot09_05 &lt;- list()\nplot09_05 &lt;- within(plot09_05, {\n  p1 &lt;- ggs_traceplot(ggs09_05) +\n    scale_color_paletteer_d(\"lisa::JanvanEyck\")\n  \n  p2 &lt;- as_draws_rvars(fit09_05) %&gt;%\n    bayesplot::mcmc_rank_overlay(pars = vars(b_a1:sigma)) +\n    scale_color_paletteer_d(\"lisa::JanvanEyck\")\n})\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\nwrap_plots(plot09_05, nrow = 2) +\n  plot_annotation(title = \"Non-identifiable parameters withw weakly informative priors\",\n                  subtitle = \"Model b9.5\") &\n  theme(legend.position = \"none\")\n\n\n\n\nwhere we can see that the autocorrelations are better now\n\nggs09_05 |&gt; \n  ggs_autocorrelation() +\n  scale_fill_paletteer_d(\"lisa::JanvanEyck\") +\n  scale_color_paletteer_d(\"lisa::JanvanEyck\", guide = FALSE) +\n  labs(title = \"Autocorrelations\",\n       subtitle = \"Model b9.5\", color = NULL, fill = \"chain\")\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\nand the means are also much improved, actually we might even take a smaller sample size.\n\nggs09_05 |&gt;\n  ggs_running() +\n  scale_color_paletteer_d(\"lisa::JanvanEyck\") +\n  labs(title = \"Running means\",\n       subtitle = \"Model b9.5\", color = \"chain\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n9.5.4.3 ggs_running and ggs_autocorrelation\nI find much easier to understand the issues by looking at the autocorrelation and the mean plots. They are a very nice complement to the trace and trank plots."
  },
  {
    "objectID": "ch09_mcmc.html#summary",
    "href": "ch09_mcmc.html#summary",
    "title": "9  Markov Chain Monte Carlo",
    "section": "9.6 Summary",
    "text": "9.6 Summary\n\n\n\n\nKurz, Solomon. 2020. Statistical Rethinking with Brms. 2nd ed. https://bookdown.org/content/4857/."
  },
  {
    "objectID": "ch10_glm.html#maximum-entropy",
    "href": "ch10_glm.html#maximum-entropy",
    "title": "10  Big Entropy and the Generalized Linear Model",
    "section": "10.1 Maximum Entropy",
    "text": "10.1 Maximum Entropy\nThe example of pebbles and bucket is also very well explained by a NASA scientist at monkey.\nThe nb of ways tp put the pebble is as follows. Let \\(N_i\\) be the number of pebbles we put in bucket \\(i\\). with the total nb of pebble being \\(N\\) defined as \\(N = \\sum_1^5N_i= 10\\).\nThen the nb of ways to put the \\(N\\) pebbles in 5 buckests is\n\\[\n\\text{nb ways to put } N_1 \\text{ pebbles in bucket } 1 \\times \\\\\n\\text{nb ways to put } N_2 = N-N_1 \\text{ pebbles in bucket } 2 \\times \\\\\n\\text{nb ways to put } N_3 = N-N_2-N_1 =  \\text{ pebbles in bucket } 3 \\times \\\\\n\\text{nb ways to put } N_4 = N-N_3-N_2-N_1 =  \\text{ pebbles in bucket } 4 \\times \\\\\n\\text{nb ways to put } N_5 = N-N_4-N_3-N_2-N_1 =  \\text{ pebbles in bucket } 5 \\times \\\\\n\\]\nwhich is\n\\[\n\\binom{10}{N_1} \\cdot \\binom{10}{N_2} \\cdot \\binom{10}{N_3} \\cdot \\binom{10}{N_4} \\cdot \\binom{10}{N_5} =\n\\binom{10}{N_1,N_2,N_3,N_4,N_5}\n\\]\nso, for example, for plot B we have\n\\[\n\\binom{10}{0,1,8,1,0} = 90\n\\]\n\np &lt;- list(\n  \"A\" = c(0, 0, 10, 0, 0),\n  \"B\" = c(0, 1, 8, 1, 0),\n  \"C\" = c(0, 2, 6, 2, 0),\n  \"D\" = c(1, 2, 4, 2, 1),\n  \"E\" = c(2, 2, 2, 2, 2)\n)\n\n\np_norm &lt;- lapply(p, function(q) q / sum(q))\n\nand the entropy is\n\nH &lt;- sapply(p_norm, FUN = function(x) -sum(ifelse(x!=0, x * log(x), 0)))\nH\n\n        A         B         C         D         E \n0.0000000 0.6390319 0.9502705 1.4708085 1.6094379 \n\n\n\n10.1.1 Gaussian\n\n\n10.1.2 Binomial"
  },
  {
    "objectID": "ch10_glm.html#generalized-linear-models",
    "href": "ch10_glm.html#generalized-linear-models",
    "title": "10  Big Entropy and the Generalized Linear Model",
    "section": "10.2 Generalized linear models",
    "text": "10.2 Generalized linear models"
  },
  {
    "objectID": "ch10_glm.html#maximum-entropy-priors",
    "href": "ch10_glm.html#maximum-entropy-priors",
    "title": "10  Big Entropy and the Generalized Linear Model",
    "section": "10.3 Maximum entropy priors",
    "text": "10.3 Maximum entropy priors"
  },
  {
    "objectID": "ch10_glm.html#summary",
    "href": "ch10_glm.html#summary",
    "title": "10  Big Entropy and the Generalized Linear Model",
    "section": "10.4 Summary",
    "text": "10.4 Summary"
  },
  {
    "objectID": "ch10_glm.html#practice",
    "href": "ch10_glm.html#practice",
    "title": "10  Big Entropy and the Generalized Linear Model",
    "section": "10.5 Practice",
    "text": "10.5 Practice"
  },
  {
    "objectID": "ch11_counting.html#binomial-regression",
    "href": "ch11_counting.html#binomial-regression",
    "title": "11  Counting and Classification",
    "section": "11.1 Binomial regression",
    "text": "11.1 Binomial regression\n\n11.1.1 Logistic regression: Prosocial chimpanzees\nLoad the data\n\ndata(chimpanzees)\ndataChimp &lt;- chimpanzees |&gt;\n  mutate(block = factor(block),\n         actor = factor(actor),\n         treatment = factor(1 + prosoc_left + 2 * condition, levels = 1:4,\n                            labels = c(\"AR\", \"AL\", \"PR\", \"PL\")))\nrm(chimpanzees)\ndataChimp |&gt;\n  skim() |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = where(is.numeric), .fns = round, digits = 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\nData summary\n\n\nName\ndataChimp\n\n\nNumber of rows\n504\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nactor\n0\n1\nFALSE\n7\n1: 72, 2: 72, 3: 72, 4: 72\n\n\nblock\n0\n1\nFALSE\n6\n1: 84, 2: 84, 3: 84, 4: 84\n\n\ntreatment\n0\n1\nFALSE\n4\nAR: 126, AL: 126, PR: 126, PL: 126\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrecipient\n252\n0.5\n5.00\n2.00\n2\n3.00\n5.0\n7.00\n8\n▇▃▃▃▇\n\n\ncondition\n0\n1.0\n0.50\n0.50\n0\n0.00\n0.5\n1.00\n1\n▇▁▁▁▇\n\n\ntrial\n0\n1.0\n36.50\n20.80\n1\n18.75\n36.5\n54.25\n72\n▇▇▇▇▇\n\n\nprosoc_left\n0\n1.0\n0.50\n0.50\n0\n0.00\n0.5\n1.00\n1\n▇▁▁▁▇\n\n\nchose_prosoc\n0\n1.0\n0.57\n0.50\n0\n0.00\n1.0\n1.00\n1\n▆▁▁▁▇\n\n\npulled_left\n0\n1.0\n0.58\n0.49\n0\n0.00\n1.0\n1.00\n1\n▆▁▁▁▇\n\n\n\n\n\nWe have added the variable treatment that is a code for prosoc_left and condition variables with the following meanings\n\ndataChimp |&gt;\n  distinct(prosoc_left, condition, treatment) |&gt;\n  mutate(description = c(\"Alone and two food items on the right\",\n                         \"Alone and two food items on the left\",\n                         \"Partner and two food items on the right\",\n                         \"Partner and two food items on the left\"))\n\n  prosoc_left condition treatment                             description\n1           0         0        AR   Alone and two food items on the right\n2           1         0        AL    Alone and two food items on the left\n3           0         1        PR Partner and two food items on the right\n4           1         1        PL  Partner and two food items on the left\n\n\nThe model used will\n\\[\n\\begin{align*}\npulled\\_left_i &\\sim \\mathcal{Binomial}(1, p_i) \\\\\nlogit(p_i) &= \\alpha_{actor[i]} + \\beta_{treatment[i]} \\\\\n\\alpha_j &\\sim \\mathcal{N}(0, \\omega) \\\\\n\\beta_k &\\sim \\mathcal{N}(0, \\omega) \\\\\n&\\text{sd to be determined}\n\\end{align*}\n\\]\n\n11.1.1.1 Prior for \\(\\alpha\\)\nWe begin with the one-intercept only model. It refers to a general mean for all \\(p_i\\).\n\\[\n\\begin{align*}\npulled\\_left_i &\\sim \\mathcal{Binomial}(1, p_i) \\\\\nlogit(p_i) &= \\alpha \\\\\n\\alpha &\\sim \\mathcal{N}(0, 10)\n\\end{align*}\n\\]\nin brm( forml = pulled_left | trials(1) ~ 1) the | indicates we have extra information about the criterion. In this case, that information is that each pulled_left corresponds to a single trial, i.e. trials(1) which corresponds to the \\(n = 1\\) in \\(Binomial(1, p_i)\\)\nWe will use 2 \\(\\omega\\) values for \\(\\alpha \\sim \\mathcal{N}(0, \\omega)\\).\n\nc(10, 1.5)\n\n[1] 10.0  1.5\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit11_01 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataChimp,\n              family = bernoulli,\n              formula = pulled_left ~ 1,\n              prior = c(\n                prior(normal(0, 10), class = Intercept)),\n             iter = 1000, warmup = 500, chains = 2,\n             cores = detectCores(), seed = 1103,\n              sample_prior = TRUE)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_01\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.11 sec elapsed\n\n\n\nposterior_summary(fit11_01) |&gt;\n  round(digits = 3)\n\n                Estimate Est.Error     Q2.5    Q97.5\nb_Intercept        0.318     0.086    0.153    0.489\nprior_Intercept   -0.296     9.588  -19.293   18.043\nlprior            -3.222     0.000   -3.223   -3.222\nlp__            -346.648     0.633 -348.437 -346.193\n\n\nand we convert the result using brms::inv_logit_scaled().\n\nbrms::inv_logit_scaled(fixef(fit11_01)) |&gt;\n  round(digits = 3)\n\n          Estimate Est.Error  Q2.5 Q97.5\nIntercept    0.579     0.522 0.538  0.62\n\n\nand we use another value for the prior to be able to calibrate it.\n\\[\n\\begin{align*}\npulled\\_left_i &\\sim \\mathcal{Binomial}(1, p_i) \\\\\nlogit(p_i) &= \\alpha \\\\\n\\alpha &\\sim \\mathcal{N}(0, 1.5)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit11_01b &lt;- xfun::cache_rds({\n  out &lt;- update(fit11_01,\n                prior = c(\n                prior(normal(0, 1.5), class = Intercept)))\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_01b\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.14 sec elapsed\n\n\n\nposterior_summary(fit11_01b) |&gt;\n  round(digits = 3)\n\n                Estimate Est.Error     Q2.5    Q97.5\nb_Intercept        0.320     0.086    0.164    0.501\nprior_Intercept   -0.029     1.432   -2.741    2.729\nlprior            -1.349     0.013   -1.380   -1.330\nlp__            -344.768     0.637 -346.668 -344.318\n\n\n\nbrms::inv_logit_scaled(fixef(fit11_01b)) |&gt;\n  round(digits = 3)\n\n          Estimate Est.Error  Q2.5 Q97.5\nIntercept    0.579     0.521 0.541 0.623\n\n\nand we visualize the outcome using the prior samples\n\nprior11_01 &lt;- list()\nprior11_01 &lt;- within(prior11_01, {\n  A &lt;- prior_draws(fit11_01)\n  B &lt;- prior_draws(fit11_01b)\n  all &lt;- bind_rows(\"sd=10\" = A, \"sd=1.5\" = B, .id = \"model\") |&gt;\n    mutate(p = inv_logit_scaled(Intercept))\n})\n# glimpse(prior11_01$all)\n\n\nggplot(prior11_01$all, aes(x = p, color = model)) +\n  geom_density(linewidth = 1, alpha = 3/4, adjust = 0.1) +\n  scale_color_paletteer_d(\"khroma::vibrant\") +\n  theme(legend.position = c(0.5, 0.8)) +\n  labs(title = \"Prior density of pulled_left\",\n       subtitle = \"Model 11.1 and 11.1b\",\n       x = \"prior prob. of pulled_left\", y = NULL)\n\n\n\n\nWe could also use the package simpr to simulate the prior. This will be the favored approach in this project. The advantage of doing this are that it\n\nis much faster and easier to modify than running brm repeatedly\navoids possible problems of convergence when using a fit\nallows us to use a single variable such as \\(treatment\\) instead of creating separate prior for each when using brms. See what Kurz (2020) has to do in its version of chapter 8. The way Kurtz did it renders the tidybayes package less useful.\n\n\n11.1.1.1.1 Prior for \\(\\alpha\\) with simpr\nUsing simpr is actually pretty simple avoid having to run the fit which could have convergence issues and is certainly more time-consuming.\n\nset.seed(1103)\nsim11_01 &lt;- simpr::specify(\n  prosoc_left = ~ rbinom(n = 4000, size = 1, prob = 0.5),\n  condition = ~ rbinom(n = 4000, size = 1, prob = 0.5),\n  treat = ~ 1 + prosoc_left + 2 * condition,\n  a = ~ rnorm(n = 4000, mean = 0, sd = the_sd),\n  p = ~ gtools::inv.logit(a),\n  pulled_left = ~ rbinom(n = 4000, size = 1, prob = p),\n  model = ~ paste(\"sd\", the_sd, sep = \"=\")) |&gt;\n  simpr::define(the_sd = c(10, 1.5)) |&gt;\n  simpr::generate(1)\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:scales':\n\n    discard\n\n\nThe following object is masked from 'package:rethinking':\n\n    map\n\nsim11_01 &lt;- sim11_01$sim |&gt;\n  bind_rows(.id = \"id\")\n\n# sim11_01 |&gt;\n#   glimpse()\n\n\nggplot(sim11_01, aes(x = p, color = model)) +\n  geom_density(size = 1, alpha = 3/4, adjust = 0.1) +\n  scale_color_paletteer_d(\"khroma::vibrant\") +\n  theme(legend.position = c(0.5, 0.8)) +\n  labs(title = \"Prior density of pulled_left\",\n       subtitle = \"Simulation with simstudy\",\n       x = \"prior prob. of pulled_left\", y = NULL)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 11.3\n\n\n\n\n\n\n\n11.1.1.2 Prior for \\(\\beta_{treatment[i]}\\)\nNow we find the sd value for the prior \\(\\beta_k \\sim \\mathcal{N}(0, sd)\\). Note that Solomon Kurtz in Kurz (2020) uses inv_logit_scaled() whereas mcElreath in McElreath (2020) uses inv_logit().\nImportant: In practice we could use the same prior as for \\(\\alpha\\) just above. In this case however we combine the impact of \\(\\alpha\\) and \\(\\beta\\) to illustrate the weirdness of flat priors. Comment from MacElreath at the end of p. 328.\n\\[\n\\begin{align*}\npulled\\_left_i &\\sim \\mathcal{Binomial}(1, p_i) \\\\\nlogit(p_i) &= \\alpha + \\beta_{treatment[i]} \\\\\n\\alpha &\\sim \\mathcal{N}(0, 1.5) \\\\\n\\beta_k &\\sim \\mathcal{N}(0, sd) \\\\\n&\\text{sd to be determined}\n\\end{align*}\n\\]\nWe get the fit with \\(sd = 10\\)\n\n# use get_prior() to get a sense of what the prior might be\nget_prior(formula = pulled_left | trials(1) ~ 1 + treatment, data = dataChimp,\n          family = binomial)\n\n                prior     class        coef group resp dpar nlpar lb ub\n               (flat)         b                                        \n               (flat)         b treatmentAL                            \n               (flat)         b treatmentPL                            \n               (flat)         b treatmentPR                            \n student_t(3, 0, 2.5) Intercept                                        \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit11_02 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataChimp,\n              family = bernoulli,\n              formula = bf(pulled_left ~ (1 | treatment)),\n              prior = c(\n                prior(normal(0, 1.5), class = Intercept),\n                prior(normal(0, 1.5), class = sd, coef = Intercept, group = treatment),\n                prior(normal(0, 10), class = sd, group = treatment)),\n              sample_prior = TRUE,\n             iter = 2000, warmup = 1000, chains = 4,\n             cores = detectCores(), seed = 1109)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  }, file = \"ch11_fit11_02\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.14 sec elapsed\n\n\nand the coefficients, converted back to natural scale are\n\ngtools::inv.logit(fixef(fit11_02)) |&gt;\n  round(digits = 3)\n\n          Estimate Est.Error  Q2.5 Q97.5\nIntercept    0.583     0.579 0.425 0.764\n\n\n\nposterior_summary(fit11_02) |&gt;\n  gtools::inv.logit() |&gt;\n  round(digits = 3)\n\n                              Estimate Est.Error  Q2.5 Q97.5\nb_Intercept                      0.583     0.579 0.425 0.764\nsd_treatment__Intercept          0.636     0.621 0.520 0.908\nr_treatment[AR,Intercept]        0.471     0.586 0.270 0.624\nr_treatment[AL,Intercept]        0.560     0.585 0.378 0.728\nr_treatment[PR,Intercept]        0.416     0.587 0.218 0.563\nr_treatment[PL,Intercept]        0.542     0.583 0.368 0.703\nprior_Intercept                  0.504     0.815 0.053 0.947\nprior_sd_treatment__Intercept    0.766     0.711 0.512 0.965\nlprior                           0.107     0.578 0.031 0.123\nlp__                             0.000     0.916 0.000 0.000\n\n\nthen the fit with \\(sd = 0.5\\)\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"35 secs.\"))\nfit11_03 &lt;- xfun::cache_rds({\n  out &lt;- update(fit11_02,\n                prior = c(\n                  prior(normal(0, 1.5), class = Intercept),\n                  prior(normal(0, 1.5), class = sd, coef = Intercept, group = treatment),\n                  prior(normal(0, 0.5), class = sd, group = treatment)))\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_03\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 35 secs., use the cache.: 0.41 sec elapsed\n\n\nthe coefficients, converted back to natural scale are\n\nposterior_summary(fit11_03) |&gt;\n  gtools::inv.logit() |&gt;\n  round(digits = 3)\n\n                              Estimate Est.Error  Q2.5 Q97.5\nb_Intercept                      0.576     0.565 0.434 0.701\nsd_treatment__Intercept          0.616     0.576 0.523 0.781\nr_treatment[AR,Intercept]        0.479     0.570 0.338 0.624\nr_treatment[AL,Intercept]        0.568     0.574 0.437 0.717\nr_treatment[PR,Intercept]        0.423     0.572 0.278 0.558\nr_treatment[PL,Intercept]        0.548     0.572 0.412 0.702\nprior_Intercept                  0.493     0.818 0.046 0.949\nprior_sd_treatment__Intercept    0.767     0.711 0.513 0.966\nlprior                           0.113     0.530 0.084 0.123\nlp__                             0.000     0.912 0.000 0.000\n\n\n\n11.1.1.2.1 Prior for \\(\\beta\\) with simstudy\nWe also simulate \\(\\beta\\) with simpr which is more versatile and easier to code (personal opinion), e.g. no need to run the model with prior_draws(). To get the prior by factor, that is AR, AL, etc, we need to specify every coefficient in the priors of brmsfit.\n\nset.seed(1109)\nsim11_02 &lt;- simpr::specify(\n  a = ~ rnorm(n = 1000, mean = 0, sd = 1.5),\n  b1 = ~ rnorm(n = 1000, mean = 0, sd = the_sd),\n  b2 = ~ rnorm(n = 1000, mean = 0, sd = the_sd),\n  b3 = ~ rnorm(n = 1000, mean = 0, sd = the_sd),\n  b4 = ~ rnorm(n = 1000, mean = 0, sd = the_sd),\n  p1 = ~ gtools::inv.logit(b1),\n  p2 = ~ gtools::inv.logit(b2),\n  p3 = ~ gtools::inv.logit(b3),\n  p4 = ~ gtools::inv.logit(b4),\n  diff = ~ abs(p1 - p2),\n  model = ~ paste(\"sd\", the_sd, sep = \"=\")) |&gt;\n  define(the_sd = c(10, 0.5)) |&gt;\n  generate(1)\n\nsim11_02 &lt;- sim11_02$sim |&gt;\n  bind_rows(.id = \"id\")\n\n# sim11_02 |&gt;\n#   glimpse()\n\n\nggplot(sim11_02, aes(x = diff, color = model)) +\n  geom_density(size = 1, alpha = 3/4, adjust = 0.1) +\n  scale_color_paletteer_d(\"khroma::vibrant\") +\n  theme(legend.position = \"none\",\n        ) +\n  labs(title = \"Prior diff between treatments\",\n       subtitle = \"Simulation with simstudy\",\n       x = \"prior diff between treatments\", y = NULL)\n\n\n\n\nFigure 11.3\n\n\n\n\n\n\n\n11.1.1.3 The full model\nNow that we have investigated the prior, let’s do the full model with them\n\\[\n\\begin{align*}\npulled\\_left_i &\\sim \\mathcal{Binomial}(1, p_i) \\\\\nlogit(p_i) &= \\alpha_{actor[i]} + \\beta_{treatment[i]} \\\\\n\\alpha_j &\\sim \\mathcal{N}(0, 1.5) \\\\\n\\beta_k &\\sim \\mathcal{N}(0, 0.5)\n\\end{align*}\n\\]\nWe have to create a separate prior for every level in order to get them in separate columns for the analysis later. Normally we would simply use prior(normal(0, 0.5), class = b) without specifying the coefficient.\nWe use get_prior() to help us figure out the priors.\n\nget_prior(data = dataChimp,\n          formula = bf(pulled_left ~ 0 + (1 | actor) + (1 | treatment)),\n          family = bernoulli)\n\n                prior class      coef     group resp dpar nlpar lb ub\n student_t(3, 0, 2.5)    sd                                      0   \n student_t(3, 0, 2.5)    sd               actor                  0   \n student_t(3, 0, 2.5)    sd Intercept     actor                  0   \n student_t(3, 0, 2.5)    sd           treatment                  0   \n student_t(3, 0, 2.5)    sd Intercept treatment                  0   \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit11_04 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataChimp,\n              family = bernoulli,\n              formula = bf(pulled_left ~ 0 + (1 | actor) + (1 | treatment)),\n              prior = c(\n                prior(normal(0, 0.5), class = sd, coef = Intercept, group = actor),\n                prior(normal(0, 1.5), class = sd, group = actor),\n                prior(normal(0, 0.5), class = sd, coef = Intercept, group = treatment),\n                prior(normal(0, 0.5), class = sd, group = treatment)),\n             iter = 2000, warmup = 1000, chains = 4,\n             cores = detectCores(), seed = 1117)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_04\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.19 sec elapsed\n\n\nand the coefficients on natural scale\n\nposterior_summary(fit11_04) |&gt;\n  gtools::inv.logit() |&gt;\n  round(digits = 4)\n\n                          Estimate Est.Error   Q2.5  Q97.5\nsd_actor__Intercept         0.7760    0.5593 0.6961 0.8522\nsd_treatment__Intercept     0.6109    0.5519 0.5320 0.7192\nr_actor[1,Intercept]        0.3873    0.5804 0.2424 0.5365\nr_actor[2,Intercept]        0.9702    0.6777 0.8999 0.9942\nr_actor[3,Intercept]        0.3191    0.5795 0.1938 0.4613\nr_actor[4,Intercept]        0.3200    0.5827 0.1885 0.4657\nr_actor[5,Intercept]        0.3860    0.5792 0.2494 0.5347\nr_actor[6,Intercept]        0.6108    0.5797 0.4537 0.7449\nr_actor[7,Intercept]        0.8670    0.6037 0.7430 0.9393\nr_treatment[AR,Intercept]   0.4998    0.5659 0.3751 0.6332\nr_treatment[AL,Intercept]   0.6124    0.5752 0.4889 0.7534\nr_treatment[PR,Intercept]   0.4247    0.5679 0.2990 0.5554\nr_treatment[PL,Intercept]   0.5889    0.5728 0.4665 0.7299\nlprior                      0.0596    0.7881 0.0029 0.3102\nlp__                        0.0000    0.9530 0.0000 0.0000\n\n\nand to obtain the estimates on the logit scale\nand we build of summary with the estimates on the natural scale\n\npost11_04 &lt;- list()\npost11_04 &lt;- within(post11_04, {\n\n  actor &lt;- fit11_04 |&gt; \n    spread_rvars(sd_actor__Intercept, r_actor[actor, term]) |&gt;\n    mutate(estimate = inv_logit_scaled(r_actor)) |&gt;\n    mean_qi(r_actor, estimate)\n  \n  treatment &lt;- fit11_04 |&gt; \n    spread_rvars(sd_treatment__Intercept, r_treatment[treatment, term]) |&gt;\n    mutate(estimate = inv_logit_scaled(r_treatment)) |&gt;\n    mean_qi(r_treatment, estimate)\n  \n  diff &lt;- fit11_04 |&gt; \n    tidy_draws(r_treatment[treatment, ]) |&gt;\n    select(matches(\"r_treatment\")) |&gt;\n    rename(AR = `r_treatment[AR,Intercept]`,\n           AL = `r_treatment[AL,Intercept]`,\n           PR = `r_treatment[PR,Intercept]`,\n           PL = `r_treatment[PL,Intercept]`) |&gt;\n    transmute(\n      ARvsPR_logit = AR - PR,\n      ARvsPR = inv_logit_scaled(ARvsPR_logit),\n      ALvsPL_logit = AL - PL,\n      ALvsPL = inv_logit_scaled(ALvsPL_logit)) |&gt;\n    pivot_longer(cols = matches(\"ARvsPR|ALvsPL\"),\n                 names_to = \"treatment\",\n                 values_to = \"estimate\") |&gt;\n    group_by(treatment) |&gt;\n    mean_qi(estimate)\n})\n# glimpse(post11_04$diff)\n\n\nplot11_04 &lt;- list()\nplot11_04 &lt;- within(plot11_04, {\n  actor &lt;- ggplot(post11_04$actor,\n                  aes(x = estimate, xmin = estimate.lower, xmax = estimate.upper, y = actor)) +\n    geom_pointinterval(color = \"darkgreen\", size = 2, fatten_point = 3) +\n    geom_vline(xintercept = 0, color = \"brown\") +\n    ggrepel::geom_text_repel(mapping = aes(label = round(estimate, 2))) +\n    scale_y_continuous(breaks = scales::breaks_width(width = 1)) +\n    theme(panel.grid.major.y = element_line(color = \"white\")) +\n    labs(title = \"Estimates for actors with 95% CI\",\n       subtitle = \"Model b11.4\",\n       x = \"probability (outcome scale)\", y = \"actor\")\n  treatment &lt;- ggplot(post11_04$treatment,\n                  aes(x = estimate, xmin = estimate.lower, xmax = estimate.upper, y = treatment)) +\n    geom_pointinterval(color = \"tomato\", size = 2, fatten_point = 3) +\n    geom_vline(xintercept = 0, color = \"brown\") +\n    ggrepel::geom_text_repel(mapping = aes(label = round(estimate, 2))) +\n    theme(panel.grid.major.y = element_line(color = \"white\")) +\n    labs(title = \"Estimates for treatments with 95% CI (McElreath is on logit scale)\",\n       subtitle = \"Model b11.4\",\n       x = \"probability (outcome scale)\", y = \"treatment\")\n  \n  treatment_logit &lt;- ggplot(post11_04$treatment,\n                  aes(x = r_treatment, xmin = r_treatment.lower, xmax = r_treatment.upper, y = treatment)) +\n    geom_pointinterval(color = \"tomato\", size = 2, fatten_point = 3) +\n    geom_vline(xintercept = 0, color = \"brown\") +\n    ggrepel::geom_text_repel(mapping = aes(label = round(r_treatment, 2))) +\n    theme(panel.grid.major.y = element_line(color = \"white\")) +\n    labs(title = \"Estimates for treatments with 95% CI (McElreath is on logit scale)\",\n       subtitle = \"Model b11.4\",\n       x = \"logit scale\", y = \"treatment\")\n  \n  diff_logit &lt;- post11_04$diff |&gt;\n    filter(treatment  %in% c(\"ALvsPL_logit\", \"ARvsPR_logit\")) |&gt;\n    ggplot(aes(x = estimate, xmin = .lower, xmax = .upper, y = treatment)) +\n    geom_pointinterval(color = \"mediumpurple\", size = 2, fatten_point = 3) +\n    geom_vline(xintercept = 0, color = \"brown\") +\n    ggrepel::geom_text_repel(mapping = aes(label = round(estimate, 2))) +\n    theme(panel.grid.major.y = element_line(color = \"white\")) +\n    labs(title = \"Effect treatments with 95% CI (McElreath is on logit scale)\",\n       subtitle = \"Model b11.4\",\n       x = \"logit scale\", y = \"treatment\")\n  diff &lt;- post11_04$diff |&gt;\n    filter(treatment  %in% c(\"ALvsPL\", \"ARvsPR\")) |&gt;\n    ggplot(aes(x = estimate, xmin = .lower, xmax = .upper, y = treatment)) +\n    geom_pointinterval(color = \"mediumpurple\", size = 2, fatten_point = 3) +\n    geom_vline(xintercept = 0, color = \"brown\") +\n    ggrepel::geom_text_repel(mapping = aes(label = round(estimate, 2))) +\n    theme(panel.grid.major.y = element_line(color = \"white\")) +\n    labs(title = \"Effect treatments with 95% CI\",\n       subtitle = \"Model b11.4\",\n       x = \"probability difference (outcome scale)\", y = \"treatment\")\n})\n\nThe plot for the actors is\n\nplot11_04$actor\n\n\n\n\nand for the treatments. This is different because McElreath gives the plot on the logit scale\n\nwrap_plots(plot11_04[c(\"treatment_logit\", \"treatment\")], ncol = 1)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe plot above is different because McElreath gives the plot for the treatment on the logit scale. Why not using the outcome scale as for actors?\n\n\nto compare the models\n\nwrap_plots(plot11_04[c(\"diff\", \"diff_logit\")], ncol = 1)\n\n\n\n\nNext we look at the observed and predicted effect by actor\n\nplot11_04all &lt;- list()\nplot11_04all &lt;- within(plot11_04all, {\n  dataObserved &lt;- dataChimp |&gt;\n    group_by(actor, condition, prosoc_left) |&gt;\n    summarize(pulled_left = mean(pulled_left)) |&gt;\n    # create the factors\n    mutate(actor_id = paste(\"actor\", actor),\n           condition = condition + 1,\n           condition = factor(condition, levels = 1:2, labels = c(\"alone\", \"partner\")),\n           prosoc_left = prosoc_left + 1,\n           prosoc_left = factor(prosoc_left, levels = 1:2, labels = c(\"right\", \"left\")))\n  \n  plotObserved &lt;- ggplot(dataObserved, \n                         aes(x = condition, y = pulled_left, \n                             group = prosoc_left, color = prosoc_left,\n                             fill = prosoc_left)) +\n    geom_line(size = 1) +\n    geom_point() +\n    geom_hline(yintercept = 0.5, color = \"brown\", linetype = 2) +\n    ggrepel::geom_text_repel(aes(label = round(pulled_left, 2)), size = 3) +\n    scale_y_continuous(labels = scales::label_percent()) +\n    coord_cartesian(ylim = c(0, 1)) +\n    theme(legend.position = \"bottom\") +\n    labs(title = \"Observed proportions\",\n         x = NULL, y = \"proportion pulled left\") +\n    facet_grid(. ~ actor_id)\n})\n\n`summarise()` has grouped output by 'actor', 'condition'. You can override\nusing the `.groups` argument.\n\n# glimpse(plot11_04all$dataObserved)\nplot11_04all$plotObserved\n\n\n\n\nFigure 11.4\n\n\n\n\n\nplot11_04all &lt;- within(plot11_04all, {\n  \n  dataEPredicted &lt;- expand_grid(actor = unique(dataChimp$actor),\n                               treatment = unique(dataChimp$treatment)) |&gt;\n    # dataPredicted &lt;- dataChimp |&gt;\n    add_epred_rvars(fit11_04) |&gt;\n    mutate(actor_id = paste(\"actor\", actor),\n           condition = if_else(substring(treatment, 1, 1) == \"A\", \"alone\", \"partner\"),\n           condition = as.factor(condition),\n           prosoc_left = if_else(substring(treatment, 2, 2) == \"R\", \"right\", \"left\"),\n           prosoc_left = as.factor(prosoc_left)) |&gt;\n    select(-treatment) |&gt;\n    group_by(actor, condition, prosoc_left) |&gt;\n    mean_qi(.epred, .width = 0.89) |&gt;\n    identity()\n  \n  plotEPredicted &lt;- ggplot(dataEPredicted,\n                         aes(x = condition, y = .epred,\n                             group = prosoc_left, color = prosoc_left,\n                             fill = prosoc_left)) +\n    geom_line(size = 1) +\n    geom_point() +\n    geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 1/3) +\n    geom_hline(yintercept = 0.5, color = \"brown\", linetype = 2) +\n    ggrepel::geom_text_repel(aes(label = round(.epred, 2)), size = 3) +\n    scale_y_continuous(labels = scales::label_percent()) +\n    coord_cartesian(ylim = c(0, 1)) +\n    theme(legend.position = \"bottom\") +\n    labs(title = \"Posterior expected predictions with 89% CI\",\n         x = NULL, y = \"proportion pulled left\") +\n    facet_grid(. ~ actor_id)\n})\n# glimpse(plot11_04all$dataEPredicted)\nplot11_04all$plotEPredicted\n\n\n\n\nFigure 11.4\n\n\n\n\nand if we compare the models\n\nloo_compare(fit11_02, fit11_03, fit11_04, criterion = \"waic\") |&gt;\n  round(digits = 1) |&gt;\n  print(simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  \nfit11_04    0.0       0.0  -267.2       9.2          8.5    0.4     534.5\nfit11_03  -74.3       9.0  -341.5       4.3          3.7    0.1     683.1\nfit11_02  -74.4       9.0  -341.6       4.4          3.8    0.1     683.2\n         se_waic\nfit11_04   18.4 \nfit11_03    8.7 \nfit11_02    8.7 \n\n\nwe can see that \\(actor\\) and \\(treatment\\) considered alone give about the same quality of fit. The better fit is when both of them are taken into account together.\n\n\n\n11.1.2 Relative shark and absolute deer\nIt is more common to see logistic regression as relative effects. relative effect are proportional change in the odds of the outcome\n\nfit11_04 |&gt;\n  spread_rvars(r_treatment[treatment, ]) |&gt;\n  pivot_wider(names_from = treatment, values_from = r_treatment) |&gt;\n  mutate(\n    ARvsPR = exp(PR - AR),\n    ALvsPL = exp(PL - AL)) |&gt;\n  mean_qi(.width = 0.89) |&gt;\n  pull(ALvsPL) |&gt;\n  identity()\n\n[1] 0.9371068\n\n\nOn average the switch from treatment AL, pulling the left lever without a partner, to treatment PL, pulling the left lever with a partner, reduces the odds by 6%. The difference with McElreath is cause by the ransom sampling.\n\nThe risk of focusing on relative effects, such as proportional odds, is that they aren’t enough to tell enough whether a variable is important or not.\n\nSee the overthinking box in section 11.1.2, p. 337.\n\n\n11.1.3 Aggregated binomial: Chimpanzees again\n\ndataChimp_agg &lt;- dataChimp |&gt;\n  group_by(treatment, actor, prosoc_left, condition) |&gt;\n  summarise(left_pulls = sum(pulled_left)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'treatment', 'actor', 'prosoc_left'. You\ncan override using the `.groups` argument.\n\n# dataChimp_agg\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit11_06 &lt;- xfun::cache_rds({\n  out &lt;- brms::brm(\n    data = dataChimp_agg,\n    family = binomial,\n    formula = bf(left_pulls | trials(18) ~ 0 + (1 | actor) + (1 | treatment)),\n    prior = c(prior(normal(0, 0.5), class = sd, coef = Intercept, group = actor),\n              prior(normal(0, 1.5), class = sd, group = actor),\n              prior(normal(0, 0.5), class = sd, coef = Intercept, group = treatment),\n              prior(normal(0, 0.5), class = sd, group = treatment)),\n    cores = detectCores(), seed = 1123)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_06\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.19 sec elapsed\n\n\n\nposterior_summary(fit11_06) |&gt;\n  round(digits = 3)\n\n                          Estimate Est.Error    Q2.5   Q97.5\nsd_actor__Intercept          1.257     0.234   0.859   1.770\nsd_treatment__Intercept      0.469     0.212   0.144   0.969\nr_actor[1,Intercept]        -0.461     0.327  -1.149   0.140\nr_actor[2,Intercept]         3.516     0.753   2.243   5.156\nr_actor[3,Intercept]        -0.751     0.333  -1.459  -0.146\nr_actor[4,Intercept]        -0.747     0.328  -1.456  -0.156\nr_actor[5,Intercept]        -0.454     0.321  -1.105   0.171\nr_actor[6,Intercept]         0.446     0.327  -0.211   1.065\nr_actor[7,Intercept]         1.865     0.404   1.096   2.699\nr_treatment[AR,Intercept]   -0.009     0.270  -0.517   0.572\nr_treatment[AL,Intercept]    0.459     0.305  -0.051   1.139\nr_treatment[PR,Intercept]   -0.305     0.268  -0.835   0.228\nr_treatment[PL,Intercept]    0.359     0.290  -0.154   0.998\nlprior                      -2.864     1.342  -6.027  -0.846\nlp__                       -74.479     3.078 -81.341 -69.695\n\n\n\n\n11.1.4 Aggregated binomial: Graduate school admissions\nIn the chimpanzees example, the number of trials was fixed at 18. This is often not the case.\n\ndata(UCBadmit)\ndataAdmit &lt;- UCBadmit |&gt;\n  rename(gender = applicant.gender) |&gt;\n  mutate(case = factor(seq_len(n())),\n         admit_pct = admit / applications,\n         reject_pct = reject / applications)\nrm(UCBadmit)\nskim(dataAdmit) |&gt;\n  select(-n_missing, -complete_rate) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 1))\n\n\nData summary\n\n\nName\ndataAdmit\n\n\nNumber of rows\n12\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\nskim_variable\nordered\nn_unique\ntop_counts\n\n\n\n\ndept\nFALSE\n6\nA: 2, B: 2, C: 2, D: 2\n\n\ngender\nFALSE\n2\nfem: 6, mal: 6\n\n\ncase\nFALSE\n12\n1: 1, 2: 1, 3: 1, 4: 1\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nadmit\n146.2\n148.4\n17.0\n45.8\n107.0\n154.0\n512.0\n▇▅▁▁▁\n\n\nreject\n230.9\n122.8\n8.0\n188.2\n261.5\n314.0\n391.0\n▃▂▃▇▆\n\n\napplications\n377.2\n216.9\n25.0\n291.5\n374.0\n452.8\n825.0\n▃▆▇▃▂\n\n\nadmit_pct\n0.4\n0.2\n0.1\n0.3\n0.3\n0.6\n0.8\n▃▇▂▃▃\n\n\nreject_pct\n0.6\n0.2\n0.2\n0.4\n0.7\n0.7\n0.9\n▃▃▂▇▃\n\n\n\n\n\nthe univariate model is\n\\[\n\\begin{align*}\nadmit_i &\\sim \\mathcal{Binomial}(n_i, p_i) \\\\\nlogit(p_i) &= \\alpha_{gid[i]} \\\\\n\\alpha_j &\\sim \\mathcal{N}(0, 1.5)\n\\end{align*}\n\\]\nand we fit the model, first we look at the default prior,\n\nget_prior(data = dataAdmit, \n          formula = bf(admit | trials(applications) ~ 0 + (1|gender)),\n          family = binomial)\n\n                prior class      coef  group resp dpar nlpar lb ub       source\n student_t(3, 0, 2.5)    sd                                   0         default\n student_t(3, 0, 2.5)    sd           gender                  0    (vectorized)\n student_t(3, 0, 2.5)    sd Intercept gender                  0    (vectorized)\n\n\nthen do the fit\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit11_07 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataAdmit,\n             family = binomial,\n             formula = bf(admit | trials(applications) ~ 0 + (1|gender)),\n             prior(normal(0, 1.5), class = sd),\n             iter = 2000, warmup = 1000, chains = 4, \n             cores = detectCores(), seed = 1129)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_07\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.51 sec elapsed\n\n\n\nposterior_summary(fit11_07) |&gt;\n  round(digits = 3)\n\n                           Estimate Est.Error     Q2.5    Q97.5\nsd_gender__Intercept          0.993     0.543    0.343    2.384\nr_gender[female,Intercept]   -0.828     0.051   -0.930   -0.731\nr_gender[male,Intercept]     -0.219     0.039   -0.293   -0.143\nlprior                       -0.916     0.316   -1.894   -0.657\nlp__                       -431.008     1.523 -434.748 -429.117\n\n\nand we compute the contrast between male and female\n\nfit11_07 |&gt;\n  spread_rvars(r_gender[gender, ]) |&gt;\n  pivot_wider(names_from = gender, values_from = r_gender) |&gt;\n  transmute(diff_a = male - female,\n         diff_p = inv_logit_scaled(male) - inv_logit_scaled(female)) |&gt;\n  pivot_longer(cols = everything(), names_to = \"var\", values_to = \"value\") |&gt;\n  mean_qi(value, .width = 0.89) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n# A tibble: 2 × 7\n  var    value .lower .upper .width .point .interval\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 diff_a  0.61   0.51   0.71   0.89 mean   qi       \n2 diff_p  0.14   0.12   0.16   0.89 mean   qi       \n\n\nwe firs get the posterior expected fit\n\nepred11_07 &lt;- dataAdmit |&gt;\n  add_epred_rvars(fit11_07) |&gt;\n  mutate(.epred_pct = .epred / applications) |&gt;\n  select(dept, case, .epred_pct) |&gt;\n  group_by(dept, case) |&gt;\n  mean_qi(.epred_pct)\n# glimpse(pred11_07)\n\nand the posterior validation check for model fit11_07 is\n\nglimpse(dataAdmit)\n\nRows: 12\nColumns: 8\n$ dept         &lt;fct&gt; A, A, B, B, C, C, D, D, E, E, F, F\n$ gender       &lt;fct&gt; male, female, male, female, male, female, male, female, m…\n$ admit        &lt;int&gt; 512, 89, 353, 17, 120, 202, 138, 131, 53, 94, 22, 24\n$ reject       &lt;int&gt; 313, 19, 207, 8, 205, 391, 279, 244, 138, 299, 351, 317\n$ applications &lt;int&gt; 825, 108, 560, 25, 325, 593, 417, 375, 191, 393, 373, 341\n$ case         &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ admit_pct    &lt;dbl&gt; 0.62060606, 0.82407407, 0.63035714, 0.68000000, 0.3692307…\n$ reject_pct   &lt;dbl&gt; 0.3793939, 0.1759259, 0.3696429, 0.3200000, 0.6307692, 0.…\n\n\n\nplot11_07 &lt;- list()\nplot11_07 &lt;- within(plot11_07, {\n  data &lt;- dataAdmit |&gt;\n    mutate(label = sprintf(\"%s=%0.2f\", substr(as.character(gender), 1, 1), admit_pct)) |&gt;\n    group_by(case, dept, label) |&gt;\n    summarize(admit_pct = mean(admit_pct))\n  \n  p &lt;- ggplot(data, aes(x = case, y = admit_pct, color = dept, fill = dept)) +\n    geom_line(aes(group = dept)) +\n    geom_point() +\n    geom_hline(yintercept = 0.5, color = \"brown\", linetype = 2) +\n    ggrepel::geom_text_repel(aes(label = label), size = 3) +\n    scale_y_continuous(labels = scales::label_percent()) +\n    scale_color_paletteer_d(\"ggthemes::Classic_10\") +\n    scale_fill_paletteer_d(\"ggthemes::Classic_10\") +\n    coord_cartesian(ylim = c(0, 1)) +\n    theme(legend.position = c(0.8, 0.8),\n          legend.direction = \"horizontal\",\n          legend.text = element_text(size = rel(0.8))) +\n    labs(title = \"Posterior validation check\", x = NULL)\n  \n  p &lt;- p +\n    geom_point(data = epred11_07, aes(x = case, y = .epred_pct, group = dept),\n               shape = 1, color = \"darkblue\") +\n    geom_errorbar(data = epred11_07,  \n                  aes(x = case, y = .epred_pct, ymin = .lower, ymax = .upper, group = dept),\n                  color = \"darkblue\", width = 1/4)\n})\n\n`summarise()` has grouped output by 'case', 'dept'. You can override using the\n`.groups` argument.\n\n# plot11_07$data\nplot11_07$p\n\n\n\n\nFigure 11.5\n\n\n\n\nand the full model is\n\\[\n\\begin{align*}\nadmit_i &\\sim \\mathcal{Binomial}(n_i, p_i) \\\\\nlogit(p_i) &= \\alpha_{gid[i]} + \\delta_{dept[i]} \\\\\n\\alpha_j &\\sim \\mathcal{N}(0, 1.5) \\\\\n\\delta_k &\\sim \\mathcal{N}(0, 1.5)\n\\end{align*}\n\\]\nand we fit the model\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit11_08 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataAdmit,\n             family = binomial,\n             formula = admit | trials(applications) ~ 0 + (1|gender) + (1|dept),\n             prior(normal(0, 1.5), class = sd),\n             iter = 2000, warmup = 1000, chains = 4,\n             cores = detectCores(), seed = 1151)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_08\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.23 sec elapsed\n\n\n\nposterior_summary(fit11_08) |&gt;\n  round(digits = 2)\n\n                           Estimate Est.Error   Q2.5  Q97.5\nsd_dept__Intercept             1.39      0.42   0.79   2.44\nsd_gender__Intercept           0.40      0.46   0.01   1.66\nr_dept[A,Intercept]            0.73      0.25   0.35   1.38\nr_dept[B,Intercept]            0.68      0.25   0.28   1.34\nr_dept[C,Intercept]           -0.52      0.24  -0.90   0.09\nr_dept[D,Intercept]           -0.56      0.24  -0.95   0.06\nr_dept[E,Intercept]           -0.99      0.25  -1.40  -0.35\nr_dept[F,Intercept]           -2.55      0.28  -3.03  -1.88\nr_gender[female,Intercept]    -0.07      0.23  -0.70   0.33\nr_gender[male,Intercept]      -0.15      0.24  -0.80   0.22\nlprior                        -1.81      0.38  -2.95  -1.43\nlp__                         -62.23      3.04 -68.91 -57.32\n\n\nwith the posterior distributions\n\n# post11_08 &lt;- expand.grid(\n#   gender = dataAdmit$gender,\n#   dept = dataAdmit$dept) |&gt;\npost11_08 &lt;- spread_rvars(fit11_08, r_gender[gender, ])\npost11_08\n\n# A tibble: 2 × 2\n  gender   r_gender[,1]\n  &lt;chr&gt;      &lt;rvar[,1]&gt;\n1 female  -0.065 ± 0.23\n2 male    -0.146 ± 0.24\n\n\nand again we compute the contrast between male and female\n\nfit11_08 |&gt;\n  spread_rvars(r_gender[gender, ]) |&gt;\n  pivot_wider(names_from = gender, values_from = r_gender) |&gt;\n  transmute(diff_a = male - female,\n         diff_p = inv_logit_scaled(male) - inv_logit_scaled(female)) |&gt;\n  pivot_longer(cols = everything(), names_to = \"var\", values_to = \"value\") |&gt;\n  mean_qi(value, .width = 0.89) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n# A tibble: 2 × 7\n  var    value .lower .upper .width .point .interval\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 diff_a -0.08  -0.21   0.03   0.89 mean   qi       \n2 diff_p -0.02  -0.05   0.01   0.89 mean   qi       \n\n\nand again the results are very close to McElreath’s.\nThis leads us to hypotheses that the department is a confound and we could have the following DAG\n\nggdag::dagify(A ~ D + G, D ~ G) |&gt;\n    ggdag::ggdag_classic(layout = \"sugiyama\") +\n    ggdag::theme_dag_blank(\n      panel.background = element_rect(fill = \"snow2\", color = \"snow2\"))"
  },
  {
    "objectID": "ch11_counting.html#poisson-regression",
    "href": "ch11_counting.html#poisson-regression",
    "title": "11  Counting and Classification",
    "section": "11.2 Poisson regression",
    "text": "11.2 Poisson regression\n\\[\n\\begin{align*}\ny_i &\\sim \\mathcal{Poisson}(\\lambda_i) \\\\\n\\log{\\lambda_i} &= \\alpha + \\beta (x_i - \\bar{x})\n\\end{align*}\n\\]\n\n11.2.1 Example: Oceanic tool complexity\n\ndata(Kline)\ndataKline &lt;- Kline |&gt;\n  mutate(log_pop = log(population),\n         log_pop_s = scale(log_pop),\n         cid = factor(contact, levels = c(\"low\", \"high\")))\nrm(Kline)\ndataKline |&gt;\n  skim() |&gt;\n  select(-n_missing, -complete_rate) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n\nData summary\n\n\nName\ndataKline\n\n\nNumber of rows\n10\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\nskim_variable\nordered\nn_unique\ntop_counts\n\n\n\n\nculture\nFALSE\n10\nChu: 1, Haw: 1, Lau: 1, Mal: 1\n\n\ncontact\nFALSE\n2\nhig: 5, low: 5\n\n\ncid\nFALSE\n2\nlow: 5, hig: 5\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npopulation\n34109.10\n84793.03\n1100.00\n3897.75\n7700.00\n12050.00\n275000.00\n▇▁▁▁▁\n\n\ntotal_tools\n34.80\n17.85\n13.00\n22.50\n30.50\n42.25\n71.00\n▇▃▃▂▂\n\n\nmean_TU\n4.83\n1.14\n3.20\n4.00\n4.85\n5.30\n6.60\n▅▅▇▂▅\n\n\nlog_pop\n8.98\n1.53\n7.00\n8.26\n8.95\n9.39\n12.52\n▃▇▃▁▂\n\n\nlog_pop_s\n0.00\n1.00\n-1.29\n-0.47\n-0.02\n0.27\n2.32\n▃▇▃▁▂\n\n\n\n\n\nthe model is\n\\[\ntotal\\_tools_i \\sim \\mathcal{Poisson}(\\lambda_i) \\\\\n\\log{\\lambda_i} = \\alpha_{cid[i]} + \\beta_{cid[i]} \\log{log\\_pop\\_s_i} \\\\\n\\alpha_j \\sim \\mathcal{N}(0, ?) \\\\\n\\beta_k \\sim \\mathcal{N}(0, ?)\n\\]\n\n11.2.1.1 Calibrating the priors\n\nSource: https://ggplot2.tidyverse.org/reference/geom_function.html\n\nFor the intercept \\(\\alpha_j\\). If \\(\\alpha_j\\) is normal then we know that \\(\\lambda_j\\) is lognormal distributed.\n\n11.2.1.1.1 With simpr\n\nset.seed(1153)\nsimKline1 &lt;- simpr::specify(\n  a = ~ rnorm(n = 100, mean = the_mean, the_sd),\n  lambda = ~ exp(a),\n  m = ~ the_mean,\n  s = ~ the_sd) |&gt;\n  define(the_mean = c(0, 2, 3), the_sd = c(0.5, 1, 2)) |&gt;\n  generate(1)\nsimKline1 &lt;- simKline1$sim |&gt;\n  bind_rows(.id = \"id\")\n\n\nggplot(simKline1, aes(x = lambda, fill = as.factor(m), color = as.factor(m))) +\n  geom_density(aes(y = after_stat(scaled))) +\n  scale_fill_paletteer_d(\"khroma::vibrant\") +\n  scale_color_paletteer_d(\"khroma::vibrant\") +\n  coord_cartesian(xlim = c(0, 100)) +\n  theme(legend.position = \"none\",\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) +\n  labs(title = \"Prior predictive distribution of the mean (lambda)\",\n       x = NULL, y = NULL) +\n  facet_grid(sprintf(\"sd of a=%.1f\", s) ~ sprintf(\"mean of a=%.1f\", m))\n\n\n\n\n\n\n11.2.1.1.2 as per textbook\n\nplotKline1 &lt;- list()\nplotKline1 &lt;- within(plotKline1, {\n  df &lt;- crossing(\"meanlog\" = c(0, 2, 3), \"sdlog\" = c(0.5, 1, 2)) |&gt;\n    expand(nesting(meanlog, sdlog), x = seq(from = 0, to = 100, length.out = 50)) |&gt;\n    mutate(density = dlnorm(x, meanlog = meanlog, sdlog = sdlog),\n         meanid = factor(paste(\"meanlog =\", meanlog)),\n         sdid = factor(paste(\"sdlog =\", sdlog))) |&gt;\n    arrange(meanlog, sdlog)\n  \n  pplotKline1 &lt;- ggplot(df, aes(x = x, y = density, fill = meanid)) +\n    geom_area() +\n    scale_y_continuous(breaks = NULL) +\n    scale_fill_paletteer_d(\"khroma::vibrant\") +\n    coord_cartesian(xlim = c(0, 50)) +\n    theme(legend.position = \"none\",\n        axis.text.y = element_blank()) +\n    labs(title = \"Prior predictive distribution of the mean (lambda)\", \n       x = NULL, y = NULL) +\n    facet_grid(sdid ~ meanid, scales = \"free_y\")\n})\nplotKline1$p\n\n\n\n\nTherefore we choose \\(\\alpha_{cid[i]} \\sim \\mathcal{LogNormal(3, 0.5)}\\) as our prior for \\(\\alpha_{cid[i]}\\).\nUsing our prior for \\(\\alpha_{cid[i]}\\) we simulate \\(\\beta_{cid[i]}\\). We show the simulation on the natural scale as it is much easier to understand\n\n\n11.2.1.1.3 slope with simpr\n\nsimKline2 &lt;- list()\nsimKline2 &lt;- within(simKline2, {\n  set.seed(1153)\n  sim1 &lt;- simpr::specify(\n    a = ~ rnorm(n = 1, mean = 3, 0.5),\n    b = ~ rnorm(n = 1, mean = 0, sd = 0.2),\n    pop_log = ~ seq(from = -2, to = 2, by = 0.1),\n    tools = ~ exp(a + b * pop_log)) |&gt;\n    generate(50) |&gt;\n    identity()\n  \n  pdf1 &lt;- sim1 |&gt;\n    unnest(sim) |&gt;\n    identity()\n  \n})\n\n\nplotKline2 &lt;- list()\nplotKline2 &lt;- within(plotKline2, {\n  p1 &lt;- ggplot(simKline2$pdf1, \n                    aes(x = pop_log, y = tools, group = rep, color = b)) +\n    geom_line() +\n    scale_color_paletteer_c(\"pals::ocean.speed\") +\n    coord_cartesian(ylim = c(0, 100)) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Prior predictive distribution of the mean tools (lambda)\",\n         subtitle = expression(paste(a %~% N(mu == 3, sigma == 0.5), \", \", b %~% N(mu == 0, sigma == 0.2))),\n       x = \"Standard log population\", y = \"mean total tools(lambda)\")\n})\nplotKline2$p1\n\n\n\n\n\nsimKline2 &lt;- within(simKline2, {\n  sim2 &lt;- simpr::specify(\n    a = ~ rnorm(n = 1, mean = 3, 0.5),\n    b = ~ rnorm(n = 1, mean = 0, sd = 10),\n    pop_log = ~ seq(from = -2, to = 2, by = 0.1),\n    tools = ~ exp(a + b * pop_log)) |&gt;\n    generate(50) |&gt;\n    identity()\n  \n  pdf2 &lt;- sim2 |&gt;\n    unnest(sim) |&gt;\n    identity()\n})\n\n\nplotKline2 &lt;- within(plotKline2, {\n  p2 &lt;- ggplot(simKline2$pdf2, \n                    aes(x = pop_log, y = tools, group = rep, color = b)) +\n    geom_line() +\n    scale_color_paletteer_c(\"pals::ocean.haline\") +\n    coord_cartesian(ylim = c(0, 100)) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Prior predictive distribution of the mean tools (lambda)\",\n         subtitle = expression(paste(a %~% N(mu == 3, sigma == 0.5), \", \", b %~% N(mu == 0, sigma == 10))),\n       x = \"Standard log population\", y = \"mean total tools(lambda)\")\n})\nplotKline2$p2\n\n\n\n\n\nsimKline2 &lt;- within(simKline2, {\n  sim3 &lt;- simpr::specify(\n    a = ~ rnorm(n = 1, mean = 3, 0.5),\n    b = ~ rnorm(n = 1, mean = 0, sd = 0.2),\n    pop_log = ~ seq(from = log(100), to = log(2e5), length.out = 40),\n    tools = ~ exp(a + b * pop_log)) |&gt;\n    generate(50) |&gt;\n    identity()\n  \n  pdf3 &lt;- sim3 |&gt;\n    unnest(sim) |&gt;\n    identity()\n})\n\n\nplotKline2 &lt;- within(plotKline2, {\n  p3 &lt;- ggplot(simKline2$pdf3, \n                    aes(x = pop_log, y = tools, group = rep, color = b)) +\n    geom_line() +\n    scale_color_paletteer_c(\"pals::ocean.delta\") +\n    coord_cartesian(ylim = c(0, 500)) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Prior predictive distribution of the mean tools (lambda)\",\n         subtitle = expression(paste(a %~% N(mu == 3, sigma == 0.5), \", \", b %~% N(mu == 0, sigma == 0.2))),\n       x = \"log population\", y = \"mean total tools (lambda)\")\n})\nplotKline2$p3\n\n\n\n\n\nsimKline2 &lt;- within(simKline2, {\n  sim4 &lt;- simpr::specify(\n    a = ~ rnorm(n = 1, mean = 3, 0.5),\n    b = ~ rnorm(n = 1, mean = 0, sd = 0.2),\n    pop_log = ~ seq(from = log(100), to = log(2e5), length.out = 40),\n    pop = ~ exp(pop_log),\n    tools = ~ exp(a + b * pop_log)) |&gt;\n    generate(50) |&gt;\n    identity()\n  \n  pdf4 &lt;- sim4 |&gt;\n    unnest(sim) |&gt;\n    identity()\n})\n\n\nplotKline2 &lt;- within(plotKline2, {\n  p4 &lt;- ggplot(simKline2$pdf4, \n                    aes(x = pop, y = tools, group = rep, color = b)) +\n    geom_line() +\n    scale_color_paletteer_c(\"pals::ocean.thermal\") +\n    coord_cartesian(ylim = c(0, 500)) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Prior predictive distribution of the mean tools (lambda)\",\n         subtitle = expression(paste(a %~% N(mu == 3, sigma == 0.5), \", \", b %~% N(mu == 0, sigma == 0.2))),\n       x = \"population\", y = \"mean total tools (lambda)\")\n})\nplotKline2$p4\n\n\n\n\n\nnames(plotKline2)\n\n[1] \"p1\" \"p2\" \"p3\" \"p4\"\n\nplotKline2[c(\"p3\")]\n\n$p3\n\n\n\n\npatchwork::wrap_plots(plotKline2[c(\"p1\", \"p2\", \"p3\", \"p4\")])\n\n\n\n\n\n\n\n11.2.1.2 Model and fit\nThe model with the priors as explained just above is\n\\[\ntotal\\_tools_i \\sim \\mathcal{Poisson}(\\lambda_i) \\\\\n\\log{\\lambda_i} = \\alpha_{cid[i]} + \\beta_{cid[i]} \\log{log\\_pop\\_s_i} \\\\\n\\alpha_j \\sim \\mathcal{N}(3, 0.5) \\\\\n\\beta_k \\sim \\mathcal{N}(0, 0.2)\n\\]\nThe fit with intercept only\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit11_09 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataKline,\n             family = poisson,\n             formula = total_tools ~ 1,\n             prior = c(prior(normal(3, 0.5), class = Intercept)),\n             iter = 1000, warmup = 500, chains = 2, \n             cores = detectCores(), seed = 1163)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_09\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.22 sec elapsed\n\n\n\nposterior_summary(fit11_09)\n\n               Estimate  Est.Error       Q2.5       Q97.5\nb_Intercept   3.5385119 0.05264197   3.433214   3.6408872\nlprior       -0.8113182 0.11347312  -1.047264  -0.6011393\nlp__        -67.0173429 0.66772263 -68.967360 -66.5318055\n\n\nand the model with the interaction between population and contact\n\nget_prior(formula = bf(total_tools ~ 0 + (1 + log_pop_s | cid)),\n          data = dataKline, family = poisson)\n\n                prior class      coef group resp dpar nlpar lb ub       source\n               lkj(1)   cor                                            default\n               lkj(1)   cor             cid                       (vectorized)\n student_t(3, 0, 2.5)    sd                                  0         default\n student_t(3, 0, 2.5)    sd             cid                  0    (vectorized)\n student_t(3, 0, 2.5)    sd Intercept   cid                  0    (vectorized)\n student_t(3, 0, 2.5)    sd log_pop_s   cid                  0    (vectorized)\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit11_10 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataKline,\n             family = poisson,\n             formula = bf(total_tools ~ 0 + (1 + log_pop_s | cid)),\n             prior = c(prior(normal(3, 0.5), class = sd, coef = Intercept, group = cid),\n                       prior(normal(0, 0.2), class = sd, coef = log_pop_s, group = cid)),\n             iter = 1000, warmup = 500, chains = 2,\n             cores = detectCores(), seed = 1171)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_10\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.25 sec elapsed\n\n\n\nposterior_summary(fit11_10) |&gt;\n  round(digits = 3)\n\n                              Estimate Est.Error    Q2.5   Q97.5\nsd_cid__Intercept                2.964     0.463   2.058   3.920\nsd_cid__log_pop_s                0.310     0.088   0.168   0.506\ncor_cid__Intercept__log_pop_s    0.640     0.321  -0.204   0.992\nr_cid[low,Intercept]             3.326     0.087   3.147   3.497\nr_cid[high,Intercept]            3.614     0.070   3.480   3.741\nr_cid[low,log_pop_s]             0.385     0.053   0.278   0.487\nr_cid[high,log_pop_s]            0.350     0.170   0.026   0.703\nlprior                          -1.262     0.936  -3.660  -0.021\nlp__                           -46.024     2.099 -50.731 -43.067\n\n\nand we compare the LOO\n\nloo::loo_compare(fit11_09, fit11_10, criterion = \"loo\") |&gt;\n  print(simplify = FALSE)\n\n         elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic\nfit11_10   0.0       0.0   -41.1      6.0         5.7   1.9     82.2  12.1   \nfit11_09 -29.3      17.0   -70.4     16.8         7.8   3.3    140.8  33.5   \n\n\nand we look at the pareto k since a warning was issued by add_criterion() above\n\nloo::loo(fit11_10) |&gt; \n  loo::pareto_k_table()\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     5     50.0%   426       \n (0.5, 0.7]   (ok)       3     30.0%   109       \n   (0.7, 1]   (bad)      2     20.0%   49        \n   (1, Inf)   (very bad) 0      0.0%   &lt;NA&gt;      \n\n\nand we add the pareto k to the data\n\n# append k value to data\ndataKline &lt;- dataKline |&gt;\n  mutate(ParetoK = round(fit11_10$criteria$loo$diagnostics$pareto_k, 1))\nstopifnot(!any(is.na(dataKline)))\n\ndataKline |&gt;\n  select(culture, ParetoK) |&gt;\n  arrange(desc(ParetoK))\n\n      culture ParetoK\n1      Hawaii     0.8\n2         Yap     0.7\n3       Tonga     0.7\n4   Trobriand     0.6\n5     Tikopia     0.5\n6    Malekula     0.4\n7  Santa Cruz     0.4\n8    Lau Fiji     0.3\n9       Manus     0.2\n10      Chuuk     0.1\n\n\nwhich shows that Hawaii is the outlier and is very influential.\n\n\n11.2.1.3 Plotting the posterior\nThe expected predicitons\n\nepred11_10 &lt;- expand_grid(\n  cid = unique(dataKline$cid),\n  log_pop_s = seq_range(dataKline$log_pop_s, n = 20, pretty = TRUE)) |&gt;\n  add_epred_rvars(fit11_10) |&gt;\n  mean_qi(.epred, .width = 0.89) |&gt;\n  mutate(population = log_pop_s * sd(log(dataKline$population)) + \n             mean(log(dataKline$population)),\n           population = round(exp(population), 0))\n# epred11_10\n\n\nplot11_10 &lt;- list()\nplot11_10 &lt;- within(plot11_10, {\n  p_log &lt;- ggplot(dataKline, aes(x = log_pop_s, y = total_tools, color = cid)) +\n  geom_smooth(epred11_10,\n              mapping = aes(x = log_pop_s, y = .epred, ymin = .lower,\n                            ymax = .upper, fill = cid, color = cid),\n              inherit.aes = FALSE, stat = \"identity\") +\n  geom_point(aes(size = ParetoK), show.legend = FALSE) +\n  ggrepel::geom_text_repel(aes(label = paste0(culture, \"(\", ParetoK, \")\")), size = 3) +\n  coord_cartesian(ylim = c(0, 100)) +\n  labs(x = \"standardized population log\")\n  \n  p_nat &lt;- ggplot(dataKline, aes(x = population, y = total_tools, color = cid)) +\n  geom_smooth(epred11_10,\n              mapping = aes(x = population, y = .epred, ymin = .lower,\n                            ymax = .upper, fill = cid, color = cid),\n              inherit.aes = FALSE, stat = \"identity\") +\n  geom_point(aes(size = ParetoK), show.legend = FALSE) +\n  ggrepel::geom_text_repel(aes(label = paste0(culture, \"(\", ParetoK, \")\")), size = 3) +\n  coord_cartesian(ylim = c(0, 100)) +\n  scale_x_continuous(breaks = scales::breaks_extended(n = 5),\n                     labels = scales::label_number(scale = 0.001)) +\n  labs(x = \"population in thousands\")\n})\nwrap_plots(plot11_10[c(\"p_log\", \"p_nat\")]) &\n  scale_color_paletteer_d(\"khroma::bright\") &\n  scale_fill_paletteer_d(\"khroma::bright\") &\n  theme(legend.position = c(0.2, 0.90),\n        plot.background = element_rect(color = NA)) &\n  plot_annotation(title = \"Posterior fitted values for Oceanic Tools model\",\n                  subtitle = \"Model b11.10 - Size of points is the paretor_k factor\")\n\n\n\n\n\n\n11.2.1.4 Overthinking: Modeling tool innovation\nUsing the scientific approach with and ODE (ordinary differential equation)\n\\[\n\\Delta T = \\alpha P^\\beta - \\gamma T\n\\]\nwhich as an equilibrium point at \\(\\Delta T = 0\\) and therefore\n\\[\n\\hat{T} = \\frac{\\alpha P^\\beta}{\\gamma}\n\\]\nwith the theorical model which has no link function\n\\[\n\\begin{align*}\nT_i &\\sim \\mathcal{Poisson}(\\lambda_i) \\\\\n\\lambda_i &\\sim \\frac{\\alpha P^\\beta}{\\gamma}\n\\end{align*}\n\\]\nin practice, the model is modified to exponentiate \\(\\alpha\\) to ensure it is always positive\n\\[\n\\begin{align*}\ntotal\\_tools_i &\\sim \\mathcal{Poisson}(\\lambda_i) \\\\\n\\lambda_i &\\sim \\exp(\\alpha_{cid[i]}) \\frac{population_i^{\\beta_{cid[i]}}}{\\gamma} \\\\\n\\alpha_j &\\sim \\mathcal{N}(1, 1) \\\\\n\\beta_j &\\sim \\mathcal{Exp}(1) \\\\\n\\gamma &\\sim \\mathcal{Exp}(1) \\\\\n\\end{align*}\n\\]\nand the fit, see identity in poisson(link = “identity”), this is important and read warning from Kurtz on this.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit11_11 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataKline,\n      family = poisson(link = \"identity\"),\n      bf(total_tools ~ exp(a) * population^b / g,\n         a + b ~ 0 + cid,\n         g ~ 1,\n         nl = TRUE),\n      prior = c(prior(normal(1, 1), nlpar = a),\n                prior(exponential(1), nlpar = b, lb = 0),\n                prior(exponential(1), nlpar = g, lb = 0)),\n      iter = 1000, warmup = 500, chains = 2,\n      cores = detectCores(), seed = 1181,\n      control = list(adapt_delta = .95))\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_11\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.26 sec elapsed\n\n\n\nposterior_summary(fit11_11) |&gt;\n  round(digits = 2)\n\n              Estimate Est.Error   Q2.5  Q97.5\nb_a_cidlow        0.97      0.66  -0.38   2.22\nb_a_cidhigh       0.91      0.84  -0.76   2.52\nb_b_cidlow        0.26      0.03   0.19   0.32\nb_b_cidhigh       0.30      0.10   0.12   0.50\nb_g_Intercept     1.22      0.82   0.25   3.46\nlprior           -4.19      1.12  -7.09  -2.90\nlp__            -44.00      1.58 -47.77 -41.78\n\n\nwith the expected predictions\n\nepred11_11 &lt;- expand_grid(\n  cid = unique(dataKline$cid),\n  population = seq_range(dataKline$population, n = 20, pretty = TRUE)) |&gt;\n  add_epred_rvars(fit11_11) |&gt;\n  mean_qi(.epred, .width = 0.89)\n\n\nggplot(dataKline, aes(x = population, y = total_tools, color = cid)) +\n  geom_smooth(epred11_11,\n              mapping = aes(x = population, y = .epred, ymin = .lower,\n                            ymax = .upper, fill = cid, color = cid),\n              inherit.aes = FALSE, stat = \"identity\") +\n  geom_point(aes(size = ParetoK), show.legend = FALSE) +\n  ggrepel::geom_text_repel(aes(label = paste0(culture, \"(\", ParetoK, \")\")), size = 3) +\n  scale_x_continuous(breaks = scales::breaks_extended(n = 5),\n                     labels = scales::label_number(scale = 0.001)) +\n  scale_color_paletteer_d(\"khroma::bright\") +\n  scale_fill_paletteer_d(\"khroma::bright\") +\n  scale_size_continuous() +\n  theme(legend.position = c(0.2, 0.8)) +\n  labs(title = \"Fitted values with the scientific model\",\n       subtitle = \"model b11.11\",\n       x = \"population in thousands\")\n\n\n\n\n\n\n11.2.1.5 final model comparison\n\nloo::loo_compare(fit11_09, fit11_10, fit11_11, criterion = \"loo\") |&gt;\n  print(simplify = FALSE)\n\n         elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic\nfit11_11   0.0       0.0   -40.5      5.9         5.3   1.8     81.1  11.9   \nfit11_10  -0.5       1.1   -41.1      6.0         5.7   1.9     82.2  12.1   \nfit11_09 -29.9      17.1   -70.4     16.8         7.8   3.3    140.8  33.5   \n\n\nSo the model b11.11 is slightly better. Note however that the difference is well within the standard deviation so that we can actually say that the 2 are as accurate. The scientific model is more interpretable nonetheless.\n\n\n\n11.2.2 Negative binomial (gamma-Poisson) models\nThis distribution is covered in chapter 12.\n\nA very comon extension of Poisson GLM is to swap the Poisson distribution for something called the Negative Binomial distribution, also called Poisson-Gamma. It s a Poisson in disguise because it is a mixture of differrent Poisson distribution.\n\n\n\n11.2.3 Example: Exposure and the offset\nWhen we have different unit of times, or distance (or other denominator), \\(\\tau_i\\) for expected number of events \\(\\mu_i\\) then\n\\[\n\\lambda = \\frac{\\mu}{\\tau}\n\\]\nand now the link is\n\\[\n\\begin{align*}\n\\log{\\lambda_i} &= \\log{\\frac{\\mu_i}{\\tau_i}}=\\alpha + \\beta x_i \\\\\n\\log{\\lambda_i} &= \\log{\\mu_i} - log{\\tau_i}=\\alpha + \\beta x_i \\\\\n&\\therefore \\\\\n\\log{\\mu_i} &= log{\\tau_i} + \\alpha + \\beta x_i\n\\end{align*}\n\\]\nWhen \\(\\tau_i = 1\\) then \\(\\log{\\tau_i} = 0\\) and we recover the original GLM link.\n\n11.2.3.1 Example: Monastery with varying \\(\\tau_i\\)\n\nndays &lt;- 30  # nb of days\nydays &lt;- rpois(ndays, lambda = 1.5)  # nb of manuscripts per day\nnweeks &lt;- 4\nyweeks &lt;- rpois(nweeks, 0.5*7)  # nb of manuscripts per week\n# create the dataframe with all data\ndataMonastery &lt;- data.frame(\n  nb = c(ydays, yweeks),\n  days = c(rep(1, ndays), rep(7, nweeks)),\n  monastery = c(rep(0, ndays), rep(1, nweeks))) |&gt;\n  mutate(days_lg = log(days))\n\nthe model is\n\\[\n\\begin{align*}\nnb_i &\\sim \\mathcal{Poisson}(\\mu_i) \\\\\n\\log{\\mu_i} &= log(days_i) + \\alpha + \\beta \\cdot monastery_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 1) \\\\\n\\beta &\\sim \\mathcal{N}(0, 1) \\\\\n\\end{align*}\n\\]\nand the fit. With brms you use the offset() function.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"100 secs.\"))\nfit11_12 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataMonastery,\n      family = poisson,\n      nb ~ 1 + offset(days_lg) + monastery,\n      prior = c(prior(normal(0, 1), class = Intercept),\n                prior(normal(0, 1), class = b)),\n      iter = 1000, warmup = 500, chains = 2,\n      cores = detectCores(), seed = 1187)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_12\")\ntictoc::toc()\n\nrun time of 100 secs., use the cache.: 0.26 sec elapsed\n\n\n\nfit11_12\n\n Family: poisson \n  Links: mu = log \nFormula: nb ~ 1 + offset(days_lg) + monastery \n   Data: dataMonastery (Number of observations: 34) \n  Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 1000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.21      0.15    -0.12     0.48 1.00      437      440\nmonastery    -1.07      0.31    -1.72    -0.49 1.00      609      680\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand to get the rates on the natural scale we use\n\\[\n\\begin{align*}\n\\lambda_{monastery[0]} &= \\exp{(\\alpha)} \\\\\n\\lambda_{monastery[1]} &= \\exp{(\\alpha + \\beta)}\n\\end{align*}\n\\]\nand the results are\n\nspread_rvars(fit11_12, b_Intercept, b_monastery) |&gt;\n  mutate(lambda_old = exp(b_Intercept),\n         lambda_new = exp(b_Intercept + b_monastery)) |&gt;\n  pivot_longer(starts_with(\"lambda\"), names_to = \"monastery\") |&gt;\n  mutate(monastery = factor(monastery, levels = c(\"lambda_old\", \"lambda_new\"))) |&gt;\n  group_by(monastery) |&gt;\n  mean_qi(value, .width = .89) |&gt; \n  mutate(across(.cols = where(is.double), .fns = round, digits = 2)) |&gt;\n  select(-b_Intercept, -b_monastery)\n\n# A tibble: 2 × 7\n  monastery  value .lower .upper .width .point .interval\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 lambda_old  1.24   0.95   1.54   0.89 mean   qi       \n2 lambda_new  0.44   0.27   0.66   0.89 mean   qi"
  },
  {
    "objectID": "ch11_counting.html#multinomial-and-categorical-models",
    "href": "ch11_counting.html#multinomial-and-categorical-models",
    "title": "11  Counting and Classification",
    "section": "11.3 Multinomial and categorical models",
    "text": "11.3 Multinomial and categorical models\nImportant: It is important to read Kurz (2020) in this section because McElreath seems to have obtained the wrong results. Kurtz gives significantly more details and extrememly important explanations\n\\[\n\\begin{align*}\nPr(y_1, \\ldots, y_K \\mid n, p_1, \\ldots, p_K) &=\n\\frac{n!}{\\prod_i y_i !} \\prod_{i=1}^{K} p_i^{y_i} \\\\\n&=\\binom{n}{y_1, \\ldots, y_K} \\prod_{i=1}^{K} p_i^{y_i}\n\\end{align*}\n\\]\nand the multinomial logit, called softmax is\n\\[\nPr(k \\mid s_1, s_2 \\ldots, s_K) = \\frac{\\exp{(s_k)}}{\\sum_{i=1}^{K}\\exp{(s_i)}}\n\\]\n\n11.3.1 Predictors matched to outcomes\nThe career are the outcomes. We now predict the career using a trait of the career (outcome itself) which is the income in this case.\n\nsimCareer &lt;- list()\nsimCareer &lt;- within(simCareer, {\n  income &lt;- c(1, 2, 5)\n  score &lt;- 0.5 * income\n  probs &lt;- round(rethinking::softmax(score), 3)\n  stopifnot(sum(probs) == 1)  # verify rounding is ok\n  \n  data &lt;- data.frame(\n    career = sample(1:3, size = 500, prob = probs, replace = TRUE)\n  )\n})\n\n# and we validate the results\ntabulate(simCareer$data$career) / nrow(simCareer$data)\n\n[1] 0.108 0.194 0.698\n\nsimCareer$probs\n\n[1] 0.100 0.164 0.736\n\n\nand the dataframe is\n\nsimCareer$data |&gt;\n  count(career) |&gt;\n  mutate(pct = 100 * n / sum(n),\n         prob = n / sum(n))\n\n  career   n  pct  prob\n1      1  54 10.8 0.108\n2      2  97 19.4 0.194\n3      3 349 69.8 0.698\n\n\nand plot the frequency of each career\n\nplotCareer &lt;- list()\nplotCareer &lt;- within(plotCareer, {\n  df &lt;- simCareer$data |&gt;\n    count(career) |&gt;\n    mutate(pct = round(n / sum(n), 3))\n  \n  p &lt;-ggplot(df, aes(x = factor(career), y = pct, fill = factor(career))) +\n    geom_bar(stat = \"identity\") +\n    geom_text(aes(label = sprintf(\"%0.1f%%\", 100 * pct)), vjust = 1.25) +\n    scale_fill_paletteer_d(\"khroma::vibrant\") +\n    theme(legend.position = \"none\",\n        axis.ticks.y = element_blank(),\n        axis.text.y = element_blank()) +\n    labs(title = \"Frequencies of careers\",\n       subtitle = sprintf(\"sample size = %d\", nrow(simCareer$data)),\n       x = \"career\", y = NULL)\n})\nplotCareer$p\n\n\n\n\n\n11.3.1.1 Scores\n\nScores can be thought of as weights.\nTheir exact values are not much important as their difference from one another.\n\nFor example if you add a constant to the scores from above, you get the same softmax\n\nscore_new &lt;- simCareer$score + 11  # 11 is an arbitrary constant added to the scores\n# compute the new softmax\nexp(score_new) / sum(exp(score_new))\n\n[1] 0.09962365 0.16425163 0.73612472\n\n# which gives the same result and shows that the difference between\n# the scores is what matters\nrethinking::softmax(score_new)\n\n[1] 0.09962365 0.16425163 0.73612472\n\n\n\n\n11.3.1.2 Model of predictors matched to outcomes\n\\[\n\\begin{align*}\n\\overrightarrow{career} &\\sim \\mathcal{multinomial(career_1, career_2, career_3)} =  \\binom{n}{career_1, career_2, career_3} \\prod_{i=1}^{3} p_i^{career_i}\\\\\np_1 &= \\frac{\\exp{(score_1)}}{\\sum_1^3\\exp{(score_i)}} \\\\\np_2 &= \\frac{\\exp{(score_2)}}{\\sum_1^3\\exp{(score_i)}} \\\\\np_3 &= \\frac{\\exp{(score_3)}}{\\sum_1^3\\exp{(score_i)}} \\\\\nscore_1 &= \\alpha_1 + \\beta \\cdot income_1 \\\\\nscore_2 &= \\alpha_2 + \\beta \\cdot income_2 \\\\\nscore_3 &= \\alpha_3 + \\beta \\cdot income_3 \\\\\n\\alpha_1 &\\sim \\mathcal{N}(0, 1) \\\\\n\\alpha_2 &\\sim \\mathcal{N}(0, 1) \\\\\n\\alpha_3 &\\sim \\mathcal{N}(0, 1) \\\\\n\\beta &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\end{align*}\n\\]\n\n\n11.3.1.3 Fit with stan\nWe fit with stan using the same code as mcElreath to demonstrate his results are different\n\n# define the model\nfit11_13_code &lt;- \"\ndata{\n  int N; // number of individuals\n  int K; // number of possible careers \n  int career[N]; // outcome\n  vector[K] career_income;\n}\nparameters{\n  vector[K - 1] a; // intercepts\n  real&lt;lower=0&gt; b; // association of income with choice\n}\nmodel{\n  vector[K] p;\n  vector[K] s;\n  a ~ normal(0, 1);\n  b ~ normal(0, 0.5);\n  s[1] = a[1] + b * career_income[1]; \n  s[2] = a[2] + b * career_income[2]; \n  s[3] = 0; // pivot\n  p = softmax(s);\n  career ~ categorical(p);\n}\n\"\n\n# create data list for Stan\ndat_list &lt;- \n  list(N = nrow(simCareer$data), \n       K = length(unique(simCareer$data$career)), \n       career = simCareer$data$career, \n       career_income = simCareer$income)\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit11_13 &lt;- xfun::cache_rds({\n  rstan::stan(data = dat_list, model_code = fit11_13_code, \n              iter = 2000, warmup = 1000, chains = 4,\n              cores = detectCores(), seed = 1193)},\n  file = \"ch11_fit11_13\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.33 sec elapsed\n\n\nand we look at the summary\n\nas_draws_df(fit11_13) |&gt;\n  summarize_draws() |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = function(x) round(x, digits = 2)))\n\n# A tibble: 4 × 10\n  variable    mean  median    sd   mad      q5     q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;      &lt;num&gt;   &lt;num&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 a[1]       -2.23   -2.22  0.19  0.19   -2.55   -1.94  1.01     629.     547.\n2 a[2]       -1.67   -1.63  0.25  0.24   -2.13   -1.33  1        568.     437.\n3 b           0.13    0.1   0.11  0.1     0.01    0.35  1        539      548.\n4 lp__     -377.   -376.    1.31  1.07 -379.   -375.    1       1156.    1339.\n\n\nand check the summary using rethinking::precis. The result from mcElreath are significantly different than what Kurtz (and the above) give.\nNote: although Kurtz results seem to work, they have a high Rhat, just like McElreath and warnings about divergent points after warmup are issued. The effective sizes for Kurtz is much lower than the ones from McElreath.\n\nBe aware that the estimates you get from these models are extraordinarily difficult to interpret. Since the parameters are relative to the pivot outcome value, they could end up positive or negative, depending upon the context. McElreath (2020) p. 361.\n\n\n\n11.3.1.4 Null Model (Intercept-only)\nAs usual we start we the model with only the intercept.\nIn the case of multinomial, since every category is a model in itself, we use an intercept per category.\nThe 3rd category is the pivot and identified as such in the brm() function below. The default of brm() is to take the first category as the pivot.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"100 secs.\"))\nfit11_13null &lt;- xfun::cache_rds({\n  out &lt;- brm(data = simCareer$data,\n      family = categorical(link = logit, refcat = 3),\n      career ~ 1,\n      prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),\n                prior(normal(0, 1), class = Intercept, dpar = mu2)),\n      iter = 2000, warmup = 1000, chains = 4,\n      cores = detectCores(), seed = 1193)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_13null\")\ntictoc::toc()\n\nrun time of 100 secs., use the cache.: 0.3 sec elapsed\n\n\n\nposterior_summary(fit11_13null) |&gt;\n  round(digits = 2)\n\n                Estimate Est.Error    Q2.5   Q97.5\nb_mu1_Intercept    -2.10      0.16   -2.43   -1.80\nb_mu2_Intercept    -1.43      0.12   -1.66   -1.19\nlprior             -5.08      0.39   -5.91   -4.37\nlp__             -375.18      1.01 -377.91 -374.19\n\n\nIt is important to understand the role of the pivot category. It is simple, the pivot category is used to center the categorical scores.\nFor example the scores we used so far, when centered with his category, are as follows\n\ntibble::tibble(\n  incomes = simCareer$income,\n  scores = simCareer$score,\n  rescaled_scores = simCareer$score - simCareer$score[3]\n)\n\n# A tibble: 3 × 3\n  incomes scores rescaled_scores\n    &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;\n1       1    0.5            -2  \n2       2    1              -1.5\n3       5    2.5             0  \n\n\nAnd we observe that \\(mu1_Intercept\\) and \\(mu2_Intercept\\) in the summary just above are the same as what we just computed which is the intercepts we obtain with the null model. This is an easy check on the null model.\nNow lets see what the fitted values for the \\(\\mu_{cat}\\) are. These fitted values correspond to the softmax which is the link function.\n\nepred11_13null &lt;- simCareer$data |&gt;\n  distinct(career) |&gt;\n  add_epred_draws(fit11_13null) |&gt;\n  ungroup() |&gt;\n  select(.category, .epred) |&gt;\n  group_by(.category) |&gt;\n  mean_qi() |&gt;\n  mutate(across(.cols = where(is.numeric), .fns= ~round(.x, digits = 2)))\nepred11_13null\n\n# A tibble: 3 × 7\n  .category .epred .lower .upper .width .point .interval\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 1           0.09   0.07   0.12   0.95 mean   qi       \n2 2           0.18   0.14   0.21   0.95 mean   qi       \n3 3           0.73   0.69   0.77   0.95 mean   qi       \n\n\n2 observations\n\nthe mean are about equal to the original softamx values which is expected since we are using the intercept-only model.\n\nand we can see that that the multinomial probability is actually very close to the theoretical softmax\n\ntibble::tibble(\n  income = simCareer$income,\n  score = simCareer$score,\n  prob = exp(simCareer$score) / sum(exp(simCareer$score))) |&gt;\n  round(digits = 2)\n\n# A tibble: 3 × 3\n  income score  prob\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1   0.5  0.1 \n2      2   1    0.16\n3      5   2.5  0.74\n\n\n\nThis is an important test to make sure we get our model right before going any further.\n\n\n\n11.3.1.5 Full model\n\n*With brms non-linear syntax we can fit the model with one \\(\\beta\\) parameter or allow it to vary. The lb argument is used to set the lower bound.\n\nWe will create 4 models with varying specs as follows\n\ncrossing(b  = factor(c(\"b1 & b2\", \"b\"), levels = c(\"b1 & b2\", \"b\")),\n         lb = factor(c(\"NA\", 0), levels = c(\"NA\", 0))) |&gt;\n  mutate(fit = paste0(\"b11.13\", letters[1:n()])) |&gt;\n  relocate(fit)\n\n# A tibble: 4 × 3\n  fit     b       lb   \n  &lt;chr&gt;   &lt;fct&gt;   &lt;fct&gt;\n1 b11.13a b1 & b2 NA   \n2 b11.13b b1 & b2 0    \n3 b11.13c b       NA   \n4 b11.13d b       0    \n\n\nand so the model fits using different priors\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"100 secs.\"))\nfit11_13a &lt;- xfun::cache_rds({\n  out &lt;- brm(data = simCareer$data,\n      family = categorical(link = logit, refcat = 3),\n      bf(career ~ 1,\n         nlf(mu1 ~ a1 + b1 * 1),\n         nlf(mu2 ~ a2 + b2 * 2),\n         a1 + a2 + b1 + b2 ~ 1),\n      prior = c(prior(normal(0, 1), class = b, nlpar = a1),\n                prior(normal(0, 1), class = b, nlpar = a2),\n                prior(normal(0, 0.5), class = b, nlpar = b1),\n                prior(normal(0, 0.5), class = b, nlpar = b2)),\n      iter = 2000, warmup = 1000, chains = 4,\n      cores = detectCores(), seed = 1193)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_13a\")\ntictoc::toc()\n\nrun time of 100 secs., use the cache.: 0.33 sec elapsed\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"130 secs.\"))\nfit11_13b &lt;- xfun::cache_rds({\n  out &lt;- brm(data = simCareer$data,\n      family = categorical(link = logit, refcat = 3),\n      bf(career ~ 1,\n         nlf(mu1 ~ a1 + b1 * 1),\n         nlf(mu2 ~ a2 + b2 * 2),\n         a1 + a2 + b1 + b2 ~ 1),\n      prior = c(prior(normal(0, 1), class = b, nlpar = a1),\n                prior(normal(0, 1), class = b, nlpar = a2),\n                prior(normal(0, 0.5), class = b, nlpar = b1, lb = 0),\n                prior(normal(0, 0.5), class = b, nlpar = b2, lb = 0)),\n      iter = 2000, warmup = 1000, chains = 4,\n      cores = detectCores(), seed = 1193,\n      control = list(adapt_delta = .99))\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_13b\")\ntictoc::toc()\n\nrun time of 130 secs., use the cache.: 0.31 sec elapsed\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"120 secs.\"))\nfit11_13c &lt;- xfun::cache_rds({\n  out &lt;- brm(data = simCareer$data,\n      family = categorical(link = logit, refcat = 3),\n      bf(career ~ 1,\n         nlf(mu1 ~ a1 + b * 1),\n         nlf(mu2 ~ a2 + b * 2),\n         a1 + a2 + b ~ 1),\n      prior = c(prior(normal(0, 1), class = b, nlpar = a1),\n                prior(normal(0, 1), class = b, nlpar = a2),\n                prior(normal(0, 0.5), class = b, nlpar = b, lb = 0)),\n      iter = 2000, warmup = 1000, chains = 4,\n      cores = detectCores(), seed = 1193,\n      control = list(adapt_delta = .99))\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_13c\")\ntictoc::toc()\n\nrun time of 120 secs., use the cache.: 1.14 sec elapsed\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"120 secs.\"))\nfit11_13d &lt;- xfun::cache_rds({\n  out &lt;- brm(data = simCareer$data,\n      family = categorical(link = logit, refcat = 3),\n      bf(career ~ 1,\n         nlf(mu1 ~ a1 + b * 1),\n         nlf(mu2 ~ a2 + b * 2),\n         a1 + a2 + b ~ 1),\n      prior = c(prior(normal(0, 1), class = b, nlpar = a1),\n                prior(normal(0, 1), class = b, nlpar = a2),\n                prior(normal(0, 0.5), class = b, nlpar = b, lb = 0)),\n      iter = 2000, warmup = 1000, chains = 4,\n      cores = detectCores(), seed = 1193,\n      control = list(adapt_delta = .99))\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch11_fit11_13d\")\ntictoc::toc()\n\nrun time of 120 secs., use the cache.: 0.37 sec elapsed\n\n\nand plot the results\n\nsumm11_13 &lt;- list()\nsumm11_13 &lt;- within(summ11_13, {\n  # get the models\n  nms &lt;- paste0(\"fit11_13\", letters[1:4])\n  models &lt;- lapply(X = nms, FUN = \\(x)(get(x)))\n  names(models) &lt;- nms\n  \n  # get the coefficient directly from the summary\n  data &lt;- purrr::map_dfr(.x = models, .f = function(m) {\n    fixef(m) |&gt;\n      as.data.frame() |&gt;\n      tibble::rownames_to_column(var = \"term\") |&gt;\n      mutate(term = sub(pattern = \"_Intercept\", replacement = \"\", x = term))\n  }, .id = \"model\")\n})\n# lpred11_13$models\n# summ11_13$data\n\n\nggplot(summ11_13$data, aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, \n                                  y = model, color = term)) +\n  geom_vline(xintercept = 0, size = 0.5, color = \"brown\", linetype = \"dotted\") +\n  geom_pointinterval(size = 3, fatten_point = 3) +\n  ggrepel::geom_text_repel(\n    mapping = aes(x = Estimate, y = model, label = round(Estimate, 2)),\n    size = 3) +\n  scale_color_paletteer_d(\"khroma::bright\") +\n  theme(legend.position = \"none\") +\n  labs(title = \"The parameters' value by model\", x = NULL, y = NULL) +\n  facet_wrap(.~ term, nrow = 1)\n\n\n\n\nand comparing the performance of the models\n\nloo_compare(fit11_13null, fit11_13a, fit11_13b, fit11_13c, fit11_13d, \n            criterion = \"loo\") |&gt;\n  print(simplify = FALSE)\n\n             elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic \nfit11_13c       0.0       0.0  -371.0     16.6         1.8    0.1    742.0\nfit11_13d       0.0       0.0  -371.0     16.6         1.8    0.1    742.0\nfit11_13a      -0.1       0.2  -371.0     16.8         1.9    0.1    742.1\nfit11_13b      -0.1       0.0  -371.1     16.6         1.9    0.1    742.1\nfit11_13null   -0.1       0.0  -371.1     16.7         1.9    0.1    742.2\n             se_looic\nfit11_13c      33.3  \nfit11_13d      33.3  \nfit11_13a      33.6  \nfit11_13b      33.3  \nfit11_13null   33.3  \n\n\nThe results are similar to what Kurtz found, this is caused by the facts that the models have very similar performance and therefore it doesn’t take much to change the ranking. Most numbers, e.g. looic are similar.\nand the model weights\n\nmodel_weights(fit11_13null, fit11_13a, fit11_13b, fit11_13c, fit11_13d, \n              weights = \"loo\") |&gt;\n  round(digits = 2)\n\nfit11_13null    fit11_13a    fit11_13b    fit11_13c    fit11_13d \n        0.19         0.20         0.19         0.21         0.21 \n\n\n\n\n\n11.3.2 Predictors matched to observations\n\n# generate probabilities from family income\ngenProbs &lt;- function(x, coef = c(-2, 0, 2), career = 1:3, income_coef = 0.5) {\n  stopifnot(x &gt;= 0, x &lt;= 1)\n  \n  sapply(x, FUN = function(x) {\n    score &lt;- income_coef * career + coef * x\n    probs &lt;- rethinking::softmax(score)\n    sample(career, size = 1, prob = probs)\n    })\n}\n\n\nsim &lt;- list()\nsim &lt;- within(sim, {\n  set.seed(1193)\n  data &lt;- specify(\n    family_income = ~ runif(n = 1, min = 0, max = 1),\n    career = ~ genProbs(family_income)) |&gt;\n    generate(100) |&gt;\n    unnest(sim) |&gt;\n    mutate(fcareer = paste(\"career\", career))\n})\ndataCareer &lt;- sim$data\n\nand we plot the distribution of the family income which is used as a predictor for each category\n\np &lt;- list()\np$dens &lt;- ggplot(dataCareer, aes(x = family_income, color = fcareer)) +\n  geom_density(size = 1.5) +\n  scale_color_paletteer_d(\"khroma::vibrant\") +\n  theme(legend.position = c(0.8, 0.8),\n        legend.title = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.y = element_blank()) +\n  labs(title = \"observed densities of familiy income by career\",\n       subtitle = sprintf(\"sample size = %d\", nrow(sim$data)),\n       x = \"family income\", y = NULL)\n  \np$dens\n\n\n\n\nNow lets fit the model with brms\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"90 secs.\"))\nfit11_14 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataCareer, \n             family = categorical(link = logit, refcat = 3),\n             bf(career ~ 1,\n                nlf(mu1 ~ a1 + b1 * family_income),\n                nlf(mu2 ~ a2 + b2 * family_income),\n                a1 + a2 + b1 + b2 ~ 1),\n             prior = c(prior(normal(0, 1.5), class = b, nlpar = a1),\n                 prior(normal(0, 1.5), class = b, nlpar = a2),\n                 prior(normal(0, 1), class = b, nlpar = b1),\n                 prior(normal(0, 1), class = b, nlpar = b2)),\n             iter = 2000, warmup = 1000, chains = 4,\n             cores = detectCores(), seed = 1193)\n  out &lt;- brms::add_criterion(out, criterion = \"loo\")},\n  file = \"ch11_fit11_14\")\ntictoc::toc()\n\nrun time of 90 secs., use the cache.: 0.37 sec elapsed\n\n\n\nsummary(fit11_14)\n\n Family: categorical \n  Links: mu1 = logit; mu2 = logit \nFormula: career ~ 1 \n         mu1 ~ a1 + b1 * family_income\n         mu2 ~ a2 + b2 * family_income\n         a1 ~ 1\n         a2 ~ 1\n         b1 ~ 1\n         b2 ~ 1\n   Data: dataCareer (Number of observations: 500) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na1_Intercept    -1.29      0.26    -1.80    -0.79 1.00     2230     2277\na2_Intercept    -1.01      0.21    -1.41    -0.59 1.00     2315     2392\nb1_Intercept    -2.49      0.57    -3.66    -1.43 1.00     2168     2231\nb2_Intercept    -1.22      0.40    -2.00    -0.43 1.00     2228     2562\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand lets see PSIS\n\nloo(fit11_14)\n\n\nComputed from 4000 by 500 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -330.3 16.9\np_loo         3.2  0.3\nlooic       660.6 33.9\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\n\npost11_14 &lt;- data.frame(\n  family_income = seq_range(dataCareer$family_income, n = 50, pretty = TRUE)) |&gt;\n  add_epred_draws(fit11_14) |&gt;\n  mean_qi() |&gt;\n  identity()\n# post11_14\n\n\nplot11_14 &lt;- list()\nplot11_14 &lt;- within(plot11_14, {\n  df &lt;- dataCareer |&gt;\n    mutate(family_income = plyr::round_any(family_income, 0.25)) |&gt;\n    count(career, family_income) |&gt;\n    mutate(probs = n / sum(n))\n  \n  p &lt;- ggplot(post11_14, aes(x = family_income, y = .epred, ymin = .lower, ymax = .upper, \n                    color = .category, fill = .category)) +\n    geom_point(df, mapping = aes(x = family_income, y = probs, color = as.factor(career), fill = as.factor(career)),\n               inherit.aes = FALSE) +\n    geom_smooth(stat = \"identity\") +\n    scale_color_paletteer_d(\"khroma::vibrant\") +\n    scale_fill_paletteer_d(\"khroma::vibrant\") +\n    theme(legend.position = \"bottom\") +\n    labs(title = \"probabilities of career relative to family income\",\n       y = \"probabilities\", color = \"career\", fill = \"career\")\n})\nplot11_14$p\n\n\n\n\n\n\n11.3.3 Multinomial in disguise as Poisson\n\ndata(UCBadmit)\ndataAdmit &lt;- UCBadmit |&gt;\n  rename(rejct = reject)  # reject is a reserved word in brms\nrm(UCBadmit)\ndataAdmit |&gt;\n  skim() |&gt;\n  select(-n_missing, -complete_rate) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 1))\n\n\nData summary\n\n\nName\ndataAdmit\n\n\nNumber of rows\n12\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\nskim_variable\nordered\nn_unique\ntop_counts\n\n\n\n\ndept\nFALSE\n6\nA: 2, B: 2, C: 2, D: 2\n\n\napplicant.gender\nFALSE\n2\nfem: 6, mal: 6\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nadmit\n146.2\n148.4\n17\n45.8\n107.0\n154.0\n512\n▇▅▁▁▁\n\n\nrejct\n230.9\n122.8\n8\n188.2\n261.5\n314.0\n391\n▃▂▃▇▆\n\n\napplications\n377.2\n216.9\n25\n291.5\n374.0\n452.8\n825\n▃▆▇▃▂\n\n\n\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit11_15binom &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataAdmit, \n             family = binomial,\n             formula = admit | trials(applications) ~ 1,\n             prior = c(prior(normal(0, 1.5), class = Intercept)),\n             iter = 2000, warmup = 1000, chains = 4,\n             cores = detectCores(), seed = 1193)\n  out &lt;- brms::add_criterion(out, criterion = \"loo\")},\n  file = \"ch11_fit11_15binom\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.13 sec elapsed\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit11_15pois &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataAdmit, \n             family = poisson,\n             formula = mvbind(admit, rejct) ~ 1,\n             prior = c(prior(normal(0, 1.5), class = Intercept)),\n             iter = 2000, warmup = 1000, chains = 4,\n             cores = detectCores(), seed = 1193)\n  out &lt;- brms::add_criterion(out, criterion = \"loo\")},\n  file = \"ch11_fit11_15pois\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.14 sec elapsed\n\n\nand the posterior distribution for the Poisson fit\n\ngather_rvars(fit11_15pois, b_admit_Intercept, b_rejct_Intercept)\n\n# A tibble: 2 × 2\n  .variable               .value\n  &lt;chr&gt;               &lt;rvar[1d]&gt;\n1 b_admit_Intercept  5.0 ± 0.025\n2 b_rejct_Intercept  5.4 ± 0.019\n\n\n\ngather_rvars(fit11_15pois, b_admit_Intercept, b_rejct_Intercept) |&gt;\n  mutate(.value = exp(.value),\n         .variable = sub(pattern = \"_Intercept\", replacement = \"\", x = .variable),\n         .variable = sub(pattern = \"b_\", replacement = \"\", x = .variable)) |&gt;\n  ggplot(aes(xdist = .value, y = .variable, fill = .variable)) +\n  stat_halfeye(point_interval = median_qi, .width = 0.95) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Distribution of rate of admission and reject across departments\",\n       x = \"nb of applicaitons\", y = NULL)"
  },
  {
    "objectID": "ch11_counting.html#summary",
    "href": "ch11_counting.html#summary",
    "title": "11  Counting and Classification",
    "section": "11.4 Summary",
    "text": "11.4 Summary\n\n\n\n\nKurz, Solomon. 2020. Statistical Rethinking with Brms. 2nd ed. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://www.taylorandfrancis.com."
  },
  {
    "objectID": "ch12_mixed.html#over-dispersed-outcomes",
    "href": "ch12_mixed.html#over-dispersed-outcomes",
    "title": "12  Monsters and Mixtures",
    "section": "12.1 Over-dispersed outcomes",
    "text": "12.1 Over-dispersed outcomes\n\n12.1.1 Beta-binomial\n\n12.1.1.1 Beta-binomial distribution\nThe beta distribution is\n\\[\n\\mathcal{Beta}(x|\\alpha, \\beta) =\n\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha -1} (1-x)^{\\beta -1} =\n\\frac{1}{B(\\alpha, \\beta)} x^{\\alpha -1} (1-x)^{\\beta -1}, 0 \\leq x \\leq 1\n\\]\nwhich is not the format used by McElreath. He uses the following shape parameters which are easier (personal opinion) to understand as \\(\\mu\\) is the average of the distribution and \\(\\kappa\\) is the spread.\n\\[\n\\begin{align*}\n\\mu &= \\bar{p} = \\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\kappa &= \\theta = \\alpha + \\beta\n\\end{align*}\n\\]\nThe beta-binomial distribution in brms is defined with brms::dbeta_binomial. This distribution uses the parameters \\(\\mu\\) and \\(\\phi\\). Now this is confusing because \\(\\mu\\) and \\(\\phi\\) are actually respectively the beta distribution’s \\(\\alpha\\) and \\(\\beta\\) mentioned above.\nSo from now on we use the following parameters with the beta-binomial distribution from brms\n\\[\n\\begin{align*}\n\\mu \\:\\text{parameter in brms:} \\:\\: \\alpha &= \\mu \\cdot \\kappa \\\\\n\\phi \\: \\text{phi parameter in brms:} \\:\\:\\beta &= (1-\\alpha) \\cdot \\kappa\n\\end{align*}\n\\]\nthe simstudy package provide the function to perform that conversion from \\(mean = \\mu\\) and \\(precision = \\kappa\\) to the shape (mathematical) parameters \\(\\alpha\\) and \\(\\beta\\)\n\nparamsMeanKappa &lt;- list(mean = 0.5, kappa = 5)\nparamsShape &lt;- with(paramsMeanKappa, simstudy::betaGetShapes(mean, kappa))\nstopifnot(paramsShape$shape1 == paramsMeanKappa$mean * paramsMeanKappa$kappa,\n          paramsShape$shape2 == (1 - paramsMeanKappa$mean) * paramsMeanKappa$kappa)\n\nVariations of the beta distribution using different parameter values can be illustrated as follows\n\nplotBeta &lt;- list()\nplotBeta &lt;- within(plotBeta, {\n  df &lt;- crossing(pbar = c(0.25, 0.5, 0.75), theta = c(5, 15, 30)) %&gt;% \n  expand(nesting(pbar, theta), \n         x = seq(from = 0, to = 1, length.out = 100)) %&gt;%\n  mutate(shape1 = simstudy::betaGetShapes(pbar, theta)$shape1,\n         shape2 = simstudy::betaGetShapes(pbar, theta)$shape2) %&gt;%\n  mutate(density = dbeta(x, shape1, shape2),\n         mu = paste(\"mu\", pbar, sep = \"==\"),\n         kappa = paste(\"kappa\", theta, sep = \"==\"))\n  \n  p &lt;- ggplot(data = df, aes(x = x, y = density)) +\n    geom_area(fill = \"darkorchid1\") + \n    scale_y_continuous(NULL, labels = NULL) +\n    theme(axis.ticks.y = element_blank()) +\n    facet_grid(kappa~mu, labeller = label_parsed) +\n    labs(title = \"Beta can take many shapes\", x = \"parameter space\")\n})\nplotBeta$p\n\n\n\n\n\n\n12.1.1.2 Beta-binomial model\nThe data used is\n\ndata(UCBadmit)\ndataAdmit &lt;- UCBadmit %&gt;%\n  mutate(gid = ifelse(applicant.gender == \"male\", \"1\", \"2\"))\nrm(UCBadmit)\n# glimpse(dataAdmit)\n\nThere is an error in the model defined by McElrath, to concur with his code at 11.26, the model is\n\\[\n\\begin{align*}\nadmit_i &\\sim \\mathcal{BetaBinomial}(N_i, \\bar{p}_i, \\phi) \\\\\nlogit(\\bar{p}_i) &= \\alpha_{gid[i]} \\\\\n\\alpha &\\sim \\mathcal{N}(0, 1.5) \\\\\n\\phi &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\nWe can fit the model in 2 ways with brms: With the beta_binomilal family or with a custom family called beta_binomial2() as explained by burkner. The family beta_binomial and beta_binomila2 give the same results! So we use brms::beta_binomilal\n\nbrms::brmsfamily(\"beta_binomial\")\n\n\nFamily: beta_binomial \nLink function: logit \n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit12_01 &lt;- xfun::cache_rds({\n  brm(\n    data = dataAdmit,\n    family = beta_binomial,\n    admit | trials(applications) ~ 0 + gid,\n    prior = c(prior(normal(0, 1.5), class = b),\n              prior(exponential(1), class = phi, lb = 2)),\n    iter = 1000, warmup = 500, chains = 2,\n    cores = detectCores(), seed = 1201)},\n  file = \"ch12_fit12_01\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.13 sec elapsed\n\n\n\nfit12_01\n\n Family: beta_binomial \n  Links: mu = logit; phi = identity \nFormula: admit | trials(applications) ~ 0 + gid \n   Data: dataAdmit (Number of observations: 12) \n  Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 1000\n\nPopulation-Level Effects: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ngid1    -0.46      0.44    -1.30     0.40 1.00      907      520\ngid2    -0.33      0.42    -1.17     0.47 1.00      802      640\n\nFamily Specific Parameters: \n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nphi     3.00      0.76     2.05     4.93 1.00      406      219\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\nNote\n\n\n\nDid you notice lb = 2 in prior(exponential(1), ...)? Since McElreath wanted the lower bound to 2, we will use lb = 2.\n\n\nSee also McElreath explanation of 2 in section 12.1.1 just before R code 12.1 on p. 371.\nand the posterior data which represents the distribution rather than the data\n\ngather_rvars(fit12_01, b_gid1, b_gid2, phi) |&gt;\n  ggplot(aes(xdist = .value, y = .variable, fill = .variable)) +\n  stat_dots(color = \"white\", quantiles = 20) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Posterior distibution of model 12.1\",\n       x = NULL, y = NULL)\n\n\n\n\nTo do figure 12.1 a) which represents the posterior distribution of the rate of admission of female applicant, that is the posterior beta distribution\n\nplot12_01_post &lt;- list()\nplot12_01_post &lt;- within(plot12_01_post, {\n  post_df &lt;- spread_draws(fit12_01, b_gid1, b_gid2, phi) |&gt;\n    mutate(p1 = inv_logit_scaled(b_gid1),\n           p2 = inv_logit_scaled(b_gid2),\n           ndraws = 100)\n  set.seed(1201)\n  sample_df &lt;- post_df |&gt;\n    slice_sample(n = 20) |&gt;\n    select(.draw, p1, p2, phi)\n  \n  # x values used to create the data.frame\n  the_x = seq(from = 0, to = 1, by = 0.01)\n\n  beta_df &lt;- purrr::map2_dfr(.x = sample_df$p2, .y = sample_df$phi, .f = function(mu, kappa) {\n    shapes = simstudy::betaGetShapes(mean = mu, precision = kappa)\n    shape1 = shapes$shape1\n    shape1 = shapes$shape1\n    data.frame(x = the_x,\n               y = dbeta(x = the_x, shape1 = shapes$shape1, shape2 = shapes$shape2),\n               p2 = mu,\n               phi = kappa)\n    }, .id = \"id\")\n  # beta_df\n  \n  mean_shapes &lt;- simstudy::betaGetShapes(\n    mean = mean(beta_df$p2), \n    precision = mean(beta_df$phi))\n  beta_mean_df &lt;- data.frame(\n    x = the_x) |&gt;\n    mutate(\n      y = dbeta(x, shape1 = mean_shapes$shape1, shape2 = mean_shapes$shape2))\n  \n  p &lt;- ggplot(data = beta_df, aes(x = x, y = y, group = id)) +\n    geom_line(color = \"yellow\") +\n    geom_line(data = beta_mean_df, aes(x = x, y = y), inherit.aes = FALSE,\n              color = \"green\", linewidth = 2) +\n    coord_cartesian(ylim = c(0, 3)) +\n    labs(title = \"Distribution of female admission rates\",\n         x = \"probability admit\", y = \"density\")\n  \n})\nplot12_01_post$p\n\n\n\n\nand for the posterior validity check\n\nplot12_01_epred &lt;- list()\nplot12_01_epred &lt;- within(plot12_01_epred, {\n  epred &lt;- dataAdmit |&gt;\n    add_epred_draws(fit12_01, ndraws = 100) |&gt;\n    mean_qi(.width = 0.89) |&gt;\n    mutate(p = admit / applications,\n           p_epred = .epred / applications,\n           p_lower = .lower / applications,\n           p_upper = .upper / applications)\n  \n  p &lt;- ggplot(epred, aes(x = .row, y = p)) +\n    geom_point(color = \"yellow\", size = 3) +\n    geom_pointinterval(aes(x = .row, y = p_epred, ymin = p_lower, ymax = p_upper),\n                       inherit.aes = FALSE, shape = 1, fatten_point = 7, size = 1, color = \"green\") +\n    scale_x_continuous(breaks = scales::breaks_width(width = 1)) +\n    scale_y_continuous(breaks = scales::breaks_extended(n = 7),\n                       labels = scales::label_percent()) +\n    labs(title = \"Posterior validity check\",\n         subtitle = \"with 89% CI\",\n         x = \"case\", y = \"admission rate\")\n})\n# plot12_01_epred$epred\nplot12_01_epred$p\n\n\n\n\n\n\n\n12.1.2 Negative-binomial or gamma-Poisson\n\n\n\n\n\n\nImportant\n\n\n\nYou absolutely need to look at the Poisson-lognormal mixture in Kurtz’s blog Kurtz lognormal.\n\n\n\n12.1.2.1 Gamma-Poisson distribution shape\nIn terms of the shape \\(\\alpha\\) and rate \\(\\beta\\) the gamma distribution is\n\\[\n\\mathcal{Gamma}(y \\mid\\alpha, \\beta) = \\frac{\\beta^\\alpha y^{\\alpha-1} e^{-\\beta y}}{\\Gamma(\\alpha)}\n\\]\nbut the rate \\(\\beta\\) and scale \\(\\theta\\) are the reciprocal of each other. Therefore the gamma distribution can be expressed in terms of shape \\(\\alpha\\) and scale \\(\\theta\\) as\n\\[\n\\mathcal{Gamma}(y \\mid\\alpha, \\theta) = \\frac{y^{\\alpha-1} e^{-\\frac{y}{\\theta}}}{\\theta^\\alpha\\Gamma(\\alpha)}\n\\]\nand, also, the gamma distribution can be expressed in terms of mean \\(\\mu\\) and shape \\(\\alpha\\)\n\\[\n\\mathcal{Gamma}(y \\mid \\mu, \\alpha) =\n\\frac{(\\frac{\\alpha}{\\mu})^\\alpha}{\\Gamma(\\alpha)}\ny^{\\alpha-1} \\exp{(-\\frac{\\alpha y}{\\mu})}\n\\]\nTo convert from the \\(\\mu = mean\\) and \\(\\theta = dispersion= \\frac{mean^2}{variance}\\) to the shape and rate parameters we use the function simstudy::gammaGetShapeRate(). To help us find the mean and dispersion to use with simstudy::gammaGetShapeRate(), the custom function gammaGetMeanDispersion is also defined. It is the inverse of simstudy::gammaGetShapeRate().\n\n# custom function which is the inverse function of gammaGetShapeRate()\ngammaGetMeanDispersion &lt;- function(shape, rate) {\n  stopifnot(shape &gt; 0, rate &gt; 0)\n  dispersion &lt;- 1 / shape\n  mean &lt;- shape / rate\n  list(\"mean\" = mean, \"dispersion\" = dispersion)\n}\n\n# test it\nprm &lt;- list()\nprm &lt;- within(prm, {\n  values &lt;- list(mean = 1, dispersion = 10)\n  # get the shape and rate from the mean and dispersion\n  sr &lt;- simstudy::gammaGetShapeRate(mean = values$mean, dispersion = values$dispersion)\n  # using the inverse should take you back to the mean and dispersion\n  md &lt;- gammaGetMeanDispersion(shape = sr$shape, rate = sr$shape)\n})\n# using the inverse should take you back to the mean and dispersion\nstopifnot(identical(prm$md, prm$values))\n\nIn the dgamma the shape parameter influence the rate which is equivalent to Poisson \\(\\lambda\\).\n\nplotGamma &lt;- list()\nplotGamma &lt;- within(plotGamma, {\n  df &lt;- crossing(shape = c(0.5, 1, 2), \n                 rate = c(0.25, 0.5, 1)) |&gt;\n    expand(nesting(shape, rate), \n           x = seq(from = 0, to = 5, length.out = 50)) |&gt;\n    mutate(density = dgamma(x, shape, rate),\n           shape_lbl    = paste(\"shape\", format(shape, nsmall = 2), sep = \"==\"),\n           rate_lbl = paste(\"rate\", format(rate, nsmall = 2), sep = \"==\"))\n  \n  p &lt;- df |&gt;\n    ggplot(aes(x = x, y = density)) +\n    geom_area(fill = \"orchid\") +\n    scale_y_continuous(NULL, labels = NULL) +\n    theme(axis.ticks.y = element_blank()) +\n    facet_grid(shape_lbl~rate_lbl, labeller = label_parsed) +\n    labs(title = \"Gamma prior with different parameter values\",\n       x = \"domain space\")\n})\nplotGamma$p\n\nWarning: Removed 3 rows containing non-finite values (`stat_align()`).\n\n\n\n\n\n\n\n12.1.2.2 Data\n\ndata(Kline)\ndataKline &lt;- Kline |&gt;\n  mutate(log_pop_s = log(population),\n         log_pop_s = as.vector(scale(log_pop_s)),\n         cid = factor(contact, levels = c(\"low\", \"high\")))\nrm(Kline)\ndataKline |&gt; skim() |&gt;\n  select(-n_missing, - complete_rate) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 1))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = where(is.numeric), .fns = round, digits = 1)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\nData summary\n\n\nName\ndataKline\n\n\nNumber of rows\n10\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\nskim_variable\nordered\nn_unique\ntop_counts\n\n\n\n\nculture\nFALSE\n10\nChu: 1, Haw: 1, Lau: 1, Mal: 1\n\n\ncontact\nFALSE\n2\nhig: 5, low: 5\n\n\ncid\nFALSE\n2\nlow: 5, hig: 5\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npopulation\n34109.1\n84793.0\n1100.0\n3897.8\n7700.0\n12050.0\n275000.0\n▇▁▁▁▁\n\n\ntotal_tools\n34.8\n17.8\n13.0\n22.5\n30.5\n42.2\n71.0\n▇▃▃▂▂\n\n\nmean_TU\n4.8\n1.1\n3.2\n4.0\n4.8\n5.3\n6.6\n▅▅▇▂▅\n\n\nlog_pop_s\n0.0\n1.0\n-1.3\n-0.5\n0.0\n0.3\n2.3\n▃▇▃▁▂\n\n\n\n\n\n\n\nNull model\n\n\n\n\n\n\nNote\n\n\n\nThis section is important as it serves to evaluate the prior to use for the full model. See how Kurz (2020) does it. My work below does not show everything (yet).\n\n\nStart with the null model, or as Kurtz calls it, the intercept-only model.\n\\[\n\\begin{align*}\ntotal\\_tools_i &\\sim \\mathcal{GammaPoisson}(\\mu, \\alpha) \\\\\nlog(\\mu) &= \\beta_0 \\\\\n\\beta_0 &\\sim \\mathcal{Normal}(3, 0.5) \\\\\n\\alpha &\\sim \\mathcal{Gamma}(0.01,0.01)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit12_02a &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataKline,\n      family = negbinomial,\n      total_tools ~ 1,\n      prior = c(prior(normal(3, 0.5), class = Intercept),  # beta_0\n                prior(gamma(0.01, 0.01), class = shape)),  # alpha\n      iter = 1000, warmup = 500, chains = 2,\n      cores = detectCores(), seed = 1213)\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  out},\n  file = \"ch12_fit12_02a\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.14 sec elapsed\n\n\n\nfit12_02a\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: total_tools ~ 1 \n   Data: dataKline (Number of observations: 10) \n  Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 1000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     3.50      0.17     3.14     3.84 1.00      548      510\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     4.89      2.86     1.31    11.92 1.00      450      526\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand the estimated parameters of the \\(mean\\) and \\(dispersion\\) can be converted to the \\(shape\\) and \\(rate\\) parameters using \\(simstudy::gammaGetShapeRate()\\)\n\nm &lt;- posterior_summary(fit12_02a)[\"b_Intercept\", \"Estimate\"]\nd &lt;- posterior_summary(fit12_02a)[\"shape\", \"Estimate\"]\nsimstudy::gammaGetShapeRate(mean = m, dispersion = d)\n\n$shape\n[1] 0.2046598\n\n$rate\n[1] 0.05849846\n\n\nBecause the model has only the intercept and no predictor, there is only one value for the Intercept which is the mean of the 10 Poisson rates \\(\\lambda_i, i =1,...10\\).\nThe \\(alpha\\) is simply the \\(shape\\) parameter of gamma … and does not really describe anything. It is really used to define the shape of the distribution.\nAnd the prediction plots show that the distributions using the same rate and shape for the gamma hyperparameters.\n\nplot12_02a_pred &lt;- list()\nplot12_02a_pred &lt;- within(plot12_02a_pred, {\n  df &lt;- dataKline |&gt;\n    add_predicted_draws(fit12_02a, ndraws = 100)\n  \n  p &lt;- df |&gt;\n    ggplot(aes(x = .prediction, color = culture)) +\n    geom_density(size = 1) +\n    scale_y_continuous(NULL, labels = NULL) +\n    scale_color_paletteer_d(\"khroma::soil\") +\n    theme(axis.text.x = element_text(size = rel(0.8)),\n          axis.ticks.y = element_blank(),\n          legend.position = \"none\") +\n    facet_wrap(. ~ culture, nrow = 4) +\n    labs(title = \"Predictive distributions\",\n         subtitle = sprintf(\"Gamma hyperparameters: mean = %0.2f and dispersion = %0.2f\",\n                            posterior_summary(fit12_02a)[\"b_Intercept\", \"Estimate\"],\n                            posterior_summary(fit12_02a)[\"shape\", \"Estimate\"]),\n        x = \"total tools\")\n})\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplot12_02a_pred$p\n\n\n\n\nand we can also visualize the distributions of our \\(rate\\) and \\(shape\\) parameters\n\nplot12_02a_post &lt;- list()\nplot12_02a_post &lt;- within(plot12_02a_post, {\n  df &lt;- gather_draws(fit12_02a, b_Intercept, shape, ndraws = 100) |&gt;\n    mutate(.variable = if_else(.variable == \"b_Intercept\", \"mean\", \"dispersion\"))\n  p &lt;- df |&gt;\n    ggplot(aes(.value, fill = .variable, color = .variable)) +\n    stat_density(geom = \"area\") +\n    scale_y_continuous(NULL, labels = NULL) +\n    scale_fill_paletteer_d(\"fishualize::Scarus_quoyi\") +\n    scale_color_paletteer_d(\"fishualize::Scarus_quoyi\") +\n    theme(axis.text.x = element_text(size = rel(0.8)),\n        axis.ticks.y = element_blank(),\n        legend.position = \"none\") +\n    labs(title = \"Posterior distributions of rate and shape\",\n       x = NULL) +\n    facet_wrap(. ~ .variable, scales = \"free_y\")\n})\nplot12_02a_post$p\n\n\n\n\n\n\nFull model\n\\[\n\\begin{align*}\ntotal\\_tools_i &\\sim \\mathcal{GammaPoisson}(\\mu_i, \\alpha) \\\\\nlog(\\mu) &= \\frac{\\exp{(\\beta_{0,cid[i]})} \\cdot population_i^{\\beta_{1,cid[i]}}}{\\gamma} \\\\\n\\beta_{0,j} &\\sim \\mathcal{Normal}(1, 1) \\\\\n\\beta_{1,j} &\\sim \\mathcal{Exponential}(1) \\\\\n\\gamma &\\sim \\mathcal{Exponential}(1) \\\\\n\\alpha &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"30 secs.\"))\nfit12_02b &lt;- xfun::cache_rds({\n  # we have to be careful when using waic with gamma-Poisson\n  # but in this case we do it. We use t in the plot.\n  out &lt;- brm(data = dataKline,\n      family = negbinomial(link = \"identity\"),\n      bf(total_tools ~ exp(b0) * population^b1 / g,\n         b0 + b1 ~ 0 + cid,\n         g ~ 1,\n         nl = TRUE),\n      prior = c(prior(normal(1, 1), nlpar = b0),\n                prior(exponential(1), nlpar = b1, lb = 0),\n                prior(exponential(1), nlpar = g, lb = 0),\n                prior(exponential(1), class = shape)),\n      iter = 1000, warmup = 500, chains = 2,\n      cores = detectCores(), seed = 1213,\n      control = list(adapt_delta = .95))\n  out &lt;- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  out},\n  file = \"ch12_fit12_02b\")\ntictoc::toc()\n\nrun time of 30 secs., use the cache.: 0.15 sec elapsed\n\n\n\nfit12_02b\n\n Family: negbinomial \n  Links: mu = identity; shape = identity \nFormula: total_tools ~ exp(b0) * population^b1/g \n         b0 ~ 0 + cid\n         b1 ~ 0 + cid\n         g ~ 1\n   Data: dataKline (Number of observations: 10) \n  Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 1000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nb0_cidlow       0.90      0.85    -0.81     2.57 1.01      470      382\nb0_cidhigh      0.99      0.95    -0.77     2.74 1.00      682      588\nb1_cidlow       0.25      0.10     0.07     0.44 1.00      379      215\nb1_cidhigh      0.27      0.13     0.04     0.52 1.00      539      365\ng_Intercept     1.09      0.90     0.12     3.58 1.00      498      553\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     3.68      1.68     1.27     7.73 1.00      706      634\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nadd the pareto k for use in the plot later\n\n# append k value to data\ndataKline &lt;- dataKline |&gt;\n  mutate(ParetoK = fit12_02b$criteria$loo$diagnostics$pareto_k)\nstopifnot(!any(is.na(dataKline)))\n\ndataKline |&gt;\n  select(culture, ParetoK) |&gt;\n  arrange(desc(ParetoK))\n\n      culture    ParetoK\n1       Tonga 0.55416839\n2      Hawaii 0.47255206\n3     Tikopia 0.36713198\n4    Malekula 0.35588800\n5       Chuuk 0.29056316\n6    Lau Fiji 0.28858412\n7         Yap 0.14289746\n8       Manus 0.11435387\n9   Trobriand 0.08526092\n10 Santa Cruz 0.07786319\n\n\nand the fitted values are\n\nepred12_02b &lt;- list()\nepred12_02b &lt;- within(epred12_02b, {\n  df &lt;- dataKline |&gt;\n  distinct(cid, culture) |&gt;\n  expand(nesting(cid, culture), \n         population = seq_range(dataKline$population, n = 20, pretty = TRUE)) |&gt;\n    add_epred_draws(fit12_02b, ndraws = 100) |&gt;\n    ggdist::mean_qi(.width = 0.89)\n  \n  p &lt;- ggplot(dataKline,\n                aes(x = population, y = total_tools, color = cid, size = ParetoK)) +\n  geom_smooth(df,\n              mapping = aes(x = population, y = .epred, ymin = .lower,\n                            ymax = .upper, fill = cid, color = cid),\n              inherit.aes = FALSE, stat = \"identity\") +\n  geom_point(show.legend = FALSE) +\n  ggrepel::geom_text_repel(aes(label = culture), size = 3) +\n  scale_x_continuous(breaks = scales::breaks_extended(n = 5),\n                     labels = scales::label_number(scale = 0.001)) +\n  scale_color_paletteer_d(\"khroma::light\") +\n  scale_fill_paletteer_d(\"khroma::light\") +\n  scale_size_continuous() +\n  theme(legend.position = c(0.2, 0.85)) +\n  labs(title = \"Fitted values with the gamma-Poisson model\",\n       subtitle = \"model 12.2b\",\n       x = \"population in thousands\")\n  \n  \n})\n# epred12_02b$df\nepred12_02b$p\n\n\n\n\nthe main difference now is that since we use predictor \\(cid\\) then the parameter \\(rate = b_0\\) of the gamma distribution used to determined the \\(\\lambda_i\\) is allowed to vary by \\(cid\\). Therefore we have different possible distribution by \\(cid\\) and can change the distribution by culture as follows.\n\npred12_02b &lt;- list()\npred12_02b &lt;- within(pred12_02b, {\n  \n  df &lt;- dataKline |&gt;\n    distinct(cid, culture) |&gt;\n    expand(nesting(cid, culture), \n          population = seq_range(dataKline$population, n = 20, pretty = TRUE)) |&gt;\n    add_predicted_draws(fit12_02b, ndraws = 100)\n  \n  p &lt;- df |&gt;\n    ggplot(aes(x = .prediction, color = cid, fill = cid)) +\n    geom_density() +\n    scale_color_paletteer_d(\"khroma::light\") +\n    scale_fill_paletteer_d(\"khroma::light\") +\n    coord_cartesian(xlim = c(0, 200)) +\n    theme(axis.text.x = element_text(size = 8),\n          axis.text.y = element_blank(),\n          axis.ticks.y = element_blank()) +\n    labs(title = \"Predictive distributions by culture colored by cid\",\n          subtitle = \"model 12.2b\", x = \"total tools\", y = NULL) +\n  facet_wrap(. ~ culture)\n})\npred12_02b$p\n\n\n\n\n\n\n\nPoisson-lognormal\n\n\n\n\n\n\nTip\n\n\n\nThis is an extra section. The result is so useful it is worth adding here. See Kurtz lognormal."
  },
  {
    "objectID": "ch12_mixed.html#zero-inflated-outcomes",
    "href": "ch12_mixed.html#zero-inflated-outcomes",
    "title": "12  Monsters and Mixtures",
    "section": "12.2 Zero-inflated outcomes",
    "text": "12.2 Zero-inflated outcomes\n\n\n\n\n\n\nTip\n\n\n\nMake sure you read this section in Kurz (2020). It is loaded with very useful informations. Especially when using brms.\n\n\n\n12.2.1 Zero-inflated Poisson\nThis type of model is called a hurdle model in the literature. This type of model has served me very well in the context of business.\nWith zero-inflated Poisson both parameters \\(p\\) and \\(\\lambda\\) can have their own equation.\n\\[\n\\begin{align*}\nprod_i &\\sim \\mathcal{ZIPoisson}(p_i, \\lambda_i) \\\\\nlogit(p_i) &= \\alpha_p + \\beta_p x_i \\\\\nlog(\\lambda_i) &= \\alpha_\\lambda + \\beta_\\lambda x_i \\\\\n\\end{align*}\n\\]\nWe use simstudy to simulate this.\n\nsimMonastery &lt;- list()\nsimMonastery &lt;- within(simMonastery, {\n  data &lt;- simpr::specify(\n    drink = ~ rbinom(n = 1, size = 1, prob = 0.2),\n    work = ~ rpois(n = 1, lambda = 1),\n    output = ~ (1 - drink) * work) |&gt;\n    generate(365) |&gt;\n    unnest(sim) |&gt;\n    mutate(fdrink = if_else(drink != 0, \"drink\", \"drinkNot\"),\n           fdrink = as.factor(fdrink))\n})\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:scales':\n\n    discard\n\n\nThe following object is masked from 'package:rethinking':\n\n    map\n\n# simMonastery$data |&gt;\n#   glimpse()\n\nplot the data\n\nsimMonastery$data |&gt;\n  ggplot(aes(x = output)) +\n  geom_histogram(aes(fill = fdrink), binwidth = 1) +\n  scale_fill_paletteer_d(\"khroma::vibrant\") +\n  stat_bin(aes(y = after_stat(count), label = ifelse(after_stat(count), after_stat(count), \"\")), \n           geom = \"text\", bins = 30, color = \"ghostwhite\", vjust = -0.5) +\n  theme(legend.position = c(0.8, 0.8),\n        legend.title = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.y = element_blank()) +\n  labs(title = \"Frequency of monastery's output\",\n       subtitle = sprintf(\"output for %d days\", nrow(simMonastery$data)),\n       y = \"nb of days\")\n\n\n\n\n\n12.2.1.1 Model and fit\n\\[\n\\begin{align*}\nprod_i &\\sim \\mathcal{ZIPoisson}(p, \\lambda) \\\\\nlogit(p) &= \\alpha_p \\\\\nlog(\\lambda) &= \\alpha_\\lambda \\\\\n\\alpha_p &\\sim \\mathcal{Beta}(2, 6) \\\\\n\\alpha_\\lambda &\\sim \\mathcal{N}(1, 0.5)\n\\end{align*}\n\\]\nIn brms, \\(p_i\\) is denoted zi. To use a non-default prior for zi, make sure to indicate class = zi. Important to read Kurz (2020).\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"65 secs.\"))\nfit12_03 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = simMonastery$data,\n      family = zero_inflated_poisson,\n      output ~ 1,\n      prior = c(prior(normal(1, 0.5), class = Intercept),\n                prior(beta(2, 6), class = zi)),  # the brms default is beta(1, 1)\n      iter = 1000, warmup = 500, chains = 2,\n      cores = detectCores(), seed = 1217)\n  brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch12_fit12_03\")\ntictoc::toc()\n\nrun time of 65 secs., use the cache.: 0.17 sec elapsed\n\n\nand we generate a summary with posterior::summarize_draws\n\nsumm_fit12_03 &lt;- list()\nsumm_fit12_03 &lt;- within(summ_fit12_03, {\n  data &lt;- as_draws(fit12_03) |&gt;\n    mutate_variables(lambda = exp(b_Intercept))\n  \n  stats &lt;- data |&gt;\n    summarize_draws() |&gt;\n    filter(variable != \"lp__\") |&gt;\n    mutate(across(.cols = where(is.numeric), round, digits = 2))\n})\nsumm_fit12_03$stats\n\n# A tibble: 4 × 10\n  variable     mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 b_Intercept -0.16  -0.16  0.09  0.09 -0.31 -0.01  1.01     301.     345.\n2 zi           0.18   0.18  0.06  0.06  0.08  0.27  1.01     360.     486.\n3 lprior      -1.96  -1.9   0.43  0.38 -2.81 -1.34  1.01     313.     450.\n4 lambda       0.86   0.86  0.08  0.07  0.74  0.99  1.01     301.     345.\n\n\nThe \\(b_Intercept\\) represents \\(\\lambda\\) on the log scale, because the link function for \\(\\lambda\\). This can be confirmed by looking at the summary which shows Links: mu = log; zi = identity.\nWe observe that \\(lambda\\) matches the actual rate of our simulation with defData(defs, varname = \"work\", dist = \"poisson\", formula = 1).\nWhen using brms the parameter \\(zi\\) has link function identity as evidenced in the summary by Links: mu = log; zi = identity. In this case we have obtained \\(zi = 0.20\\) which is close enough to McEleath’s estimate of 0.23.\nWe observe that \\(zi\\) is the actual rate of our simulation with defData(varname = \"drink\", dist = \"categorical\", formula = \"0.8;0.2\")."
  },
  {
    "objectID": "ch12_mixed.html#ordered-categorical-outcomes",
    "href": "ch12_mixed.html#ordered-categorical-outcomes",
    "title": "12  Monsters and Mixtures",
    "section": "12.3 Ordered categorical outcomes",
    "text": "12.3 Ordered categorical outcomes\n\n12.3.1 Example: Moral intuition\n\ndata(Trolley)\ndataTrolley &lt;- Trolley |&gt;\n  mutate(response = factor(response, ordered = TRUE))\nrm(Trolley)\ndataTrolley |&gt;\n  skim() |&gt;\n  select(-n_missing, -complete_rate) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n\nData summary\n\n\nName\ndataTrolley\n\n\nNumber of rows\n9930\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\nskim_variable\nordered\nn_unique\ntop_counts\n\n\n\n\ncase\nFALSE\n30\ncfa: 331, cfb: 331, cfr: 331, cib: 331\n\n\nresponse\nTRUE\n7\n4: 2323, 5: 1462, 7: 1446, 6: 1445\n\n\nid\nFALSE\n331\n96;: 30, 96;: 30, 96;: 30, 96;: 30\n\n\nedu\nFALSE\n8\nBac: 3540, Som: 2460, Mas: 1410, Gra: 1050\n\n\nstory\nFALSE\n12\nbox: 1324, bur: 1324, spe: 993, swi: 993\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\norder\n16.50\n9.29\n1\n9\n16.5\n24\n32\n▇▆▇▆▇\n\n\nage\n37.49\n14.23\n10\n26\n36.0\n48\n72\n▅▇▇▅▂\n\n\nmale\n0.57\n0.49\n0\n0\n1.0\n1\n1\n▆▁▁▁▇\n\n\naction\n0.43\n0.50\n0\n0\n0.0\n1\n1\n▇▁▁▁▆\n\n\nintention\n0.47\n0.50\n0\n0\n0.0\n1\n1\n▇▁▁▁▇\n\n\ncontact\n0.20\n0.40\n0\n0\n0.0\n0\n1\n▇▁▁▁▂\n\n\naction2\n0.63\n0.48\n0\n0\n1.0\n1\n1\n▅▁▁▁▇\n\n\n\n\n\n\n\n12.3.2 Describing and ordered distribution with intercepts\nThe histogram of response\n\nplotTrolley &lt;- list()\nplotTrolley &lt;- within(plotTrolley, {\n  df &lt;- dataTrolley |&gt;\n    count(response)\n  freq &lt;- df |&gt;\n    ggplot(aes(x = response, y = n, fill = response)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_paletteer_d(\"khroma::bright\") +\n    theme(legend.position = \"none\") +\n    labs(title = \"Histogram of Trolley responses\")\n})\n# plotTrolley$freq\n\nThe cumulative proportions plot\n\nplotTrolley &lt;- within(plotTrolley, {\n  df2 &lt;- dataTrolley |&gt;\n    count(response) |&gt;\n    arrange(response) |&gt;\n    mutate(pct = n / sum(n),\n          cum_pct = cumsum(pct))\n  cumfreq &lt;- ggplot(df2, aes(x = as.integer(response), y = cum_pct)) +\n    geom_line(color = \"yellow\", size = 1) +\n    geom_point(color = \"orange\", size = 2) +\n    coord_cartesian(ylim = c(0, 1)) +\n    labs(title = \"Cumulative proportions\", \n       x = \"response\", y = \"cumulative probabilities\")\n})\n# plotTrolley$cumfreq\n\nAnd the plot of logit\n\nplotTrolley &lt;- within(plotTrolley, {\n  df3 &lt;- dataTrolley |&gt;\n    count(response) |&gt;\n    mutate(pct = n / sum(n),\n         cum_pct = cumsum(pct),\n         logit = log(cum_pct / (1 - cum_pct)),\n         logit_ctr = scale(logit, center = TRUE, scale = FALSE))\n\n  center &lt;- df3 |&gt;\n    ggplot(aes(x = as.integer(response), y = logit)) +\n    geom_line(color = \"pink\", size = 1) +\n    geom_point(color = \"violetred\", size = 2) +\n    labs(title = \"Log of Cumulative Odds\",\n       y = \"log of cumulative odds (centered)\")\n})\n# plotTrolley$center\n\nand the 3 plots in figure 12.4 are\n\nwrap_plots(plotTrolley[c(\"freq\", \"cumfreq\", \"center\")]) +\n  plot_annotation(title = \"Figure 12.4\")\n\n\n\n\nFigure 12.4\n\n\n\n\nThe model is\n\\[\n\\begin{align*}\nresponse_i &\\sim \\mathcal{Categorical}(\\overrightarrow{p}) \\\\\nlogit(p_k) &= \\alpha_k - \\phi \\\\\n\\phi &= 0 \\\\\n\\alpha_k &\\sim \\mathcal{N}(0, 1.5)\n\\end{align*}\n\\]\nand the fit with brms\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"5 mins.\"))\nfit12_04 &lt;- xfun::cache_rds({\n  # define start values\n  inits &lt;- list(\n    `Intercept[1]` = -2,\n    `Intercept[2]` = -1,\n    `Intercept[3]` = 0,\n    `Intercept[4]` = 1,\n    `Intercept[5]` = 2,\n    `Intercept[6]` = 2.5)\n  inits_list &lt;- list(inits, inits, inits, inits)\n  out &lt;- brm(\n    data = dataTrolley,\n    family = cumulative,\n    response ~ 1,\n    prior = c(\n      prior(normal(0, 1.5), class = Intercept)),\n    # the start values\n    init = inits_list,\n    iter = 1000, warmup = 500, chains = 2,\n    cores = detectCores(), seed = 1223)\n  brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch12_fit12_04\")\ntictoc::toc()\n\nrun time of 5 mins., use the cache.: 0.18 sec elapsed\n\n\nwhich gives the summary\n\nprint(fit12_04)\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: response ~ 1 \n   Data: dataTrolley (Number of observations: 9930) \n  Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 1000\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]    -1.92      0.03    -1.98    -1.86 1.00      664      569\nIntercept[2]    -1.27      0.02    -1.31    -1.22 1.00      905      854\nIntercept[3]    -0.72      0.02    -0.76    -0.68 1.00     1033      814\nIntercept[4]     0.25      0.02     0.21     0.29 1.00     1120      682\nIntercept[5]     0.89      0.02     0.85     0.93 1.00     1180      735\nIntercept[6]     1.77      0.03     1.71     1.82 1.00     1239      836\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand we convert the intercepts to the normal scale\n\nfit12_04 |&gt;\n  fixef() |&gt;\n  brms::inv_logit_scaled() |&gt;\n  round(digits = 3)\n\n             Estimate Est.Error  Q2.5 Q97.5\nIntercept[1]    0.128     0.508 0.122 0.135\nIntercept[2]    0.220     0.506 0.212 0.227\nIntercept[3]    0.328     0.505 0.318 0.337\nIntercept[4]    0.562     0.505 0.552 0.571\nIntercept[5]    0.709     0.505 0.700 0.718\nIntercept[6]    0.854     0.507 0.847 0.861\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe SD i.e. Est.Error are not valid using the inv_logit_scaled, that is using a direct inverse exp function.\n\n\nThey must be computed using a posterior sample.\n\nsumm12_04 &lt;- list()\nsumm12_04 &lt;- within(summ12_04, {\n  data &lt;- tidy_draws(fit12_04)\n  summ &lt;- data |&gt;\n    select(!matches(match = \"__$|disc|lprior|chain|draw|iteration\")) |&gt;\n    mutate(across(.cols = where(is.double), .fns = ~gtools::inv.logit(.))) |&gt;\n    pivot_longer(cols = everything()) |&gt;\n    mutate(name = sub(pattern = \"^X[[:digit:]][.]b_\", replacement = \"\", x = name),\n           name = sub(pattern = \"[.]$\", replacement = \"]\", x = name),\n           name = sub(pattern = \"[.]\", replacement = \"[\", x = name)) |&gt;\n    group_by(name) |&gt;\n    ggdist::mean_qi(.width = 0.89) |&gt;\n    mutate(across(.cols = where(is.numeric), .fns = round, digits = 3))\n})\n# glimpse(samples$data)\n# glimpse(samples$summ)\nsumm12_04$summ\n\n# A tibble: 6 × 7\n  name           value .lower .upper .width .point .interval\n  &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 b_Intercept[1] 0.128  0.122  0.134   0.89 mean   qi       \n2 b_Intercept[2] 0.22   0.214  0.226   0.89 mean   qi       \n3 b_Intercept[3] 0.328  0.32   0.335   0.89 mean   qi       \n4 b_Intercept[4] 0.562  0.554  0.569   0.89 mean   qi       \n5 b_Intercept[5] 0.709  0.702  0.716   0.89 mean   qi       \n6 b_Intercept[6] 0.854  0.849  0.86    0.89 mean   qi       \n\n\nand to validate our fit, we see that the \\(value\\) in the summary is the same as the \\(cum_pct\\) previously computed.\n\nplotTrolley$df3$cum_pct |&gt;\n  round(digits = 3)\n\n[1] 0.128 0.220 0.328 0.562 0.709 0.854 1.000\n\n\n\n\n12.3.3 Adding predictor variables\n\n\n\n\n\n\nNote\n\n\n\nThis form automatically ensure the correct ordering of the outcome values, while still morphing the likelihood of each individual value as the predictor \\(x_i\\) changes value. Why is the linear model \\(\\phi\\) substracted from each intercept? Because if we decrease the log-cumulative-odds of every outcome value \\(k\\) below the maximum, this necessarily shifts probability mass upwards towards higher outcome values.\n\n\n\\[\n\\begin{align*}\n\\log{\\left[ \\frac{Pr(y_i \\le k)}{1-Pr(y_i \\le k)} \\right]} &= \\alpha_k - \\phi_i \\\\\n\\phi_i &= \\beta x_i\n\\end{align*}\n\\]\nFor example lets take model b12.4\n\nfixef(fit12_04)\n\n               Estimate  Est.Error       Q2.5      Q97.5\nIntercept[1] -1.9162272 0.03087756 -1.9766096 -1.8578615\nIntercept[2] -1.2669664 0.02330554 -1.3149549 -1.2226463\nIntercept[3] -0.7186470 0.02152095 -0.7619131 -0.6778996\nIntercept[4]  0.2475712 0.01981348  0.2095279  0.2863656\nIntercept[5]  0.8902421 0.02184882  0.8484594  0.9335528\nIntercept[6]  1.7702158 0.02761296  1.7142981  1.8230910\n\n\n\n12.3.3.1 Logistic / Logit functions\nSee the appendix A of this book for a detailed treatment of all these functions. They will be added the suffix .new to identify them.\nThe logistic() and inv_logit() functions are actually the same as stats::plogis().\nAlso, the function logit() already exists as stats::qlogis().\ntherefore dordlogit() as given\n\ndordlogit.new &lt;- function(x, phi = 0L, log = FALSE) {\n  x &lt;- sort(x)  # the ordering is important\n  p &lt;- stats::plogis(q = c(x, Inf), location = phi)\n  p &lt;- c( p[1], p[2:length(p)] - p[1:(length(p)-1)] )\n  if (log) p &lt;- log(p)\n  p\n}\n\nwhich gives about the same result as R code 11.9 in McElreath on p. 386 with R code 12.20, and Kurtz.\n\nprobk &lt;- dordlogit.new(fixef(fit12_04)[, 1])\nround(probk, 2)\n\nIntercept[1] Intercept[2] Intercept[3] Intercept[4] Intercept[5] Intercept[6] \n        0.13         0.09         0.11         0.23         0.15         0.15 \n             \n        0.15 \n\n\nwhich gives and expected value of\n\nsum(1:7 * probk)\n\n[1] 4.199246\n\n\n\n\n12.3.3.2 Subtracting from the log-cumulative odds\nIf we subtract from the log-cumulative odds then we shift the probability mass to higher outcome values.\nFor example with model b12.4\n\nprobk &lt;- dordlogit.new(fixef(fit12_04)[, 1])\nround(probk, 2)\n\nIntercept[1] Intercept[2] Intercept[3] Intercept[4] Intercept[5] Intercept[6] \n        0.13         0.09         0.11         0.23         0.15         0.15 \n             \n        0.15 \n\n\nwhich gives an expected value\n\nsum(1:7 * probk)\n\n[1] 4.199246\n\n\nbut if we substract 0.5\n\n(dordlogit.new(fixef(fit12_04)[, 1], phi = 0.5))\n\nIntercept[1] Intercept[2] Intercept[3] Intercept[4] Intercept[5] Intercept[6] \n  0.08194363   0.06397636   0.08225465   0.20905114   0.15911520   0.18443871 \n             \n  0.21922031 \n\n\nthen we have a higher expected value\n\nsum(dordlogit.new(fixef(fit12_04)[, 1], phi = 0.5) * 1:7)\n\n[1] 4.729615\n\n\n\n\n12.3.3.3 Ordered categorical with several predictors\nOur model with several predictors is\n\\[\n\\begin{align*}\nresponse_i &\\sim Categorical(\\overrightarrow{p}) \\\\\nlogit(Pr(y_i \\leq k)) &= \\frac{Pr(y_i \\leq k)}{1 - Pr(y_i \\leq k)}  = \\alpha_k - \\phi_i \\\\\n\\phi_i &= \\beta_{action} \\cdot action_i + \\beta_{intention} \\cdot intention_i +  \\beta_{contact} \\cdot contact_i + \\beta{a,i} \\cdot(action_i \\times intention_i) +\n\\beta{c,i} \\cdot(contact_i \\times intention_i) \\\\\n\\alpha_k &\\sim \\mathcal{N}(0, 1.5) \\\\\n\\beta_{\\bullet} &\\sim \\mathcal{N}(0, 0.5)\n\\end{align*}\n\\]\nand the fit is\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"6 mins.\"))\nfit12_05 &lt;- xfun::cache_rds({\n  out &lt;- brms::brm(data = dataTrolley,\n                   family = cumulative,\n                   formula = response ~ 1 + action + intention + contact +\n                     action:intention + contact:intention,\n                   prior = c(prior(normal(0, 1.5), class = Intercept),\n                             prior(normal(0, 0.5), class = b)),\n                   iter = 1000, warmup = 500, chains = 2,\n                   cores = detectCores(), seed = 1229)\n  brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch12_fit12_05\")\ntictoc::toc()\n\nrun time of 6 mins., use the cache.: 0.18 sec elapsed\n\n\n\nfit12_05\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: response ~ 1 + action + intention + contact + action:intention + contact:intention \n   Data: dataTrolley (Number of observations: 9930) \n  Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 1000\n\nPopulation-Level Effects: \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]         -2.64      0.05    -2.74    -2.53 1.01      750      594\nIntercept[2]         -1.94      0.05    -2.03    -1.84 1.01      729      681\nIntercept[3]         -1.35      0.05    -1.43    -1.25 1.01      756      836\nIntercept[4]         -0.31      0.04    -0.39    -0.22 1.01      760      853\nIntercept[5]          0.36      0.04     0.28     0.45 1.01      744      772\nIntercept[6]          1.27      0.05     1.18     1.35 1.01      808      799\naction               -0.48      0.05    -0.59    -0.37 1.00      807      704\nintention            -0.29      0.06    -0.41    -0.18 1.00      793      637\ncontact              -0.35      0.07    -0.48    -0.20 1.00      777      748\naction:intention     -0.43      0.08    -0.58    -0.27 1.00      837      702\nintention:contact    -1.23      0.10    -1.42    -1.03 1.00      763      683\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand plot the coefficients\n\npost12_05 &lt;- gather_draws(model = fit12_05, `b_action.*`, `b_contact.*`, `b_intention.*`,\n                           regex = TRUE)\n\npost12_05 |&gt; \n  ggplot(aes(x = .value, y = .variable)) +\n  geom_vline(xintercept = 0, alpha = 1/5, linetype = 3) +\n  stat_gradientinterval(.width = .5, size = 1, point_size = 3/2, shape = 21,\n                      point_fill = \"darkgreen\",\n                      fill =\"green\",\n                      color = \"darkgreen\") +\n  scale_x_continuous(\"marginal posterior\", breaks = -5:0 / 4) +\n  coord_cartesian(xlim = c(-1.4, 0)) +\n  labs(x = \"marginal posterior\", y = NULL,\n       title = \"Model b12.5 coefficients\")\n\nWarning: `fill_type = \"gradient\"` is not supported by the current graphics device, which\nis `\"png\"`.\nℹ Falling back to `fill_type = \"segments\"`.\nℹ If you believe your current graphics device does support `fill_type =\n  \"gradient\"` but auto-detection failed, try setting `fill_type = \"gradient\"`\n  explicitly. If this causes the gradient to display correctly, then this\n  warning is likely a false positive caused by the graphics device failing to\n  properly report its support for the `\"LinearGradient\"` pattern via\n  `grDevices::dev.capabilities()`. Consider reporting a bug to the author of\n  the graphics device.\nℹ See the documentation for `fill_type` in `ggdist::geom_slabinterval()` for\n  more information."
  },
  {
    "objectID": "ch12_mixed.html#ordered-categorical-predictors",
    "href": "ch12_mixed.html#ordered-categorical-predictors",
    "title": "12  Monsters and Mixtures",
    "section": "12.4 Ordered categorical predictors",
    "text": "12.4 Ordered categorical predictors\n\n12.4.1 Dirichlet distribution\nThe Dirichlet distribution, used in this section, can be illustrated as follows\n\nset.seed(1805)  # seed from McElreath\ndp &lt;- gtools::rdirichlet(10, alpha = rep(2, 7))  %&gt;%\n  data.frame() %&gt;%\n  setNames(1:7) %&gt;%\n  mutate(row = seq_len(nrow(.))) %&gt;%\n  pivot_longer(cols = -row, names_to = \"index\", values_to = \"prob\")\n\nggplot(dp, aes(x = index, y = prob, group = row)) +\n  geom_line(aes(color = row == 3)) +\n  geom_point(aes(color = row == 3)) +\n  scale_color_manual(values = c(\"TRUE\" = \"darkgreen\", \"FALSE\" = \"lightgreen\")) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Dirichlet distribution\",\n       subtitle = \"Figure 12.7\",\n       x = \"index of variable in vector\",\n       y = \"probability\")\n\n\n\n\nNOTE: The brms package also has a rdirchlet() function which is very useful when investigating priors. See Kurz (2020) for details.\n\n\n12.4.2 Data\n\ndata(Trolley)\ndataTrolley &lt;- Trolley\nrm(Trolley)\ndataTrolley &lt;- dataTrolley |&gt; \n  mutate(edu_new = \n           recode_factor(edu,\n                  \"Elementary School\" = 1,\n                  \"Middle School\" = 2,\n                  \"Some High School\" = 3,\n                  \"High School Graduate\" = 4,\n                  \"Some College\" = 5, \n                  \"Bachelor's Degree\" = 6,\n                  \"Master's Degree\" = 7,\n                  \"Graduate Degree\" = 8,\n                  .ordered = TRUE) |&gt;\n           as.integer())\n\ndataTrolley |&gt; \n  distinct(edu, edu_new) |&gt; \n  arrange(edu_new)\n\n                   edu edu_new\n1    Elementary School       1\n2        Middle School       2\n3     Some High School       3\n4 High School Graduate       4\n5         Some College       5\n6    Bachelor's Degree       6\n7      Master's Degree       7\n8      Graduate Degree       8\n\n\n\n\n12.4.3 Model and fit\nThe model is\n\\[\n\\begin{align*}\nresponse_i &\\sim \\mathcal{Categorical}(\\overrightarrow{\\textbf{p}}) \\\\\nlogit(p_k) &= \\alpha_k - \\phi_i \\\\\n\\phi_i &= \\beta_E \\sum_{j=0}^{E_i-1} \\delta_j + \\beta_A \\cdot action_i + \\beta_I \\cdot intention_i + \\beta_C \\cdot contact_i \\\\\n\\alpha_k &\\sim \\mathcal{N}(0,1.5) \\\\\n\\beta_A, \\beta_I, \\beta_C &\\sim \\mathcal{N}(0,1) \\\\\n\\beta_E &\\sim \\mathcal{N}(0, 0.143) \\\\\n\\overrightarrow{\\mathbf{\\delta}} &\\sim \\mathcal{Dirichlet}([2,2,2,2,2,2,2])\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"35 mins.\"))\nfit12_06 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataTrolley,\n      family = cumulative,\n      response ~ 1 + action + contact + intention + mo(edu_new),  # note the `mo()` syntax\n      prior = c(prior(normal(0, 1.5), class = Intercept),\n                prior(normal(0, 1), class = b),\n                # note the new kinds of prior statements\n                # for monotonic variable edu_new\n                prior(normal(0, 0.143), class = b, coef = moedu_new),\n                prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu_new1)),\n      iter = 1000, warmup = 500, chains = 2,\n      cores = detectCores(), seed = 1231)\n  brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch12_fit12_06\")\ntictoc::toc()\n\nrun time of 35 mins., use the cache.: 0.22 sec elapsed\n\n\n\nfit12_06\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: response ~ 1 + action + contact + intention + mo(edu_new) \n   Data: dataTrolley (Number of observations: 9930) \n  Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 1000\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]    -3.13      0.17    -3.50    -2.84 1.00      506      401\nIntercept[2]    -2.45      0.17    -2.81    -2.17 1.00      517      507\nIntercept[3]    -1.87      0.16    -2.23    -1.59 1.00      520      468\nIntercept[4]    -0.85      0.16    -1.19    -0.57 1.00      538      556\nIntercept[5]    -0.18      0.16    -0.52     0.09 1.00      529      520\nIntercept[6]     0.73      0.16     0.38     1.00 1.00      534      572\naction          -0.71      0.04    -0.79    -0.63 1.00      979      740\ncontact         -0.96      0.05    -1.05    -0.87 1.00     1058      661\nintention       -0.72      0.04    -0.79    -0.65 1.00     1233      722\nmoedu_new       -0.05      0.03    -0.11    -0.01 1.00      467      645\n\nSimplex Parameters: \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nmoedu_new1[1]     0.26      0.15     0.04     0.60 1.00      755      578\nmoedu_new1[2]     0.14      0.09     0.02     0.35 1.00     1361      705\nmoedu_new1[3]     0.20      0.12     0.03     0.45 1.00     1449      894\nmoedu_new1[4]     0.16      0.09     0.03     0.36 1.00      991      666\nmoedu_new1[5]     0.03      0.04     0.00     0.12 1.00      820      523\nmoedu_new1[6]     0.09      0.06     0.01     0.24 1.01     1189      577\nmoedu_new1[7]     0.12      0.07     0.02     0.28 1.00     1242      633\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\ndelta_labels &lt;- c(\"Elem\", \"MidSch\", \"SHS\", \"HSG\", \"SCol\", \"Bach\", \"Mast\", \"Grad\")\n\ndp &lt;- fit12_06 |&gt;\n  tidy_draws() |&gt;\n  select(contains(\"simo_moedu_new1\")) |&gt;\n  setNames(paste0(delta_labels[2:8], \"~(delta[\", 1:7, \"])\")) |&gt;\n  identity()\n# glimpse(dp)\n\nGGally::ggpairs(dp, labeller = label_parsed) +\n  ggthemes::theme_hc() +\n  theme(strip.text = element_text(size = 8))"
  },
  {
    "objectID": "ch12_mixed.html#summary",
    "href": "ch12_mixed.html#summary",
    "title": "12  Monsters and Mixtures",
    "section": "12.5 Summary",
    "text": "12.5 Summary\n\n\n\n\nKurz, Solomon. 2020. Statistical Rethinking with Brms. 2nd ed. https://bookdown.org/content/4857/."
  },
  {
    "objectID": "ch13_multilevels.html#example-multilevel-tadpoles",
    "href": "ch13_multilevels.html#example-multilevel-tadpoles",
    "title": "13  Multilevel Models",
    "section": "13.1 Example: Multilevel tadpoles",
    "text": "13.1 Example: Multilevel tadpoles\n\ndata(reedfrogs)\ndataFrogs &lt;- reedfrogs |&gt;\n  mutate(tank = seq_len(n()),\n         tank = factor(tank))\nrm(reedfrogs)\ndataFrogs |&gt;\n  skim() |&gt;\n  select(-n_missing, -complete_rate) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = where(is.numeric), .fns = round, digits = 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\nData summary\n\n\nName\ndataFrogs\n\n\nNumber of rows\n48\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\nskim_variable\nordered\nn_unique\ntop_counts\n\n\n\n\npred\nFALSE\n2\nno: 24, pre: 24\n\n\nsize\nFALSE\n2\nbig: 24, sma: 24\n\n\ntank\nFALSE\n48\n1: 1, 2: 1, 3: 1, 4: 1\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndensity\n23.33\n10.38\n10.00\n10.0\n25.00\n35.00\n35\n▇▁▇▁▇\n\n\nsurv\n16.31\n9.88\n4.00\n9.0\n12.50\n23.00\n35\n▇▂▂▂▃\n\n\npropsurv\n0.72\n0.27\n0.11\n0.5\n0.89\n0.92\n1\n▁▂▂▁▇\n\n\n\n\n\nwith the plot of data\n\nplotFrogs &lt;- ggplot(dataFrogs, aes(x = as.integer(tank), y = propsurv)) +\n  geom_point(color = \"sienna\") +\n  geom_hline(yintercept = 0.8, color = \"sienna1\", linetype = \"dashed\") +\n  geom_vline(xintercept = c(16.5, 32.5), size = 1/3, color = \"sienna1\") +\n  scale_x_continuous(breaks = c(1, 16, 32, 48)) +\n  scale_y_continuous(breaks = scales::breaks_width(width = 0.2),\n                     labels = scales::label_percent(accuracy = 1)) +\n  annotate(geom = \"text\",\n           x = c(8, 16 + 6, 32 + 8), y = 0,\n           label = c(\"small tanks\", \"medium tanks\", \"large tanks\"),\n           color = \"midnightblue\") +\n  theme(axis.text.x = element_text(size = rel(1))) +\n  labs(title = \"Tadpole tanks\",\n       subtitle = sprintf(\"%d data points\", nrow(dataFrogs)),\n       x = \"tank\", y = \"proportion survival\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplotFrogs\n\n\n\n\n\n13.1.1 Simple\nand the model, without multilevel effect, is\n\\[\n\\begin{align*}\nsurv_i &\\sim \\mathcal{Binomial}(n_i, p_i) \\\\\nlogit(p_i) &= \\alpha_{tank[i]} \\\\\n\\alpha_{tank} &\\sim \\mathcal{N}(0, 1.5)\n\\end{align*}\n\\]\nThis fit gives the closest waic to the one shown in R code 13.4 on p. 404.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit13_01 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataFrogs,\n    family = binomial,\n    formula = bf(surv | trials(density) ~ 0 + tank),\n    prior = c(\n      prior(normal(0, 1.5), class = b)),\n    iter = 1000, warmup = 500, chains = 2,\n    cores = detectCores(), seed = 1301)\n  add_criterion(out, c(\"loo\", \"waic\"))},\n  file = \"ch13_fit13_01\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.11 sec elapsed\n\n\n\nfit13_01 |&gt;\n  spread_draws(`b_.+`, regex = TRUE) |&gt;\n  summarize_draws(\"mean\", \"sd\", ~quantile(.x, probs = c(0.055, 0.945)),\n                  default_convergence_measures()) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),\n         across(.cols = starts_with(\"ess\"), .fns = as.integer))\n\n# A tibble: 48 × 8\n   variable  mean    sd `5.5%` `94.5%`  rhat ess_bulk ess_tail\n   &lt;chr&gt;    &lt;num&gt; &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n 1 b_tank1   1.73  0.77   0.53    3.09  1        1512      824\n 2 b_tank2   2.38  0.93   1       4.01  1        1403      657\n 3 b_tank3   0.74  0.63  -0.24    1.78  1.01      991      716\n 4 b_tank4   2.44  0.91   1.08    3.98  1         875      683\n 5 b_tank5   1.69  0.79   0.55    2.99  1        1636      731\n 6 b_tank6   1.71  0.77   0.54    2.95  1.01     1484      675\n 7 b_tank7   2.4   0.9    1.08    3.97  1        1230      742\n 8 b_tank8   1.71  0.78   0.56    3.04  1        1254      718\n 9 b_tank9  -0.36  0.63  -1.39    0.61  1        1310      610\n10 b_tank10  1.73  0.76   0.57    2.97  1.01     1222      761\n# ℹ 38 more rows\n\n\nand visualize the intercepts which correspond to the logit of the probabilities.\n\nplot13_01 &lt;- list()\nplot13_01 &lt;- within(plot13_01, {\n  data &lt;- fit13_01 |&gt;\n    summarize_draws() |&gt;\n    filter(variable != \"lp__\") |&gt;\n    select(variable, a = mean) |&gt;\n    mutate(p = inv_logit_scaled(a)) |&gt;\n    rename(exp_surv_log_odds = a, exp_surv_prob = p) |&gt;\n    pivot_longer(cols = c(exp_surv_log_odds, exp_surv_prob))\n\n  p &lt;- ggplot(data, aes(x = value, fill = name, color = name)) +\n    geom_dotplot() +\n    scale_fill_manual(values = c(\"orange1\", \"orange4\")) +\n    scale_color_manual(values = c(\"orange1\", \"orange4\")) +\n    scale_y_continuous(breaks = NULL) +\n    theme(legend.position = \"none\") +\n    facet_wrap(. ~ name, scales = \"free_x\") +\n    labs(title = \"Tank-level intercepts from the no-pooling model\",\n         x = NULL, y = NULL)\n  })\nplot13_01$p\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n13.1.2 Multilevel\nand now the multilevel model\n\\[\n\\begin{align*}\nsurv_i &\\sim \\mathcal{Binomial}(n_i, p_i) \\\\\nlogit(p_i) &= \\alpha_{tank[i]} \\\\\n\\alpha_j &\\sim \\mathcal{N}(\\bar{\\alpha}, \\sigma) \\\\\n\\bar{\\alpha} &\\sim \\mathcal{N}(0, 1.5) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\nand the fit is as follows. Note the prior prior(exponential(0, 1), class = sd) which is parametrized in the standard deviation metric (Kurtz). It is common for multilevel software to model the variance metric. This will be further explained in chapter 14.\nThis fit gives the closest waic to the one shown in R code 13.4 on p. 404.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit13_02 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataFrogs,\n    family = binomial,\n    formula = bf(surv | trials(density) ~ 1 + (1 | tank)),\n    prior = c(\n      prior(normal(0, 1.5), class = Intercept),\n      prior(exponential(1), class = sd)),\n    sample_prior = TRUE,\n    iter = 1000, warmup = 500, chains = 2,\n    cores = detectCores(), seed = 1303)\n  add_criterion(out, c(\"loo\", \"waic\"))},\n  file = \"ch13_fit13_02\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.13 sec elapsed\n\n\n\nfit13_02 |&gt;\n  spread_draws(`b_.+`, `sd_.+`,`r_.+`, regex = TRUE) |&gt;\n  summarize_draws(\"mean\", \"sd\", ~quantile(.x, probs = c(0.055, 0.945)),\n                  default_convergence_measures()) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),\n         across(.cols = starts_with(\"ess\"), .fns = as.integer))\n\n# A tibble: 50 × 8\n   variable             mean    sd `5.5%` `94.5%`  rhat ess_bulk ess_tail\n   &lt;chr&gt;               &lt;num&gt; &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n 1 b_Intercept          1.38  0.27   0.96    1.82  1         179      371\n 2 sd_tank__Intercept   1.64  0.21   1.35    2.03  1         374      540\n 3 r_tank[1,Intercept]  0.76  0.9   -0.59    2.32  1        1211      461\n 4 r_tank[2,Intercept]  1.73  1.15   0.05    3.71  1        1557      550\n 5 r_tank[3,Intercept] -0.39  0.7   -1.46    0.76  1.01     1047      649\n 6 r_tank[4,Intercept]  1.74  1.15   0.12    3.84  1        1675      834\n 7 r_tank[5,Intercept]  0.79  0.9   -0.55    2.33  1        1033      660\n 8 r_tank[6,Intercept]  0.76  0.92  -0.66    2.28  1        1242      624\n 9 r_tank[7,Intercept]  1.74  1.15   0.11    3.79  1         935      663\n10 r_tank[8,Intercept]  0.8   0.92  -0.49    2.26  1        1335      513\n# ℹ 40 more rows\n\n\n\n\n13.1.3 Comparison\nand compare the models\n\nprint(loo_compare(fit13_01, fit13_02, criterion = \"waic\"), simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  \nfit13_02    0.0       0.0   -99.7       3.8         20.6    1.0     199.3\nfit13_01   -8.1       2.2  -107.7       2.2         26.0    1.4     215.5\n         se_waic\nfit13_02    7.5 \nfit13_01    4.4 \n\n\n\nmodel_weights(fit13_01, fit13_02, weights = \"waic\") |&gt;\n  round(digits = 2)\n\nfit13_01 fit13_02 \n       0        1 \n\n\n\n\n13.1.4 Posterior distribution\n\nfit13_02\n\n Family: binomial \n  Links: mu = logit \nFormula: surv | trials(density) ~ 1 + (1 | tank) \n   Data: dataFrogs (Number of observations: 48) \n  Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 1000\n\nGroup-Level Effects: \n~tank (Number of levels: 48) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.64      0.21     1.28     2.13 1.00      375      540\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.38      0.27     0.87     1.94 1.00      179      372\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThis time we don’t have a list of intercepts as in fit13_02. We have a description of their distribution \\(\\alpha_j \\sim \\mathcal{N}(\\bar{\\alpha}, \\sigma)\\) where \\(Intercept = \\bar{\\alpha}\\) and \\(sd(Intercept) = \\sigma\\).\nThe task of getting the posterior is easier with tidybayes and posterior than the way McElreath and Kurtz do it.\n\n# IMPORTANT: We use the median value, not the mean because of skewed\n#            binomial dist. You don't always have to use the mean!\nepred13_02 &lt;- dataFrogs |&gt;\n  add_epred_draws(fit13_02, ndraws = 500) |&gt;\n  mutate(propsurv.epred = .epred / density) |&gt;\n  median_qi(.width = 0.89)\nepred13_02\n\n# A tibble: 48 × 16\n# Groups:   density, pred, size, surv, propsurv, tank [48]\n   density pred  size   surv propsurv tank   .row .epred .epred.lower\n     &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;    &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1      10 no    big       7      0.7 3         3   7.30         4.97\n 2      10 no    big       9      0.9 1         1   8.92         6.96\n 3      10 no    big      10      1   2         2   9.50         8.13\n 4      10 no    big      10      1   4         4   9.53         8.17\n 5      10 no    small     9      0.9 5         5   8.84         7.06\n 6      10 no    small     9      0.9 6         6   8.89         6.82\n 7      10 no    small     9      0.9 8         8   8.97         6.93\n 8      10 no    small    10      1   7         7   9.54         8.29\n 9      10 pred  big       4      0.4 9         9   4.56         2.47\n10      10 pred  big       6      0.6 12       12   6.47         3.86\n# ℹ 38 more rows\n# ℹ 7 more variables: .epred.upper &lt;dbl&gt;, propsurv.epred &lt;dbl&gt;,\n#   propsurv.epred.lower &lt;dbl&gt;, propsurv.epred.upper &lt;dbl&gt;, .width &lt;dbl&gt;,\n#   .point &lt;chr&gt;, .interval &lt;chr&gt;\n\n\n\nplotFrogs +\n  geom_point(epred13_02, mapping = aes(x = as.integer(tank), y = propsurv.epred),\n             inherit.aes = FALSE, shape = 1, size = 2, color = \"darkorange\")\n\n\n\n\nFigure 13.1\n\n\n\n\nFirst, we take a sampling of size 100 of the 4000 draws from the posterior sample with slice_sample(n = 100).\nSecond, to simulate the distribution of the logodds values described by the b_Intercept and tank_Intercepts of each draw we create a sequence of 100 logodds values between -4 and 5 (based on the acutal range of -2, 3.5 shown just above) which is done by expand(nesting(iter, b_Intercept,  sd_tank__Intercept), x = seq(from = -4, to = 5, length.out = 100)) and which will result in 10000 lines.\nThird, we compute the normal density for each of the 10000 lines using the b_Intercept and tank_Intercepts of each line. The normal density is used because the model is \\(\\alpha_{tank} \\sim \\mathcal{N}(0, 5)\\) and \\(\\alpha \\sim \\mathcal{N}(0, 1)\\). This is done with\n\nsamples &lt;- list()\nsamples &lt;- within(samples, {\n  data1 &lt;- as_draws_df(fit13_02) |&gt;\n    slice_sample(n = 100) |&gt;\n    expand(nesting(.draw, b_Intercept, sd_tank__Intercept),\n           x = seq(from = -4, to = 5, length.out = 100)) |&gt;\n    mutate(density = dnorm(x, mean = b_Intercept, sd = sd_tank__Intercept))\n  data2 &lt;- as_draws_df(fit13_02) |&gt;\n    slice_sample(n = 1000, replace = TRUE) |&gt;\n    mutate(p_logit = rnorm(n(), mean = b_Intercept, sd = sd_tank__Intercept),\n           p = gtools::inv.logit(p_logit))\n  \n  p1 &lt;- ggplot(data1, aes(x = x, y = density, group = .draw)) +\n    geom_line(alpha = .2, color = \"sienna2\") +\n    scale_y_continuous(NULL, breaks = NULL) +\n    coord_cartesian(xlim = c(-3, 4)) +\n    labs(title = \"Population survival distribution\",\n         subtitle = \"log-odds scale\", x = NULL, y = NULL)\n  \n  p2 &lt;-  ggplot(data2, aes(x = p)) +\n    geom_density(size = 0, fill = \"sienna2\", color = \"sienna2\", adjust = 0.1) +\n    scale_y_continuous(NULL, breaks = NULL) +\n    labs(title = \"Probability of survival\",\n         subtitle = \"transformed by the inverse-logit function\",\n         x = NULL, y = NULL)\n})\nwrap_plots(samples$p1, samples$p2)\n\n\n\n\nFigure 13.2\n\n\n\n\nTo improve the model you could use Half-Normal (or Half-Cauchy) instead of exponential, and now the multilevel model\n\\[\n\\begin{align*}\nsurv_i &\\sim \\mathcal{Binomial}(n_i, p_i) \\\\\nlogit(p_i) &= \\alpha_{tank[i]} \\\\\n\\alpha_j &\\sim \\mathcal{N}(\\bar{\\alpha}, \\sigma) \\\\\n\\bar{\\alpha} &\\sim \\mathcal{N}(0, 1.5) \\\\\n\\sigma &\\sim \\mathcal{Half-Normal}(0, 1)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch13_multilevels.html#varying-effects",
    "href": "ch13_multilevels.html#varying-effects",
    "title": "13  Multilevel Models",
    "section": "13.2 Varying effects",
    "text": "13.2 Varying effects\n\n13.2.1 The model\n\\[\n\\begin{align*}\nsurv_i &\\sim \\mathcal{Binomial}(n_i, p_i) \\\\\nlogit(p_i) &= \\alpha_{pond[i]} \\\\\n\\alpha_{pond[i]} &\\sim \\mathcal{N}(\\bar{\\alpha}, \\sigma) \\\\\n\\bar{\\alpha} &\\sim \\mathcal{N}(0, 1.5) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\nwhere we have\n\n\\(\\bar{\\alpha}\\): Average log-odd of the survival rate for entire population of ponds\n\\(\\sigma\\): Standard deviation of log-odds of survival among ponds\n\\(\\alpha_{pond}\\): vector of individual pond intercept (mean)\n\n\n\n13.2.2 Assign value to the parameters\n\nsim &lt;- list()\nsim &lt;- within(sim, {\n  a_bar &lt;- 1.5\n  sigma &lt;- 1.5\n  nponds &lt;- 60\n  set.seed(5005)  # same seed as McElreath\n  data &lt;- data.frame(\n    pond = seq_len(nponds),\n    Ni = rep(as.integer(c(5, 10, 25, 35)), each = 15),\n    true_a = rnorm(nponds, mean = a_bar, sd = sigma)) |&gt;\n    mutate(true_p = inv_logit_scaled(true_a))\n})\n# because of stan, Ni must be an integer\nstopifnot(is.integer(sim$data$Ni))\n# glimpse(sim$data)\n\nand we plot the data to see the real distributions\n\nggplot(sim$data, aes(x = true_a, y = as.factor(Ni))) +\n  stat_dotsinterval(.width = 0.5, fill = \"orange\", fatten_point = 3) +\n  labs(title = \"Distribution of log odd of survival by pond\",\n       y = NULL)\n\n\n\n\n\n\n13.2.3 Simulate survivors\nThe model uses\n\\[\n\\begin{align*}\nsurv_i &\\sim \\mathcal{Binomial}(n_i, p_i) \\\\\nlogit(p_i) &= \\alpha_{pond[i]} \\\\\n\\alpha_{pond[i]} &\\sim \\mathcal{N}(\\bar{\\alpha}, \\sigma) \\\\\n\\bar{\\alpha} &\\sim \\mathcal{N}(0, 1.5) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\ntherefore the simulation of \\(p_i\\) must use the logistic function\n\nsim &lt;- within(sim, {\n  set.seed(5005)\n  data &lt;- data |&gt;\n    mutate(Si = rbinom(n(), prob = true_p, size = Ni))\n  # data$Si &lt;- rbinom(nponds, prob = data$true_p, size = data$Ni)\n})\n\n\n\n13.2.4 Compute the no-pooling estimates\n\nsim$data &lt;- sim$data |&gt;\n  mutate(nopool_p = Si / Ni)\n# glimpse(sim$data)\n\n\n\n13.2.5 Compute the partial pooling estimates\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit13_03 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = sim$data,\n    family = binomial,\n    formula = bf(Si | trials(Ni) ~ 1 + (1 | pond)),\n    prior = c(prior(normal(0, 1.5), class = Intercept),\n            prior(exponential(1), class = sd)),\n    iter = 1000, warmup = 500, chains = 2,\n    cores = detectCores(), seed = 1307)\n  out &lt;- add_criterion(out, c(\"loo\", \"waic\"))\n  out},\n  file = \"ch13_fit13_03\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.15 sec elapsed\n\n\nand we have our point summaries and interval as follows\n\nfit13_03 |&gt;\n  spread_draws(`b_.+`, `sd_.+`,`r_.+`, regex = TRUE) |&gt;\n  summarize_draws(\"mean\", \"sd\", ~quantile(.x, probs = c(0.055, 0.945)),\n                  default_convergence_measures()) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),\n         across(.cols = starts_with(\"ess\"), .fns = as.integer))\n\n# A tibble: 62 × 8\n   variable             mean    sd `5.5%` `94.5%`  rhat ess_bulk ess_tail\n   &lt;chr&gt;               &lt;num&gt; &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n 1 b_Intercept          1.47  0.22   1.12    1.83  1.01      264      395\n 2 sd_pond__Intercept   1.49  0.2    1.19    1.84  1.01      440      652\n 3 r_pond[1,Intercept]  0.09  0.96  -1.33    1.71  1        1181      660\n 4 r_pond[2,Intercept]  0.07  0.95  -1.33    1.61  1.01     1668      648\n 5 r_pond[3,Intercept] -0.69  0.86  -2.03    0.7   1        1817      676\n 6 r_pond[4,Intercept]  1.15  1.2   -0.61    3.12  1        1921      647\n 7 r_pond[5,Intercept]  1.14  1.13  -0.52    3.14  1        2019      687\n 8 r_pond[6,Intercept]  0.11  0.98  -1.34    1.79  1        1423      721\n 9 r_pond[7,Intercept]  0.04  0.92  -1.34    1.62  1        1801      797\n10 r_pond[8,Intercept]  0.12  1.04  -1.44    1.81  1        1853      790\n# ℹ 52 more rows\n\n\nIt is important to understand that epred_draws is not simply the result of linpred-draws converted with the inverse link function. It can be illustrated in this case as follows\n\n# the linear/link-level predictor\nlp &lt;- linpred_draws(fit13_03, newdata = sim$data[, c(\"Ni\", \"pond\")]) |&gt;\n  select(-.chain, -.iteration) |&gt;\n  summarize_draws()\n# the\nep &lt;- epred_draws(fit13_03, newdata = sim$data[, c(\"Ni\", \"pond\")]) |&gt;\n  select(-.chain, -.iteration) |&gt;\n  summarize_draws()\n\n# the link-level predictor\nhead(lp[, c(\"pond\", \"Ni\", \"mean\")])\n\n# A tibble: 6 × 3\n# Groups:   Ni, pond [6]\n   pond    Ni  mean\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     5 1.56 \n2     2     5 1.54 \n3     3     5 0.779\n4     4     5 2.61 \n5     5     5 2.61 \n6     6     5 1.58 \n\n# the expected posterior predictive\nhead(ep[, c(\"pond\", \"Ni\", \"mean\")])\n\n# A tibble: 6 × 3\n# Groups:   Ni, pond [6]\n   pond    Ni  mean\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     5  3.95\n2     2     5  3.94\n3     3     5  3.31\n4     4     5  4.47\n5     5     5  4.49\n6     6     5  3.96\n\n# now the inverse of the link-level predictor\n# is not the same as the expected posterior predictive\nhead(data.frame(\n  linpred = lp$mean,\n  linpred_inv = gtools::inv.logit(lp$mean),\n  epred = ep$mean / ep$Ni\n))\n\n    linpred linpred_inv     epred\n1 1.5570842   0.8259346 0.7896738\n2 1.5399453   0.8234568 0.7886283\n3 0.7792846   0.6855259 0.6615856\n4 2.6139794   0.9317559 0.8947860\n5 2.6059193   0.9312416 0.8980664\n6 1.5815556   0.8294247 0.7920721\n\n\nand we get the information on the level 1, the fixed effect, with fixef(b13.3) and the level 2, random effects, with ranef(fit13_03)\n\n# fixef(fit13_03)\n# ranef(fit13_03)\n\nand the whole thing is obtained with the fit which calls the stan data\n\n# fit13_03$fit\n\n\n\n\n\n\n\nNote\n\n\n\nIn this project we favor using tidybayes and posterior. fixef(fit13_03), ranef(fit13_03), fit13_03$fit and coef()\\[, ,\\] are avoided.\n\n\n\nsim &lt;- within(sim, {\n  linpred &lt;- linpred_draws(fit13_03, newdata = sim$data[, c(\"Ni\", \"pond\")]) |&gt;\n    select(-.chain, -.iteration) |&gt;\n    summarize_draws()\n\n  data &lt;- data |&gt;\n    # bind_cols(partpool_p) |&gt;\n    mutate(partpool_p = inv_logit_scaled(linpred$mean)) |&gt;\n    mutate(nopool_error = abs(nopool_p - true_p),\n           partpool_error = abs(partpool_p - true_p))\n  })\n# glimpse(sim$data)\n\n\nggplot(sim$data, aes(x = pond, y = nopool_error)) +\n  geom_point(color = \"sienna\") +\n  geom_point(mapping = aes(y = partpool_error),\n             shape = 1, size = 2, color = \"darkorange\") +\n  geom_vline(xintercept = c(16.5, 32.5), size = 1/3, color = \"sienna1\") +\n  scale_x_continuous(breaks = c(1, 10, 20, 30, 40, 50, 60)) +\n  scale_y_continuous(breaks = scales::breaks_width(width = 0.10),\n                     labels = scales::label_percent(accuracy = 1)) +\n  annotate(geom = \"text\", \n           x = c(15 - 7.5, 30 - 7.5, 45 - 7.5, 60 - 7.5), y = .45, \n           label = c(\"tiny (5)\", \"small (10)\", \"medium (25)\", \"large (35)\")) +\n  theme(axis.text.x = element_text(size = rel(1))) +\n  labs(title = \"Tadpole tanks\",\n       subtitle = \"Same results as Kurtz, difference with McElreath caused by the seed\",\n       x = NULL, y = NULL)\n\n\n\n\nFigure 13.3"
  },
  {
    "objectID": "ch13_multilevels.html#more-than-one-type-of-cluster",
    "href": "ch13_multilevels.html#more-than-one-type-of-cluster",
    "title": "13  Multilevel Models",
    "section": "13.3 More than one type of cluster",
    "text": "13.3 More than one type of cluster\n\n13.3.1 Multilevel chimpanzees\n\n13.3.1.1 The model\n\\[\n\\begin{align*}\npull\\_left_i &\\sim \\mathcal{Binomial}(1, p_i) \\\\\nlogit(p_i) &= \\alpha + \\alpha_{actor[i]} + \\gamma_{block[i]} +\\beta_{treatment[i]} \\\\\n\\beta_j &\\sim \\mathcal{N}(0, 0.5), \\, \\text{for } j = 1 \\ldots 4 \\\\\n\\alpha_j &\\sim \\mathcal{N}(\\bar{\\alpha}, \\sigma_{\\alpha}), \\, \\text{for } j = 1 \\ldots 7 \\\\\n\\gamma_j &\\sim \\mathcal{N}(0, \\sigma_{\\gamma}), \\, \\text{for } j = 1 \\ldots 6 \\\\\n\\bar{\\alpha} &\\sim \\mathcal{N}(0, 1.5) \\\\\n\\sigma_{\\alpha} &\\sim \\mathcal{Exponential}(1) \\\\\n\\sigma_{\\gamma} &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\n\n\n13.3.1.2 The fit\nWe load the data\n\ndata(chimpanzees)\ndataChimp &lt;- chimpanzees |&gt;\n  mutate(actor = factor(actor),\n         block = factor(block),\n         treatment = factor(1 + prosoc_left + 2 * condition,\n                            levels = as.character(1:4),\n                            labels = c(\"R/N\", \"L/N\", \"R/P\", \"L/P\")))\nrm(chimpanzees)\ndataChimp |&gt;\n  skim() |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n\nData summary\n\n\nName\ndataChimp\n\n\nNumber of rows\n504\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nactor\n0\n1\nFALSE\n7\n1: 72, 2: 72, 3: 72, 4: 72\n\n\nblock\n0\n1\nFALSE\n6\n1: 84, 2: 84, 3: 84, 4: 84\n\n\ntreatment\n0\n1\nFALSE\n4\nR/N: 126, L/N: 126, R/P: 126, L/P: 126\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrecipient\n252\n0.5\n5.00\n2.00\n2\n3.00\n5.0\n7.00\n8\n▇▃▃▃▇\n\n\ncondition\n0\n1.0\n0.50\n0.50\n0\n0.00\n0.5\n1.00\n1\n▇▁▁▁▇\n\n\ntrial\n0\n1.0\n36.50\n20.80\n1\n18.75\n36.5\n54.25\n72\n▇▇▇▇▇\n\n\nprosoc_left\n0\n1.0\n0.50\n0.50\n0\n0.00\n0.5\n1.00\n1\n▇▁▁▁▇\n\n\nchose_prosoc\n0\n1.0\n0.57\n0.50\n0\n0.00\n1.0\n1.00\n1\n▆▁▁▁▇\n\n\npulled_left\n0\n1.0\n0.58\n0.49\n0\n0.00\n1.0\n1.00\n1\n▆▁▁▁▇\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis fit does not give the exact result obtained by McElreath. This is caused by the fact that brms non-centered parametrization. See Kurtz in his sections 13.3.1 about that.\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit13_04 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataChimp,\n      family = bernoulli,\n      formula = bf(pulled_left ~ 0 + a + g + b,\n                   a ~ 1 + (1 | actor),\n                   g ~ 0 + block,\n                   b ~ 0 + treatment,\n                   nl = TRUE),\n      prior = c(prior(normal(0, 1.5), class = b, coef = Intercept, nlpar = a),\n                prior(exponential(1), class = sd, group = actor, nlpar = a),\n                prior(normal(0, sigma_g), class = b, nlpar = g),\n                prior(\"target += exponential_lpdf(sigma_g | 1)\", check = FALSE),\n                prior(normal(0, 0.5), class = b, nlpar = b)\n                ),\n      stanvars = c(stanvar(scode = \"  real&lt;lower=0&gt; sigma_g;\", \n                           block = \"parameters\")),\n      iter = 1000, warmup = 500, chains = 2, \n      cores = detectCores(), seed = 1319)\n  add_criterion(out, c(\"loo\", \"waic\"))},\n  file = \"ch13_fit13_04\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.11 sec elapsed\n\n\n\nsummarize_draws(fit13_04, \"mean\", \"sd\", ~quantile(.x, probs = c(0.055, 0.945)),\n                default_convergence_measures()) |&gt;\n  filter(!grepl(\"^lp\", x = variable)) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),\n         across(.cols = starts_with(\"ess\"), .fns = as.integer))\n\n# A tibble: 20 × 8\n   variable                 mean    sd `5.5%` `94.5%`  rhat ess_bulk ess_tail\n   &lt;chr&gt;                   &lt;num&gt; &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n 1 b_a_Intercept            0.53  0.75  -0.65    1.71  1.01      175      319\n 2 b_g_block1              -0.15  0.2   -0.52    0.07  1.02      328      504\n 3 b_g_block2               0.04  0.17  -0.2     0.3   1.02      726      345\n 4 b_g_block3               0.05  0.18  -0.18    0.35  1.03      759      468\n 5 b_g_block4               0.01  0.16  -0.23    0.28  1.01      758      484\n 6 b_g_block5              -0.02  0.16  -0.29    0.22  1.01      688      457\n 7 b_g_block6               0.11  0.19  -0.11    0.46  1.01      511      476\n 8 b_b_treatmentRDN        -0.12  0.3   -0.61    0.39  1.01      500      717\n 9 b_b_treatmentLDN         0.41  0.32  -0.1     0.91  1         725      676\n10 b_b_treatmentRDP        -0.47  0.31  -0.99    0.03  1         515      736\n11 b_b_treatmentLDP         0.28  0.3   -0.21    0.77  1.01      546      743\n12 sd_actor__a_Intercept    2.02  0.62   1.23    3.08  1         343      506\n13 r_actor__a[1,Intercept] -0.92  0.75  -2.09    0.25  1.01      167      411\n14 r_actor__a[2,Intercept]  4.13  1.4    2.31    6.54  1         364      577\n15 r_actor__a[3,Intercept] -1.21  0.74  -2.32   -0.03  1.01      186      340\n16 r_actor__a[4,Intercept] -1.22  0.75  -2.42   -0.07  1.01      200      391\n17 r_actor__a[5,Intercept] -0.9   0.76  -2.07    0.28  1.01      201      261\n18 r_actor__a[6,Intercept]  0.03  0.75  -1.14    1.18  1.01      167      411\n19 r_actor__a[7,Intercept]  1.56  0.8    0.31    2.82  1.01      202      369\n20 sigma_g                  0.19  0.15   0.03    0.51  1.03       86      327\n\n\nthe posterior samples is\n\npost13_04 &lt;- list()\npost13_04 &lt;- within(post13_04, {\n  summ &lt;- summarize_draws(fit13_04, mean, ~quantile(.x, probs = c(0.055, 0.945))) |&gt;\n    filter(!grepl(pattern = \"^lp.+\", x = variable)) |&gt;\n    rename(.lower = `5.5%`,\n           .upper = `94.5%`)\n  sd &lt;- gather_draws(fit13_04, `sd_.*`, regex = TRUE)\n})\n# glimpse(post13_04$summ)\n\nand we look at the standard deviation of the random effect of actor\n\nplot13_04 &lt;- list()\nplot13_04 &lt;- within(plot13_04, {\n  dens &lt;- post13_04$sd |&gt; \n    ggplot(aes(x = .value, color = .variable)) +\n    geom_density(adjust = 0.5, linewidth = 1) +\n    scale_color_paletteer_d(\"ggthemes::Classic_10\") +\n    # tidybayes::stat_halfeye(.width = 0.89, fill = \"orange\") +\n    # tidybayes::stat_halfeye(aes(x = sd_block__a_Intercept), .width = 0.95, fill = \"darkgreen\") +\n    coord_cartesian(xlim = c(0, 4)) +\n    theme(legend.position = c(0.6, 0.8),\n          legend.background = element_rect(fill = \"transparent\"),\n          legend.title = element_blank()) +\n    labs(title = expression(sigma[actor] *\", \"* sigma[block]),\n         x = expression(sigma), y = NULL)\n  \n  coef &lt;- post13_04$summ |&gt;\n    ggplot(aes(x = mean, xmin = .lower, xmax = .upper, y = variable)) + \n    geom_pointinterval(fatten_point = 3,color = \"navyblue\") +\n    ggrepel::geom_text_repel(aes(label = round(mean, 2)), size = 3, color = \"purple\") +\n    geom_vline(xintercept = 0) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Posterior distribution\",\n         subtitle = \"With mean and 89% CI\",\n         x = NULL, y = NULL)\n  \n})\nwrap_plots(plot13_04) +\n  plot_annotation(\n    title = \"Posterior distributions\"\n  )\n\n\n\n\nFigure 13.4\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis fit gives the same waic as found on page 418 of section 13.3.1.\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit13_05 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataChimp,\n      family = bernoulli,\n      formula = bf(pulled_left ~ 0 + a + b,\n                   a ~ 1 + (1 | actor),\n                   b ~ 0 + treatment,\n                   nl = TRUE),\n      prior = c(prior(normal(0, 1.5), class = b, coef = Intercept, nlpar = a),\n                prior(exponential(1), class = sd, group = actor, nlpar = a),\n                prior(normal(0, 0.5), class = b, nlpar = b)\n                ),\n      iter = 1000, warmup = 500, chains = 2, \n      cores = detectCores(), seed = 1319)\n  add_criterion(out, c(\"loo\", \"waic\"))},\n  file = \"ch13_fit13_05\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.17 sec elapsed\n\n\nand we obtain the same results as on p. 418 of section 13.3.1.\n\nprint(loo_compare(fit13_04, fit13_05, criterion = \"waic\"), simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  \nfit13_05    0.0       0.0  -265.9       9.6          8.8    0.5     531.8\nfit13_04   -0.3       0.8  -266.2       9.7         10.6    0.5     532.3\n         se_waic\nfit13_05   19.2 \nfit13_04   19.4 \n\n\n\n\n\n13.3.2 Even more clusters\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nfit13_06 &lt;- xfun::cache_rds({\n  out &lt;- brm(data = dataChimp,\n      family = bernoulli,\n      formula = bf(pulled_left ~ 1 + (1 | actor) + (1 | treatment) + (1 | block)),\n      prior = c(prior(normal(0, 1.5), class = Intercept),\n                prior(exponential(1), class = sd)),\n      iter = 1000, warmup = 500, chains = 2, \n      cores = detectCores(), seed = 1319)\n  add_criterion(out, c(\"loo\", \"waic\"))},\n  file = \"ch13_fit13_06\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.15 sec elapsed\n\n\n\nprint(loo_compare(fit13_04, fit13_05, fit13_06, criterion = \"waic\"), \n      simplify = FALSE)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  \nfit13_05    0.0       0.0  -265.9       9.6          8.8    0.5     531.8\nfit13_04   -0.3       0.8  -266.2       9.7         10.6    0.5     532.3\nfit13_06   -0.5       0.8  -266.4       9.6         10.8    0.5     532.7\n         se_waic\nfit13_05   19.2 \nfit13_04   19.4 \nfit13_06   19.3"
  },
  {
    "objectID": "ch13_multilevels.html#divergent-transitions-and-non-centered-priors",
    "href": "ch13_multilevels.html#divergent-transitions-and-non-centered-priors",
    "title": "13  Multilevel Models",
    "section": "13.4 Divergent transitions and non-centered priors",
    "text": "13.4 Divergent transitions and non-centered priors\n\n\nNot covered"
  },
  {
    "objectID": "ch13_multilevels.html#multilevel-posterior-predictions",
    "href": "ch13_multilevels.html#multilevel-posterior-predictions",
    "title": "13  Multilevel Models",
    "section": "13.5 Multilevel posterior predictions",
    "text": "13.5 Multilevel posterior predictions\nStrongly recommended to read Kurz (2020) who is more elaborate in this section.\n\n13.5.1 Posterior prediction for same clusters\nWe can do a posterior fit for chimp #2. Remember this chimp was an outlier as it was always pulling the left lever no matter what.\n\nfitd &lt;- list()\nfitd &lt;- within(fitd, {\n  chimp &lt;- 2L\n  labels&lt;- c(\"R/N\", \"L/N\", \"R/P\", \"L/P\")\n  newdata &lt;- data.frame(\n    actor = factor(chimp),\n    treatment = unique(dataChimp$treatment),\n    block = 1\n  )\n  data &lt;- epred_draws(fit13_04, newdata = newdata) |&gt;\n    select(-.chain, -.iteration, -.draw) |&gt;\n    summarize_draws()\n  # and the empirical frequencies are\n  obs &lt;- dataChimp |&gt;\n    filter(actor == chimp) |&gt;\n    group_by(treatment) |&gt;\n    summarize(prob = mean(pulled_left))\n  p &lt;- ggplot(obs, aes(x = treatment, y = prob, group = 1)) +\n    geom_lineribbon(data, \n                    mapping = aes(x = treatment, y = mean, ymin = q5, ymax = q95),\n                    fill = \"orange\", color = \"brown\") +\n    geom_point(color = \"navyblue\", size = 2) +\n    coord_cartesian(ylim = c(0.75, 1))+\n    labs(title = sprintf(\"Fitted prediction for chimp # %d\", chimp), \n         x = NULL, y = NULL)\n})\n# glimpse(fitd$data)\nfitd$p\n\n\n\n\nand for chimp # 5\n\nfitd &lt;- list()\nfitd &lt;- within(fitd, {\n  chimp &lt;- 5L\n  labels&lt;- c(\"R/N\", \"L/N\", \"R/P\", \"L/P\")\n  newdata &lt;- data.frame(\n    actor = factor(chimp),\n    treatment = unique(dataChimp$treatment),\n    block = 1\n  )\n  data &lt;- epred_draws(fit13_04, newdata = newdata) |&gt;\n    select(-.chain, -.iteration, -.draw) |&gt;\n    summarize_draws()\n  # and the empirical frequencies are\n  obs &lt;- dataChimp |&gt;\n    filter(actor == chimp) |&gt;\n    group_by(treatment) |&gt;\n    summarize(prob = mean(pulled_left))\n  p &lt;- ggplot(obs, aes(x = treatment, y = prob, group = 1)) +\n    geom_lineribbon(data, \n                    mapping = aes(x = treatment, y = mean, ymin = q5, ymax = q95),\n                    fill = \"orange\", color = \"brown\") +\n    geom_point(color = \"navyblue\", size = 2) +\n    coord_cartesian(ylim = c(0, 1))+\n    labs(title = sprintf(\"Fitted prediction for chimp # %d\", chimp), \n         x = NULL, y = NULL)\n})\n# glimpse(fitd$data)\nfitd$p\n\n\n\n\n\n\n13.5.2 Posterior prediction for new clusters\n\n\n13.5.3 Post-stratification"
  },
  {
    "objectID": "ch13_multilevels.html#summary",
    "href": "ch13_multilevels.html#summary",
    "title": "13  Multilevel Models",
    "section": "13.6 Summary",
    "text": "13.6 Summary\n\n\n\n\nKurz, Solomon. 2020. Statistical Rethinking with Brms. 2nd ed. https://bookdown.org/content/4857/."
  },
  {
    "objectID": "ch14_covariances.html#varying-slopes-by-construction",
    "href": "ch14_covariances.html#varying-slopes-by-construction",
    "title": "14  Adventures in Covariance",
    "section": "14.1 Varying slopes by construction",
    "text": "14.1 Varying slopes by construction\n\n14.1.1 Simulate the population\n\nsimCafes &lt;- list()\nsimCafes &lt;- within(simCafes, {\n  # a := average morning wait time\n  # b := average difference afternoon wait time\n  Mu &lt;- c(\"a\" = 3.5, \"b\" = -1)\n  \n  # a := std dev of intercepts\n  # b := std dev of slopes\n  sigmas &lt;- c(\"a\" = 1, \"b\" = 0.5)\n  \n  # correlation between intercepts and slopes\n  rho &lt;- -0.7\n  \n  cov_ab &lt;- prod(sigmas) * rho\n})\n\nMcElreath mentions a difficulty using the matrix function. He misses the argument byrow which resolve this.\n\n# use byrow = TRUE to solve McElrath's issue\nmatrix(1:4, nrow = 2, ncol = 2, byrow = TRUE)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nwe get the covariance matrix sigma as follows\n\nsimCafes &lt;- within(simCafes, {\n  # matrix of correlation\n  Rho &lt;- matrix(c(1, rho, rho, 1), nrow = 2)\n  \n  # covariance matrix\n  Sigma &lt;- diag(sigmas) %*% Rho %*% diag(sigmas)\n})\n\nand we simulate the bivariate normal distribution\n\nsimCafes &lt;- within(simCafes, {\n  n_cafes &lt;- 20\n\n  set.seed(1409)\n  vary_effects &lt;- MASS::mvrnorm(n = n_cafes, mu = Mu, Sigma = Sigma) |&gt;\n    as.data.frame() |&gt;\n    mutate(cafe = seq_len(n_cafes)) |&gt;\n    relocate(cafe)\n})\n# glimpse(simCafes$vary_effects)\n\nand we plot the simulated data which represents the intercept and slope\n\nsimCafes$vary_effects |&gt;\n  ggplot(aes(x = a, y = b)) +\n  geom_point(shape = 1, size = 3, color = \"purple\") +\n  lapply(X = 1:5 / 5, FUN = function(x) {\n      stat_ellipse(type = \"norm\", level = x, linetype = \"dotted\", size = 0.25)}) +\n  theme(legend.position = \"none\") +\n  labs(title = sprintf(\"Distribution of intercept and slopes for %d cafes\", \n                       simCafes$n_cafes),\n       x = \"intercepts (a_cafe)\", y = \"slope (b_cafe)\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 14.2\n\n\n\n\n\n\n14.1.2 Simulate the observations (visits by cafe)\nNow using the simulated intercepts and slopes, we create the simulated visits to each cafe.\n\nsimCafes &lt;- within(simCafes, {\n  n_visits &lt;- 10  # nb of visits to each cafe by robot\n  sigma &lt;- 0.5  # std dev within cafes\n  \n  set.seed(1409)\n  data &lt;- vary_effects |&gt;\n    expand_grid(simCafes$cafe, \"visit\" = seq_len(n_visits)) |&gt;\n    mutate(afternoon = rep(0:1, times = n()/2)) |&gt;\n    mutate(mu = a + b * afternoon) |&gt;\n    mutate(wait = rnorm(n = n(), mean = mu, sd = sigma))\n})\n# glimpse(simCafes$vary_effects)\n# glimpse(simCafes$data)\n\nand plot the simulated observations.\n\nsimCafes$data |&gt;\n  mutate(afternoon = if_else(afternoon == 0, \"M\", \"A\"),\n         day = rep(rep(1:5, each = 2), times = simCafes$n_cafes),\n         label = paste(\"cafe\", simCafes$data$cafe)) |&gt;\n  filter(cafe %in% c(1, 5)) |&gt;\n  ggplot(aes(x = visit, y = wait, group = day)) +\n  geom_point(aes(color = afternoon), size = 2) +\n  geom_line(color = \"green\") +\n  scale_color_manual(values = c(\"M\" = \"royalblue\", \"A\" = \"hotpink\")) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Varying slopes simulation\") +\n  facet_wrap(~ label, ncol = 1)\n\n\n\n\n\n\n14.1.3 The varying slopes model\n\n14.1.3.1 The model\n\\[\n\\begin{align*}\nwait_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{cafe[i]} + \\beta_{cafe[i]} \\cdot afternoon_i \\\\\n\\begin{bmatrix}\n\\alpha_{cafe} \\\\\n\\beta_{cafe}\n\\end{bmatrix}\n&\\sim\n\\mathcal{MVNormal}(\n\\begin{bmatrix}\n\\alpha \\\\\n\\beta\n\\end{bmatrix}\n,\n\\bf{\\Sigma}\n) \\\\\n\\bf{\\Sigma} &=\n\\begin{bmatrix}\n\\sigma_{\\alpha} & 0 \\\\\n0 & \\sigma_{\\beta}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_{\\alpha} & 0 \\\\\n0 & \\sigma_{\\beta}\n\\end{bmatrix} \\\\\n\\alpha &\\sim \\mathcal{N}(0, 10) \\\\\n\\beta &\\sim \\mathcal{N}(0, 10) \\\\\n\\sigma &\\sim \\mathcal{HalfCauchy}(0, 1) \\\\\n\\sigma_{\\alpha} &\\sim \\mathcal{HalfCauchy}(0, 1) \\\\\n\\sigma_{\\beta} &\\sim \\mathcal{HalfCauchy}(0, 1) \\\\\n\\rho\n&\\sim \\mathcal{LKJcorr}(K=2)\n\\end{align*}\n\\]\n\n\n14.1.3.2 LKJ prior\nWe use the ggdist package to illustrate the LKJ distribution.\n\nlkj_dist &lt;- list()\nlkj_dist &lt;- within(lkj_dist, {\n  df &lt;- crossing(K = 2:4, eta = c(1, 2, 4), x = seq(from = -1, to = 1, by = 0.05)) %&gt;%\n    mutate(dens = purrr::pmap_dbl(.l = ., \n                              .f = \\(K, eta, x) ggdist::dlkjcorr_marginal(x = x, K = K, eta = eta))) |&gt;\n    mutate(\n      id = K^eta,\n      label_K = paste0(\"K==\", K),\n      label_eta = paste0(\"eta==\", eta)) |&gt;\n    identity()\n  \n  p &lt;- df |&gt;\n    ggplot(mapping = aes(x = x, y = dens, color = id)) +\n    geom_line(size = 1) +\n    scale_y_continuous(breaks = c(0, 0.5, 1)) +\n    scale_color_paletteer_c(\"palr::sst_pal\") +\n    theme(legend.position = \"none\") +\n    facet_grid(facets = label_eta ~ label_K, scales= \"fixed\", labeller = label_parsed) +\n    labs(title = \"Lewandowski-Kurowicka-Joe Distribution\",\n         x = \"correlation\", y = \"density\")\n})\n\nWarning: The `facets` argument of `facet_grid()` is deprecated as of ggplot2 2.2.0.\nℹ Please use the `rows` argument instead.\n\n# lkj_dist$df |&gt;\n#   glimpse()\nlkj_dist$p\n\n\n\n\nFigure 14.3\n\n\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit14_01 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = simCafes$data,\n    family = gaussian,\n    formula = wait ~ 1 + afternoon + (1 + afternoon | cafe),\n    prior = c(\n      prior(normal(5, 2), class = Intercept),\n      prior(normal(-1, 0.5), class = b),\n      prior(exponential(1), class = sd),\n      prior(exponential(1), class = sigma),\n      prior(lkj(2), class = cor)),\n    sample_prior = TRUE,\n    iter = 1000, warmup = 500, chains = 2,\n    cores = detectCores(), seed = 1423)\n  add_criterion(out, c(\"loo\", \"waic\"))},\n  file = \"ch14_fit14_01\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.14 sec elapsed\n\n\n\npost14_01 &lt;- list()\npost14_01 &lt;- within(post14_01, {\n  prior &lt;- prior_draws(x = fit14_01)\n  \n  post &lt;- tidy_draws(model = fit14_01)\n  \n  # dataframe of correlations to plot\n  corr = data.frame(\"value\" = c(prior$cor_cafe, post$cor_cafe__Intercept__afternoon),\n                    \"id\" = c(rep(\"prior\", nrow(prior)), rep(\"post\", nrow(post))))\n\n  coefs &lt;- fit14_01 |&gt;\n    spread_draws(b_Intercept, b_afternoon, r_cafe[cafe, term]) |&gt;\n    pivot_wider(id_cols = c(\"b_Intercept\", \"b_afternoon\", \"cafe\"),\n                names_from = \"term\", values_from = \"r_cafe\") |&gt;\n    group_by(cafe) |&gt;\n    summarize(b_Intercept = mean(b_Intercept), b_afternoon = mean(b_afternoon),\n              r_afternoon = mean(afternoon), r_Intercept = mean(Intercept)) |&gt;\n    mutate(Intercept = b_Intercept + r_Intercept,\n           afternoon = b_afternoon + r_afternoon) |&gt;\n    select(cafe, Intercept, afternoon) |&gt;\n    identity()\n  all &lt;- simCafes$vary_effects |&gt;\n    rename(\"Intercept\" = a, \"afternoon\" = b)\n  \n  all &lt;- bind_rows(\"real\" = all, \"post\" = coefs, .id = \"id\")\n})\n# glimpse(post14_01$post)\n# glimpse(post14_01$coefs)\n# glimpse(post14_01$all)\n\n\nplot14_01 &lt;- list()\nplot14_01 &lt;- within(plot14_01, {\n  cor &lt;- post14_01$corr |&gt; \n    ggplot(aes(x = value, color = id, linetype = id)) +\n    geom_density(size = 1, adjust = 0.75) +\n    scale_color_manual(values = c(\"prior\" = \"black\", \"post\" = \"blue\")) +\n    scale_linetype_manual(values = c(\"prior\" = \"longdash\", \"post\" = \"solid\")) +\n    theme(legend.position = c(0.8, 0.8), legend.title = element_blank()) +\n    labs(title = \"Posterior and Prior distribution of the correlation\",\n         x = \"correlation\")\n})\nplot14_01$cor\n\n\n\n\nFigure 14.4\n\n\n\n\n\npred14_01 &lt;- list()\npred14_01 &lt;- within(pred14_01, {\n  pred_df &lt;- simCafes$data |&gt;\n    group_by(cafe, afternoon) |&gt;\n    summarise(mwait = mean(wait)) |&gt;\n    add_predicted_draws(object = fit14_01) |&gt;\n    mean_qi(.width = 0.89) |&gt;\n    mutate(term = if_else(afternoon == 0, \"Intercept\", \"afternoon\"))\n  \n  real &lt;- pred_df |&gt;\n    select(cafe, term, mwait) |&gt;\n    pivot_wider(id_cols = cafe, names_from = term, values_from = mwait)\n  \n  pred &lt;- pred_df |&gt;\n    select(cafe, term, .prediction) |&gt;\n    pivot_wider(id_cols = cafe, names_from = term, values_from = .prediction)\n  \n  all &lt;- bind_rows(\"real\" = real, \"pred\" = pred, .id = \"id\")\n})\n\n`summarise()` has grouped output by 'cafe'. You can override using the\n`.groups` argument.\n\n# glimpse(pred14_01$all)\n# glimpse(simCafes$data)\n\n\nplot14_01 &lt;- within(plot14_01, {\n  coefs &lt;- post14_01$all |&gt;\n    ggplot(mapping = aes(x = Intercept, y = afternoon, group = cafe, color = id)) +\n    lapply(X = 1:5 / 5, FUN = function(x) {\n      stat_ellipse(data = post14_01$all, mapping = aes(x = Intercept, y = afternoon),\n                   inherit.aes = FALSE,\n                   geom = \"polygon\", type = \"norm\", level = x, linewidth = 1/5,\n                 color = \"dodgerblue\", fill = \"transparent\")}) +\n    geom_point() +\n    geom_line(color = \"black\") +\n    scale_color_paletteer_d(\"awtools::spalette\", direction = 1) +\n    theme(legend.position = c(0.2, 0.2),\n          legend.title = element_blank()) +\n    labs(title = \"Coefficients and shrinkage\",\n         x = \"Intercept\", y = \"Slope\")\n    \n  \n  wait &lt;- pred14_01$all |&gt;\n    ggplot(mapping = aes(x = Intercept, y = afternoon, group = cafe, color = id)) +\n    lapply(X = 1:5 / 5, FUN = function(x) {\n      stat_ellipse(data = pred14_01$all, mapping = aes(x = Intercept, y = afternoon),\n                   inherit.aes = FALSE,\n                   geom = \"polygon\", type = \"norm\", level = x, linewidth = 1/5,\n                 color = \"dodgerblue\", fill = \"transparent\")}) +\n    geom_point() +\n    geom_line(color = \"black\") +\n    scale_color_paletteer_d(\"awtools::spalette\", direction = -1) +\n    theme(legend.position = c(0.8, 0.2),\n          legend.title = element_blank()) +\n    labs(title = \"Waiting time and shrinkage\",\n         x = \"morning wait\", y = \"afternoon wait\")\n})\n# plot14_01$coefs\nwrap_plots(plot14_01[c(\"coefs\", \"wait\")]) +\n  plot_annotation(title = \"Shrinkage in two dimensions\")\n\n\n\n\nFigure 14.5"
  },
  {
    "objectID": "ch14_covariances.html#advanced-varying-slopes",
    "href": "ch14_covariances.html#advanced-varying-slopes",
    "title": "14  Adventures in Covariance",
    "section": "14.2 Advanced varying slopes",
    "text": "14.2 Advanced varying slopes\n\ndata(chimpanzees)\ndataChimp &lt;- chimpanzees |&gt;\n  mutate(block = factor(block),\n         actor = factor(actor),\n         treatment = factor(1 + prosoc_left + 2 * condition, levels = 1:4,\n                            labels = c(\"AR\", \"AL\", \"PR\", \"PL\")))\nrm(chimpanzees)\ndataChimp |&gt;\n  skim() |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = where(is.numeric), .fns = round, digits = 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\nData summary\n\n\nName\ndataChimp\n\n\nNumber of rows\n504\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nactor\n0\n1\nFALSE\n7\n1: 72, 2: 72, 3: 72, 4: 72\n\n\nblock\n0\n1\nFALSE\n6\n1: 84, 2: 84, 3: 84, 4: 84\n\n\ntreatment\n0\n1\nFALSE\n4\nAR: 126, AL: 126, PR: 126, PL: 126\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrecipient\n252\n0.5\n5.00\n2.00\n2\n3.00\n5.0\n7.00\n8\n▇▃▃▃▇\n\n\ncondition\n0\n1.0\n0.50\n0.50\n0\n0.00\n0.5\n1.00\n1\n▇▁▁▁▇\n\n\ntrial\n0\n1.0\n36.50\n20.80\n1\n18.75\n36.5\n54.25\n72\n▇▇▇▇▇\n\n\nprosoc_left\n0\n1.0\n0.50\n0.50\n0\n0.00\n0.5\n1.00\n1\n▇▁▁▁▇\n\n\nchose_prosoc\n0\n1.0\n0.57\n0.50\n0\n0.00\n1.0\n1.00\n1\n▆▁▁▁▇\n\n\npulled_left\n0\n1.0\n0.58\n0.49\n0\n0.00\n1.0\n1.00\n1\n▆▁▁▁▇\n\n\n\n\n\n\n14.2.0.1 The model\n$$ \\[\\begin{align*}\nL_i &\\sim \\mathcal{Binomial}(1, p_i) \\\\\nlogit(p_i) &\\sim \\gamma_{treatment[i]} + \\alpha_{actor[i], treatment[i]} + \\beta_{block[i], treatment[i]} \\\\\n\\gamma_{treatment[i]} &\\sim \\mathcal{N}(0, 1), \\, \\text{for } i = 1 \\ldots 4 \\\\\n\n\n\\begin{bmatrix}\n\\alpha_{j, 1} \\\\\n\\alpha_{j, 2} \\\\\n\\alpha_{j, 3} \\\\\n\\alpha_{j, 4}\n\\end{bmatrix}\n&\\sim\n\\mathcal{MVNormal}(\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0\\\\\n\\end{bmatrix}\n,\n\\bf{\\Sigma_{actor}}\n) \\\\\n\n\n\\begin{bmatrix}\n\\beta_{j, 1} \\\\\n\\beta_{j, 2} \\\\\n\\beta_{j, 3} \\\\\n\\beta_{j, 4}\n\\end{bmatrix}\n&\\sim\n\\mathcal{MVNormal}(\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n,\n\\bf{\\Sigma_{block}}\n) \\\\\n\n\n\n\\bf{\\Sigma_{actor}} &=\n\\begin{bmatrix}\n\\sigma_{factor} & 0 \\\\\n0 & \\sigma_{factor}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 & \\rho_{factor} \\\\\n\\rho_{factor} & 1\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n\\sigma_{factor} & 0 \\\\\n0 & \\sigma_{factor}\\\\\n\\end{bmatrix} \\\\\n\n\n\n\\bf{\\Sigma_{block}} &=\n\\begin{bmatrix}\n\\sigma_{block} & 0 \\\\\n0 & \\sigma_{block}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 & \\rho_{block} \\\\\n\\rho_{block} & 1\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n\\sigma_{block} & 0 \\\\\n0 & \\sigma_{block}\\\\\n\\end{bmatrix} \\\\\n\n\n\n\\sigma_{actor, j} &\\sim \\mathcal{Exponential}(1), \\, \\text{for } i = 1 \\ldots 4 \\\\\n\\sigma_{block, j} &\\sim \\mathcal{Exponential}(1), \\, \\text{for } i = 1 \\ldots 4 \\\\\n\n\n\\rho_{actor}, \\rho_{block} &\\sim \\mathcal{LKJcorr}(2)\n\n\\end{align*}\\] $$\nWe don’t do model m14.2 since it is only done to illustrate centralized vs non-centralized parametrization and that brms uses only non-centralized parametrization.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nfit14_03 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = dataChimp,\n    family = bernoulli,\n    formula = bf(pulled_left ~ 0 + treatment + (0 + treatment | actor) + (0 + treatment | block)),\n    prior = c(\n      prior(normal(0, 1), class = b),\n      prior(exponential(1), class = sd, group = actor),\n      prior(exponential(1), class = sd, group = block),\n      prior(lkj(2), class = cor, group = actor),\n      prior(lkj(2), class = cor, group = block)),\n    iter = 1000, warmup = 500, chains = 2,\n    cores = detectCores(), seed = 1427)\n  add_criterion(out, c(\"loo\", \"waic\"))},\n  file = \"ch14_fit14_03\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.14 sec elapsed\n\n\nThe plot is slightly different than what McElreath has. The open circle represent the actual results and the solid circles are the predicted mean.\n\nplot14_03 &lt;- list()\nplot14_03 &lt;- within(plot14_03, {\n\n  df &lt;- dataPredicted &lt;- dataChimp |&gt;\n    group_by(actor, treatment, block) |&gt;\n    summarize(prop = mean(pulled_left)) |&gt;\n    ungroup() |&gt;\n    add_epred_draws(fit14_03) |&gt;\n    summarize(prop = mean(prop),\n              mean_qi(.epred, .width = 0.89) ) |&gt;\n    rename(\".epred\" = y, \".lower\" = ymin, \".upper\" = ymax)\n  \n  # every block is different but, for plotting, we use the average of the blocks\n  df &lt;- df |&gt;\n    group_by(actor, treatment) |&gt;\n    summarize(\n      prop = mean(prop),\n      .epred = mean(.epred),\n      .lower = mean(.lower),\n      .upper = mean(.upper)) |&gt;\n    mutate(label = paste(\"actor\", actor)) |&gt;\n    mutate(condition = if_else(substring(treatment, 1, 1) == \"A\", \"alone\", \"partner\"),\n           condition = as.factor(condition),\n           prosoc_left = if_else(substring(treatment, 2, 2) == \"R\", \"right\", \"left\"),\n           prosoc_left = as.factor(prosoc_left)) |&gt;\n    mutate(label = paste(\"actor\", actor))\n    \n    \n    \n  p &lt;- df |&gt;\n    ggplot(aes(x = treatment, y = .epred,\n                             group = prosoc_left, color = prosoc_left,\n                             fill = prosoc_left)) +\n    geom_line(linetype = \"solid\", size = 1) +\n    geom_point(shape = 16, size = 3) +\n    geom_line(aes(y = prop), linetype = \"solid\", size = 1) +\n    geom_point(aes(y = prop), shape = 1, size = 3) +\n    geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 1/3) +\n    geom_hline(yintercept = 0.5, color = \"brown\", linetype = 2) +\n    scale_y_continuous(labels = scales::label_percent()) +\n    scale_color_paletteer_d(\"jcolors::pal9\") +\n    coord_cartesian(ylim = c(0, 1)) +\n    theme(legend.position = \"bottom\") +\n    labs(title = \"Posterior expected predictions with 89% CI\",\n         subtitle = \"Open circles are actual results, solid circles are mean predictions.\",\n         x = NULL, y = \"proportion pulled left\") +\n    facet_grid(. ~ label)\n})\n\n`summarise()` has grouped output by 'actor', 'treatment'. You can override\nusing the `.groups` argument.\n`summarise()` has grouped output by 'actor', 'treatment', 'block', 'prop'. You\ncan override using the `.groups` argument.\n`summarise()` has grouped output by 'actor'. You can override using the\n`.groups` argument.\n\n# glimpse(plot14_03$df)\nplot14_03$p\n\n\n\n\nFigure 14.7"
  },
  {
    "objectID": "ch14_covariances.html#instruments-and-causal-designs",
    "href": "ch14_covariances.html#instruments-and-causal-designs",
    "title": "14  Adventures in Covariance",
    "section": "14.3 Instruments and Causal designs",
    "text": "14.3 Instruments and Causal designs\n\nggdag::dagify(E ~ U, W ~ U, W ~ E) |&gt;\n    ggdag::ggdag_classic(layout = \"sugiyama\", text_col = \"royalblue\") +\n    ggdag::theme_dag_blank(\n      panel.background = element_rect(fill = \"snow\", color = \"snow\"))\n\n\n\n\n\nIn causal terms, an instrument variable is a variable that acts like a natural experiment on the exposure \\(E\\).\n\nIn mathematical terms the instrumental variable \\(Q\\) is characterized as follows:\n\nIndependent of \\(U\\), i.e. \\(Q \\perp\\!\\!\\!\\perp U\\)\nNot independent of \\(E\\), i.e. \\(Q \\not\\!\\perp\\!\\!\\!\\perp E\\)\nHas no effect on \\(W\\) except through \\(E\\), also called the exclusion condition\n\n\nThe exclusion restriction cannot be tested, and it is often implausible.\n\nIn the education and wage example, the simplest instrument variable \\(Q\\) would be as follows\n\nggdag::dagify(E ~ U + Q, W ~ U, W ~ E) |&gt;\n    ggdag::ggdag_classic(layout = \"sugiyama\", text_col = \"royalblue\") +\n    ggdag::theme_dag_blank(\n      panel.background = element_rect(fill = \"snow\", color = \"snow\"))\n\n\n\n\nWe now use a simulation to illustrate.\n\nWith real data, you never know what the right anser is. This is h=why studying simulated examples is so important.\n\n\nsimInstrument &lt;- list()\nsimInstrument &lt;- within(simInstrument, {\n  n = 500L\n  U &lt;- rnorm(n = n)\n  Q &lt;- sample(x = 1:4, size = n, replace = TRUE)\n  E &lt;- rnorm(n = n, mean = U + Q)\n  # we assume that the true influence of E (education)\n  #  on W (wage) is zero. Just for the sake of the example.\n  W &lt;- rnorm(n = n, mean = U + 0 * E)\n  df &lt;- data.frame(\n    \"W\" = scale(W),\n    \"E\" = scale(E),\n    \"Q\" = scale(Q)\n  )\n})\nsimInstrument$df |&gt;\n  glimpse()\n\nRows: 500\nColumns: 3\n$ W &lt;dbl&gt; -1.12780402, -0.78848452, 0.94201935, -0.04856341, -0.20283111, -0.2…\n$ E &lt;dbl&gt; -0.50147547, -0.16726498, 0.44072307, -0.47604532, 0.52109651, -1.25…\n$ Q &lt;dbl&gt; -1.4403794, 0.3782814, -1.4403794, -0.5310490, 1.2876118, -1.4403794…\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit14_04 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = simInstrument$df,\n    family = gaussian,\n    formula = W ~ 1 + E,\n    prior = c(\n      prior(normal(0, 0.2), class = Intercept),\n      prior(normal(0, 0.5), class = b),\n      prior(exponential(1), class = sigma)),\n    iter = 1000, warmup = 500, chains = 2,\n    cores = detectCores(), seed = 1429)\n  add_criterion(out, c(\"loo\", \"waic\"))},\n  file = \"ch14_fit14_04\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.11 sec elapsed\n\n\nwhich gives us the results\n\nsummarize_draws(fit14_04, \"mean\", \"sd\", ~quantile(.x, probs = c(0.055, 0.945)),\n                default_convergence_measures()) |&gt;\n  filter(!grepl(\"^lp\", x = variable)) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),\n         across(.cols = starts_with(\"ess\"), .fns = as.integer))\n\n# A tibble: 3 × 8\n  variable     mean    sd `5.5%` `94.5%`  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;num&gt; &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 b_Intercept  0     0.04  -0.06    0.06     1     1103      636\n2 b_E          0.4   0.04   0.34    0.46     1      970      696\n3 sigma        0.92  0.03   0.87    0.96     1     1172      843\n\n\nThe value \\(b_E\\) should have been close to zero. The oncorrect value is caused by the confounding effect of \\(U\\).\nNow, lets see what happens when we include the instrumental variable \\(Q\\).\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit14_05 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = simInstrument$df,\n    family = gaussian,\n    formula = W ~ 1 + E + Q,\n    prior = c(\n      prior(normal(0, 0.2), class = Intercept),\n      prior(normal(0, 0.5), class = b),\n      prior(exponential(1), class = sigma)),\n    iter = 1000, warmup = 500, chains = 2,\n    cores = detectCores(), seed = 1429)\n  add_criterion(out, c(\"loo\", \"waic\"))},\n  file = \"ch14_fit14_05\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.11 sec elapsed\n\n\nwhich gives us the results\n\nsummarize_draws(fit14_05, \"mean\", \"sd\", ~quantile(.x, probs = c(0.055, 0.945)),\n                default_convergence_measures()) |&gt;\n  filter(!grepl(\"^lp\", x = variable)) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),\n         across(.cols = starts_with(\"ess\"), .fns = as.integer))\n\n# A tibble: 4 × 8\n  variable     mean    sd `5.5%` `94.5%`  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;num&gt; &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 b_Intercept  0     0.04  -0.06    0.06  1         896      646\n2 b_E          0.6   0.05   0.52    0.68  1.01      941      577\n3 b_Q         -0.35  0.05  -0.43   -0.27  1         843      650\n4 sigma        0.87  0.03   0.83    0.92  1         872      737\n\n\nThe results are now even more confusing as the influence of \\(Q\\) makes the effect of \\(E\\) difficult to evaluate on its own.\nSo lets use the instrumental variable \\(Q\\) again but taking into account the covariance of \\(E\\) and \\(Q\\). That is, we express the model as a *multivariate statistical model as follows\n\\[\n\\begin{align*}\n\\begin{bmatrix}\nW_i \\\\\nE_i \\\\\n\\end{bmatrix}\n&\\sim\n\\mathcal{MVNormal}(\n\\begin{bmatrix}\n\\mu_{W, i} \\\\\n\\mu_{E, i}\n\\end{bmatrix},\n\\bf{\\Sigma}\n) \\\\\n\\mu_{W, i} &= \\alpha_W + \\beta_{EW}E_i \\\\\n\\mu_{E, i} &= \\alpha_E + \\beta_{QE}Q_i \\\\\n\\bf{\\Sigma} &=\n\\begin{bmatrix}\n\\sigma_W & 0 \\\\\n0 & \\sigma_E\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n\\sigma_W & 0 \\\\\n0 & \\sigma_E\n\\end{bmatrix}\n\\\\\n\\alpha_W, \\alpha_E &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_{EW}, \\beta_{QE} &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma_W, \\sigma_E &\\sim \\mathcal{Exponential}(1) \\\\\n\\rho &\\sim \\mathcal{LKJ}(2)\n\\end{align*}\n\\]\nand the final fit is\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"90 secs.\"))\nfit14_06 &lt;- xfun::cache_rds({\n  out &lt;- brm(\n    data = simInstrument$df,\n    family = gaussian,\n    formula = bf(W ~ 1 + E) + bf(E ~ 1 + Q),\n    prior = c(\n      prior(normal(0, 0.2), class = Intercept, resp = E),\n      prior(normal(0, 0.5), class = b, resp = E),\n      prior(exponential(1), class = sigma, resp = E),\n      prior(normal(0, 0.2), class = Intercept, resp = W),\n      prior(normal(0, 0.5), class = b, resp = W),\n      prior(exponential(1), class = sigma, resp = W),\n      prior(lkj(2), class = rescor)),\n    iter = 1000, warmup = 500, chains = 2,\n    cores = detectCores(), seed = 1429)\n  add_criterion(out, c(\"loo\", \"waic\"))},\n  file = \"ch14_fit14_06\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 90 secs., use the cache.: 0.14 sec elapsed\n\n\nwhich gives us the results\n\nsummarize_draws(fit14_06, \"mean\", \"sd\", ~quantile(.x, probs = c(0.055, 0.945)),\n                default_convergence_measures()) |&gt;\n  filter(!grepl(\"^lp\", x = variable)) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),\n         across(.cols = starts_with(\"ess\"), .fns = as.integer))\n\n# A tibble: 7 × 8\n  variable       mean    sd `5.5%` `94.5%`  rhat ess_bulk ess_tail\n  &lt;chr&gt;         &lt;num&gt; &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 b_W_Intercept  0     0.04  -0.07    0.07  1         899      640\n2 b_E_Intercept  0     0.04  -0.06    0.06  1         823      701\n3 b_W_E          0     0.08  -0.13    0.13  1.01      452      738\n4 b_E_Q          0.58  0.04   0.52    0.64  1         752      727\n5 sigma_W        1     0.04   0.93    1.07  1.01      513      738\n6 sigma_E        0.82  0.02   0.78    0.86  1         895      662\n7 rescor__W__E   0.49  0.06   0.39    0.58  1.01      444      731"
  },
  {
    "objectID": "ch14_covariances.html#social-relations-as-correlated-varying-effects",
    "href": "ch14_covariances.html#social-relations-as-correlated-varying-effects",
    "title": "14  Adventures in Covariance",
    "section": "14.4 Social relations as correlated varying effects",
    "text": "14.4 Social relations as correlated varying effects\n\n14.4.1 Data\nThe research can be find in this paper. However I could not find what the variable \\(dlndist\\) is. I suppose it it the log of some distance . . but why is it negative? It doesn’t matter, it is only used to color the edges of the network and not in the analysis per se.\n\ndata(KosterLeckie, package = \"rethinking\")\ndataKL &lt;- list(\n  \"dyads\" = kl_dyads,\n  \"houses\" = kl_households)\nrm(list = c(\"kl_dyads\", \"kl_households\"))\ndataKL$dyads &lt;- dataKL$dyads |&gt;\n  # we only use these variable\n  select(hidA, hidB, did, giftsAB, giftsBA, offset, dlndist) |&gt;\n  mutate(hidA = factor(hidA),\n         hidB = factor(hidB),\n         did = factor(did))\n\nThe data set can be summarized with the skimr package\n\nnames(dataKL$dyads)\n\n[1] \"hidA\"    \"hidB\"    \"did\"     \"giftsAB\" \"giftsBA\" \"offset\"  \"dlndist\"\n\ndataKL$dyads |&gt; skim() |&gt;\n  select(-n_missing, -complete_rate) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n\nData summary\n\n\nName\ndataKL$dyads\n\n\nNumber of rows\n300\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\nskim_variable\nordered\nn_unique\ntop_counts\n\n\n\n\nhidA\nFALSE\n24\n1: 24, 2: 23, 3: 22, 4: 21\n\n\nhidB\nFALSE\n24\n25: 24, 24: 23, 23: 22, 22: 21\n\n\ndid\nFALSE\n300\n1: 1, 2: 1, 3: 1, 4: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngiftsAB\n3.87\n7.97\n0.00\n0.00\n1.00\n4.00\n75.00\n▇▁▁▁▁\n\n\ngiftsBA\n5.70\n11.47\n0.00\n1.00\n2.00\n6.00\n110.00\n▇▁▁▁▁\n\n\noffset\n-0.13\n0.21\n-1.24\n-0.11\n-0.02\n-0.01\n0.00\n▁▁▁▁▇\n\n\ndlndist\n-2.26\n0.59\n-4.07\n-2.58\n-2.18\n-1.85\n-1.03\n▁▂▆▇▂\n\n\n\n\n\nWe can visualize the distribution of dyadic gifts using a scatter plot\n\ndataKL$dyads |&gt; ggplot(aes(x = giftsAB, y = giftsBA)) +\n  geom_point(color = \"darkblue\") +\n  geom_abline(slope = 1, intercept = 0, linetype = 2, color = \"royalblue\") +\n  coord_equal(ratio = 1, xlim = c(0, 120), ylim = c(0, 120)) +\n  labs(title = \"Distribution of dyadic gifts\")\n\n\n\n\nFigure 14.8\n\n\n\n\nand since this a social network then we can visualize the network with the ggnetwork package.For details see ggnetwork.\nWe use the tidygraph to use dplyr verbs on relational data and ggraph to plot it.\n\nnetKL &lt;- list()\nnetKL &lt;- within(netKL, {\n  # the tbl_graph object\n  the_edges &lt;- dataKL$dyads |&gt;\n    mutate(giftsSum = abs(giftsAB - giftsBA))\n  the_nodes &lt;- dataKL$houses\n  grf &lt;- tidygraph::tbl_graph(nodes = the_nodes, edges = the_edges, directed = FALSE)\n\n  colrs &lt;- paletteer::paletteer_c(\"oompaBase::jetColors\", direction = -1, n = 16)\n  p &lt;- ggraph(grf, layout = \"circle\") +\n    geom_edge_link(aes(color = dlndist)) +\n    geom_node_point(aes(size = hwealth), color = \"brown\") +\n    scale_color_paletteer_c(\"oompaBase::jetColors\", direction = -1) +\n    scale_size_continuous(range = c(1, 4)) +\n    scale_edge_color_gradientn(colors = colrs) +\n    ggraph::theme_graph() +\n    theme(title = element_text(color = \"midnightblue\")) +\n    labs(title = \"The KosterLeckie Network\")\n})\nnetKL$p\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\n14.4.2 Model\n$$ \\[\\begin{align*}\n\\begin{bmatrix}\ny_{a \\rightarrow b} \\\\\ny_{b \\rightarrow a} \\\\\n\\end{bmatrix}\n&\\sim\n\\begin{bmatrix}\n\\mathcal{Poisson}(\\lambda_{AB}) \\\\\n\\mathcal{Poisson}(\\lambda_{BA}) \\\\\n\\end{bmatrix} \\\\\n\\log{\\lambda_{AB}} &= \\alpha + g_A + r_B + d_{AB} \\\\\n\\log{\\lambda_{BA}} &= \\alpha + g_B + r_A + d_{BA} \\\\\n\\begin{bmatrix}\ng_i \\\\\nr_i \\\\\n\\end{bmatrix}\n&\\sim\n\\mathcal{MVNormal}(\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix},\n\\Sigma_{gr}\n) \\\\\n\\begin{bmatrix}\nd_{ij} \\\\\nd_{ji} \\\\\n\\end{bmatrix}\n&\\sim\n\\mathcal{MVNormal}(\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix},\n\\Sigma_d\n) \\\\\n\n\\Sigma_{gr} &=\n\\begin{bmatrix}\n\\sigma_g & 0 \\\\\n0 & \\sigma_r &\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 & \\rho_{gr} \\\\\n\\rho_{gr} & 1 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n\\sigma_g & 0 \\\\\n0 & \\sigma_r \\\\\n\\end{bmatrix} \\\\\n\n\\Sigma_d &=\n\\begin{bmatrix}\n\\sigma_d & 0 \\\\\n0 & \\sigma_d &\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 & \\rho_d \\\\\n\\rho_d & 1 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n\\sigma_d & 0 \\\\\n0 & \\sigma_d \\\\\n\\end{bmatrix} \\\\\n\n\\sigma_d, \\sigma_g, \\sigma_r &\\sim \\mathcal{Exponential}(1) \\\\\n\n\n\\rho_{gr}, \\rho_d &\\sim \\mathcal{LKJ}(4)\n\\end{align*}\\] $$\nKurtz says that there is no known way to use brms. The package bisonR that specializes in social networks and uses brms. It can be found at bison with a useful vignette at vignette.\n\n\n\n\n\n\nNote\n\n\n\nThis section is skipped. But one day, it might be interesting to do it with the bisonR package."
  },
  {
    "objectID": "ch14_covariances.html#continuous-categories-and-the-gaussian-process",
    "href": "ch14_covariances.html#continuous-categories-and-the-gaussian-process",
    "title": "14  Adventures in Covariance",
    "section": "14.5 Continuous categories and the Gaussian process",
    "text": "14.5 Continuous categories and the Gaussian process\n\n14.5.1 Spatial autocorrelation in Oceanic tools\n\n14.5.1.1 Data\nThe observed data is as follows. All the data is put in 2 dataframes to be able to analyse it as graph data.\n\nnodes_df: The data related to the nodes\nedges_df: The data related to the edges, e.g. the distance between the cultures\n\nFirst we define the nodes dataframes\n\ndata(Kline2)\ndataKline &lt;- list()\ndataKline &lt;- within(dataKline, {\n  nodes_df &lt;- Kline2 |&gt;\n    mutate(log_pop_s = as.vector(scale(logpop)),\n           cid = factor(contact, levels = c(\"low\", \"high\"))) |&gt;\n    # latitude and longitude coverted to thousands of km\n    # using the average distance at the equator\n    mutate(lat_pos = lat * 0.11132,\n           lon2_pos = lon2 * 0.11132)\n  })\nrm(Kline2)\ndataKline$nodes_df |&gt;\n  skim() |&gt;\n  select(-n_missing, -complete_rate) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n\nData summary\n\n\nName\ndataKline$nodes_df\n\n\nNumber of rows\n10\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\nskim_variable\nordered\nn_unique\ntop_counts\n\n\n\n\nculture\nFALSE\n10\nChu: 1, Haw: 1, Lau: 1, Mal: 1\n\n\ncontact\nFALSE\n2\nhig: 5, low: 5\n\n\ncid\nFALSE\n2\nlow: 5, hig: 5\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npopulation\n34109.10\n84793.03\n1100.00\n3897.75\n7700.00\n12050.00\n275000.00\n▇▁▁▁▁\n\n\ntotal_tools\n34.80\n17.85\n13.00\n22.50\n30.50\n42.25\n71.00\n▇▃▃▂▂\n\n\nmean_TU\n4.83\n1.14\n3.20\n4.00\n4.85\n5.30\n6.60\n▅▅▇▂▅\n\n\nlat\n-5.22\n13.51\n-21.20\n-15.30\n-9.70\n5.03\n19.90\n▇▇▂▅▂\n\n\nlon\n93.71\n137.16\n-175.20\n140.30\n151.25\n167.12\n178.10\n▂▁▁▁▇\n\n\nlon2\n-14.29\n19.89\n-41.90\n-28.93\n-13.25\n-4.23\n24.40\n▇▂▇▅▂\n\n\nlogpop\n8.98\n1.53\n7.00\n8.26\n8.95\n9.39\n12.52\n▃▇▃▁▂\n\n\nlog_pop_s\n0.00\n1.00\n-1.29\n-0.47\n-0.02\n0.27\n2.32\n▃▇▃▁▂\n\n\nlat_pos\n-0.58\n1.50\n-2.36\n-1.70\n-1.08\n0.56\n2.22\n▇▇▂▅▂\n\n\nlon2_pos\n-1.59\n2.21\n-4.66\n-3.22\n-1.47\n-0.47\n2.72\n▇▂▇▅▂\n\n\n\n\n# glimpse(dataKline$df)\n\nand we will also use a matrix of the longitudinal and latitudinal relative positions of the societies. This is an approximation only as it used the km per degree at the equator which is 111.32 km per degree at the equator. This gives the following relative position matrix.\n\ndata(islandsDistMatrix)\ndataKline &lt;- within(dataKline, {\n  dist &lt;- islandsDistMatrix\n  # create the edges dataframe\n  edges_df &lt;- dist\n  # set lower triangle and diagonal to 0 to avoid double values.\n  # zeros can be removed later\n  edges_df[lower.tri(edges_df, diag = FALSE)] &lt;- NA_real_\n  \n  # The distances in dataframe format\n  edges_df &lt;- dist |&gt;\n    as.data.frame() |&gt;\n    tibble::rownames_to_column(var = \"x\") |&gt;\n    pivot_longer(cols = -x, names_to = \"y\", values_to = \"dist\")\n  })\nrm(islandsDistMatrix)\n\nand we can visualize the distance in a heatmap\n\nheatmaply::heatmaply(\n  x = dataKline$dist,\n  cellnote = round(dataKline$dist, 1),\n  colors = unclass(paletteer::paletteer_c(\"pals::ocean.speed\", n = 16)),\n  hide_colorbar = TRUE,\n  main = \"Distances between Oceanic Societies in Thousands of km\")\n\n\n\n\n\nAnd the basic map, shown as a graph, with latitude and longitude of the culture can be visualize with the ggraph package.\n\ngraph14_08 &lt;- list()\ngraph14_08 &lt;- within(graph14_08, {\n  # create the graph object\n  # the manual layout requires a special treatment when using ggraph\n  # source for manual layout:\n  # https://stackoverflow.com/questions/67756538/plot-ggraph-using-supplied-node-coordinates\n  # must add the x and y coords to the nodes for manual layout to work\n  the_nodes &lt;- dataKline$nodes_df |&gt;\n    select(culture, x = lon2_pos, y = lat_pos, logpop)\n  the_edges &lt;- dataKline$edges_df\n  grf &lt;- tbl_graph(nodes = the_nodes, edges = the_edges, directed = FALSE)\n\n  # we use a special layout, when the x and y have different names to avoid\n  # conflict. See the source on manual layout mentioned above\n  p &lt;- ggraph(grf, layout = data.frame(lx = NA, ly = NA)) +\n    geom_node_point(aes(color = logpop, size = logpop)) +\n    geom_node_text(aes(label = culture), color = \"darkblue\", size = 4, repel = TRUE) +\n    scale_color_paletteer_c(\"pals::isol\") +\n    theme(legend.position = \"none\") +\n    labs(title = \"Relative positions of societies\",\n         x = \"longitude in thousands of km\", y = \"latitude in thousands of km\")\n})\ngraph14_08$p\n\n\n\n\n\n\n\n\n\n\n14.5.1.2 Model\nWe use the scientific model. See section 11.2.1, p. 356, in the Overthinking box.\n\\[\n\\begin{align*}\nT_i &\\sim \\mathcal{Poisson}(\\lambda_i) \\\\\n\\lambda_i &= \\frac{\\alpha P_i^\\beta}{\\gamma}\n\\end{align*}\n\\]\nand since we need a varying intercept we had one in a multiplicative form. We could simply add it but then it could become negative.\n\\[\n\\begin{align*}\nT_i &\\sim \\mathcal{Poisson}(\\lambda_i) \\\\\n\\lambda_i &= \\exp(k_{society}) \\frac{\\alpha P_i^\\beta}{\\gamma}\n\\end{align*}\n\\] Here \\(k_{society}\\) is the varying intercept. These intercept are part of a multivariate distribution. The multivariate intercept prior is defined as\n\\[\n\\begin{align*}\n\\begin{bmatrix}\nk_1 \\\\\nk_2 \\\\\nk_3 \\\\\n\\vdots \\\\\nk_{10}\n\\end{bmatrix}\n&\\sim\n\\mathcal{MVNormal} \\left(\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n,\n\\bf{K}\n\\right) \\\\\n\\bf{K} &= \\eta^2 \\exp(-\\rho^2D_{ij}^2) + \\delta_{ij}\\sigma^2\n\\end{align*}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe rest of the section comes from Kurz (2020) who did such a fantastic work at adapting the wonderful rethinking to brms. I am so grateful for the opportunity to enjoy all this.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere is a lot more details and info at Kurz (2020). Please read it to get the full picture.\n\n\nWe could have used \\(D_{ij}\\) instead of \\(D_{ij}^2\\) for \\(\\bf{K}\\). \\(D_{ij}^2\\) because of its half-Gaussian shape which is more sigmoidal in shape. As illustrated just below.\n\nggplot(data.frame(x = c(0, 4)), aes(x = x)) +\n  stat_function(aes(color = \"-exp(D)\"), fun = function(x) exp(-1 * x), size = 2) +\n  stat_function(aes(color = \"-exp(D^2)\"), fun = function(x) exp(-1 * x^2), size = 2) +\n  scale_color_paletteer_d(\"ggsci::alternating_igv\") +\n  theme(legend.position = c(0.8, 0.8),\n        legend.title = element_blank()) +\n  labs(title = \"Shape of function relating distance to covariance\",\n       x = \"distance\", y = \"correlation\")\n\n\n\n\nFigure 14.10\n\n\n\n\nTherefore we have the model used by rethinking and the one by brms\n\n14.5.1.2.1 rethinking model\n\\[\n\\begin{align*}\nT_i &\\sim \\mathcal{Poisson}(\\lambda_i) \\\\\n\\lambda_i &= \\exp(k_{society}) \\frac{\\alpha Population_i^\\beta}{\\gamma} \\\\\n\\begin{bmatrix}\nk_1 \\\\\nk_2 \\\\\nk_3 \\\\\n\\vdots \\\\\nk_{10}\n\\end{bmatrix}\n&\\sim\n\\mathcal{MVNormal}\\left(\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n,\n\\bf{K}\n\\right) \\\\\n\\bf{K} &= \\eta^2 \\exp(-\\rho^2Distance_{ij}^2) + \\delta_{ij}\\sigma^2 \\\\\n\\delta_{ij} &= 0 \\implies \\delta_{ij}\\sigma^2 = 0 \\; \\text{because only 1 observation per island} \\\\\n\\alpha, \\beta, \\gamma &\\sim \\mathcal{Exponential}(1) \\\\\n\\eta^2 &\\sim \\mathcal{Exponential}(2) \\\\\n\\rho^2 &\\sim \\mathcal{Exponential}(0.5)\n\\end{align*}\n\\] ##### brms model\n\\[\n\\begin{align*}\nT_i &\\sim \\mathcal{Poisson}(\\lambda_i) \\\\\n\\lambda_i &= \\exp(k_{society}) \\frac{\\alpha Population_i^\\beta}{\\gamma} \\\\\n\\begin{bmatrix}\nk_1 \\\\\nk_2 \\\\\nk_3 \\\\\n\\vdots \\\\\nk_{10}\n\\end{bmatrix}\n&\\sim\n\\mathcal{MVNormal} \\left(\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n,\n\\bf{K}\n\\right) \\\\\n\\bf{K_{ij}} &= sdgp^2 \\exp \\left(\\frac{-Distance_{ij}^2}{2 \\cdot lscale^2} \\right) \\\\\n\\alpha &\\sim \\mathcal{N}(0, 1) \\\\\n\\beta, \\gamma &\\sim \\mathcal{Exponential}(1) \\\\\nsgdp^2 &\\sim \\mathcal{Exponential}(1) \\\\\nlscale^2 &\\sim \\mathcal{InvGammal}(2.874624, 2.941204)\n\\end{align*}\n\\]\n\n\n\n14.5.1.3 Fit\nSince our model is non-linear (scientific formula) then the fit with brms uses the non-linear form.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"90 secs.\"))\nfit14_08 &lt;- xfun::cache_rds({\n  brm(\n    data = dataKline$nodes_df,\n    family = poisson(link = \"identity\"),\n    formula =  bf(total_tools ~ exp(a) * population^b / g,\n                  a ~ 1 + gp(lat_pos, lon2_pos, scale = FALSE),\n                  b + g ~ 1,\n                  nl = TRUE),\n    prior = c(\n      prior(normal(0, 1), nlpar = a),\n      prior(exponential(1), nlpar = b, lb = 0),\n      prior(exponential(1), nlpar = g, lb = 0),\n      prior(inv_gamma(2.874624, 2.941204), class = lscale, coef = gplat_poslon2_pos, nlpar = a),\n      prior(exponential(1), class = sdgp, coef = gplat_poslon2_pos, nlpar = a)),\n    iter = 2000, warmup = 1000, chains = 2,\n    sample_prior = TRUE,\n    cores = detectCores(), seed = 1433)},\n  file = \"ch14_fit14_08\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 90 secs., use the cache.: 0.14 sec elapsed\n\n\nwhich gives the results\n\nfit14_08\n\n Family: poisson \n  Links: mu = identity \nFormula: total_tools ~ exp(a) * population^b/g \n         a ~ 1 + gp(lat_pos, lon2_pos, scale = FALSE)\n         b ~ 1\n         g ~ 1\n   Data: dataKline$nodes_df (Number of observations: 10) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nGaussian Process Terms: \n                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsdgp(a_gplat_poslon2_pos)       0.50      0.34     0.14     1.42 1.00      628\nlscale(a_gplat_poslon2_pos)     1.69      0.98     0.51     4.29 1.00      643\n                            Tail_ESS\nsdgp(a_gplat_poslon2_pos)       1169\nlscale(a_gplat_poslon2_pos)     1068\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_Intercept     0.37      0.85    -1.32     1.95 1.00     1418     1418\nb_Intercept     0.25      0.09     0.07     0.43 1.00      761      446\ng_Intercept     0.65      0.62     0.06     2.34 1.00     1103      946\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand to have a summary similar to McElreath on p. 472 we can use\n\nfit14_08 |&gt;\n  summarize_draws(\"mean\", \"sd\", ~quantile(.x, probs = c(0.055, 0.945)),\n                  default_convergence_measures()) |&gt;\n  filter(!grepl(\"^lp\", x = variable)) |&gt;\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),\n         across(.cols = starts_with(\"ess\"), .fns = as.integer))\n\n# A tibble: 20 × 8\n   variable                    mean    sd `5.5%` `94.5%`  rhat ess_bulk ess_tail\n   &lt;chr&gt;                      &lt;num&gt; &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n 1 b_a_Intercept               0.37  0.85  -1.02    1.68     1     1417     1417\n 2 b_b_Intercept               0.25  0.09   0.11    0.38     1      761      445\n 3 b_g_Intercept               0.65  0.62   0.09    1.78     1     1102      946\n 4 sdgp_a_gplat_poslon2_pos    0.5   0.34   0.18    1.09     1      627     1169\n 5 lscale_a_gplat_poslon2_pos  1.69  0.98   0.64    3.52     1      642     1068\n 6 zgp_a_gplat_poslon2_pos[1] -0.47  0.75  -1.7     0.66     1     1131     1397\n 7 zgp_a_gplat_poslon2_pos[2]  0.44  0.87  -0.99    1.8      1     1521     1190\n 8 zgp_a_gplat_poslon2_pos[3] -0.62  0.7   -1.65    0.55     1     1335     1371\n 9 zgp_a_gplat_poslon2_pos[4]  0.97  0.68  -0.06    2.1      1     1013     1140\n10 zgp_a_gplat_poslon2_pos[5]  0.24  0.76  -0.91    1.43     1     1366     1307\n11 zgp_a_gplat_poslon2_pos[6] -1.03  0.79  -2.25    0.27     1     1079      987\n12 zgp_a_gplat_poslon2_pos[7]  0.15  0.68  -0.97    1.2      1     1594     1179\n13 zgp_a_gplat_poslon2_pos[8] -0.18  0.87  -1.6     1.21     1     2175     1102\n14 zgp_a_gplat_poslon2_pos[9]  0.4   0.92  -1.12    1.81     1     1676     1228\n15 zgp_a_gplat_poslon2_pos[1… -0.36  0.81  -1.71    0.87     1     1171     1299\n16 prior_b_a                  -0.01  1.01  -1.62    1.58     1     1943     1719\n17 prior_sdgp_a_gplat_poslon…  0.98  0.97   0.06    2.85     1     1737     1789\n18 prior_lscale_a__1_gplat_p…  1.51  1.31   0.49    3.59     1     2190     2046\n19 prior_b_b                   1.01  1.05   0.06    2.95     1     1955     1906\n20 prior_b_g                   1     1.02   0.06    2.96     1     1653     2050\n\n\nor with posterior_summary which is simpler but lacks de ESS\n\nposterior_summary(fit14_08) |&gt;\n  round(digits = 2)\n\n                                    Estimate Est.Error   Q2.5  Q97.5\nb_a_Intercept                           0.37      0.85  -1.32   1.95\nb_b_Intercept                           0.25      0.09   0.07   0.43\nb_g_Intercept                           0.65      0.62   0.06   2.34\nsdgp_a_gplat_poslon2_pos                0.50      0.34   0.14   1.42\nlscale_a_gplat_poslon2_pos              1.69      0.98   0.51   4.29\nzgp_a_gplat_poslon2_pos[1]             -0.47      0.75  -1.99   0.98\nzgp_a_gplat_poslon2_pos[2]              0.44      0.87  -1.32   2.13\nzgp_a_gplat_poslon2_pos[3]             -0.62      0.70  -1.90   0.94\nzgp_a_gplat_poslon2_pos[4]              0.97      0.68  -0.28   2.39\nzgp_a_gplat_poslon2_pos[5]              0.24      0.76  -1.27   1.77\nzgp_a_gplat_poslon2_pos[6]             -1.03      0.79  -2.55   0.59\nzgp_a_gplat_poslon2_pos[7]              0.15      0.68  -1.27   1.47\nzgp_a_gplat_poslon2_pos[8]             -0.18      0.87  -1.92   1.55\nzgp_a_gplat_poslon2_pos[9]              0.40      0.92  -1.54   2.17\nzgp_a_gplat_poslon2_pos[10]            -0.36      0.81  -1.99   1.20\nprior_b_a                              -0.01      1.01  -1.97   1.87\nprior_sdgp_a_gplat_poslon2_pos          0.98      0.97   0.03   3.68\nprior_lscale_a__1_gplat_poslon2_pos     1.51      1.31   0.40   5.09\nprior_b_b                               1.01      1.05   0.02   3.91\nprior_b_g                               1.00      1.02   0.02   3.73\nlprior                                 -4.02      1.57  -7.77  -2.02\nlp__                                  -51.68      3.42 -59.55 -46.03\n\n\n\n\n14.5.1.4 Analysis\nThe differences with rethinking are because brms uses non-centering. The coefficient that is really different is the \\(\\alpha\\) in \\(\\lambda_i = \\exp(k_{society}) \\frac{\\alpha P_i^\\beta}{\\gamma}\\).\nTo evaluate the differences between brms and rethinking we can look at what the posterior would be like, without taking into account the covariances.\nFor brms we have\n\npop &lt;- as.integer(c(min(dataKline$nodes_df$population),\n             median(dataKline$nodes_df$population),\n             max(dataKline$nodes_df$population)))\ntools &lt;- as.integer(c(min(dataKline$nodes_df$total_tools),\n               mean(dataKline$nodes_df$total_tools),\n               max(dataKline$nodes_df$total_tools)))\na &lt;- fixef(fit14_08)[\"a_Intercept\", \"Estimate\"]\nb &lt;- fixef(fit14_08)[\"b_Intercept\", \"Estimate\"]\ng &lt;- fixef(fit14_08)[\"g_Intercept\", \"Estimate\"]\n# brms uses a different equation for lambda\nl &lt;- exp(a) * pop ^ b / g\nprob_brms &lt;- round(dpois(x = tools, lambda = l), 4)\nprob_brms\n\n[1] 0.1099 0.0028 0.0027\n\n\nas opposed to `rehinking`` which would give\n\na &lt;- 1.41\nb &lt;- 0.28\ng &lt;- 0.60\n# rethinking uses a different equation for lambda\nl &lt;- a * pop ^ b / g\nprob_rethink &lt;- round(dpois(x = tools, lambda = l), 4)\nprob_rethink\n\n[1] 0.0706 0.0437 0.0331\n\n\nThe results between the 2 are different and cannot really be compared because\n\ncorrelations between \\(D\\) in rethinking and \\(D^2\\) in brms is not the same and\nbrms has a very different way of computing the likelihood\n\nBasically, rethinking uses the following definition of \\(\\bf{K_{ij}}\\)\n\\[\n\\bf{K_{ij}} = \\eta^2 \\exp \\left(-\\rho^2D_{ij}^2 \\right) + \\delta_{ij}\\sigma^2\n\\]\nwhere the term \\(\\delta_{ij}\\sigma^2\\) is used when \\(i=j\\), see p. 470 of McElreath (2020). In the oceanic case it is not applicable since we have only one observation per society. Therefore the equation is this case is\n\\[\n\\bf{K_{ij}} = \\eta^2 \\exp \\left(-\\rho^2D_{ij}^2 \\right)\n\\]\nwhereas brms uses\n\\[\n\\bf{K_{ij}} = sdgp^2 \\exp \\left(\\frac{-D_{ij}^2}{2 \\cdot lscale^2} \\right)\n\\]\nand so the 2 equations can be compared as follows\n\\[\n\\begin{align*}\n\\eta^2 &= sdgp^2 \\\\\n-\\rho^2 &= \\frac{1}{2 \\cdot lscale^2}\n\\end{align*}\n\\]\nand we note that the distance used can always be found as follows\n\\[\n\\begin{align*}\n\\bf{K_{ij}} &= sdgp^2 \\exp \\left(\\frac{-D_{ij}^2}{2 \\cdot lscale^2} \\right) \\\\\n\\log \\left( \\bf{K_{ij}} \\right) &= \\log \\left(sdgp^2 \\right) + \\frac{-D_{ij}^2}{2 \\cdot lscale^2} \\\\\nD_{ij}^2 &= 2 \\cdot lscale^2 \\cdot \\left[ \\log \\left(sdgp^2 \\right) - \\log \\left( \\bf{K_{ij}} \\right) \\right]\n\\end{align*}\n\\]\nand therefore we can figure out the covariance and convert the coefficients between brms and `rethinking``,\n\npost14_08 &lt;- list()\npost14_08 &lt;- within(post14_08, {\n  # function to compute eta squared as per rethinking\n  rethink_eta2 &lt;- function(sdgp) {\n    sdgp^2\n  }\n  # function to compute rho squared as per rethinking\n  rethink_rho2 &lt;- function(lscale) {\n    -1 / (2 * lscale^2)\n  }\n  # function to calculate the covariance as per brms\n  brms_cov &lt;- function(x, sdgp, lscale) {\n    sdgp^2 * exp(-(x^2) / 2 * lscale^2)\n  }\n})\n\nand we transform the posterior data to be able to plot as shown in figure 14.11 on p. 473\n\npost14_08 &lt;- within(post14_08, {\n\n  post &lt;- tidy_draws(fit14_08)\n  \n  # the median of the parameters is used later\n  stats &lt;- post |&gt;\n    select(prior_sdgp_a_gplat_poslon2_pos,\n           sdgp_a_gplat_poslon2_pos,\n           prior_lscale_a__1_gplat_poslon2_pos,\n           lscale_a_gplat_poslon2_pos) |&gt;\n    median_qi(.width = 0.89)\n  \n  set.seed(1433)\n  sampl &lt;- post |&gt;\n    select(.draw,\n           prior_sdgp_a_gplat_poslon2_pos,\n           sdgp_a_gplat_poslon2_pos,\n           prior_lscale_a__1_gplat_poslon2_pos,\n           lscale_a_gplat_poslon2_pos) |&gt;\n    mutate(prior_eta2 = rethink_eta2(prior_sdgp_a_gplat_poslon2_pos),\n           eta2 = rethink_eta2(sdgp_a_gplat_poslon2_pos),\n           prior_rho2 = rethink_rho2(prior_lscale_a__1_gplat_poslon2_pos ),\n           rho2 = rethink_rho2(lscale_a_gplat_poslon2_pos)) |&gt;\n    slice_sample(n = 50) |&gt;\n    expand_grid(dist = seq(from = 0, to = 10, by = 0.05)) |&gt;\n    mutate(prior_cov = brms_cov(x = dist,\n                                sdgp = prior_sdgp_a_gplat_poslon2_pos,\n                                lscale = prior_lscale_a__1_gplat_poslon2_pos),\n           cov = brms_cov(x = dist,\n                          sdgp = sdgp_a_gplat_poslon2_pos,\n                          lscale = lscale_a_gplat_poslon2_pos))\n})\n# glimpse(post14_08$stats)\n# glimpse(post14_08$sampl)\n\n\nplot14_08 &lt;- list()\nplot14_08 &lt;- within(plot14_08, {\n  prior &lt;- post14_08$sampl |&gt; ggplot(aes(x = dist, y = prior_cov)) +\n    geom_line(aes(group = .draw, color = .draw)) +\n    stat_function(\n      fun = function(x) {\n        post14_08$brms_cov(x = median(post14_08$sampl$dist),\n                           sdgp = post14_08$stats$prior_sdgp_a_gplat_poslon2_pos,\n                          lscale = post14_08$stats$prior_lscale_a__1_gplat_poslon2_pos)},\n      color = \"firebrick\", linewidth = 1) +\n    scale_x_continuous(breaks = scales::breaks_width(width = 2),\n                       labels = scales::label_number_auto()) +\n    scale_color_paletteer_c(\"pals::ocean.speed\") +\n    coord_cartesian(ylim = c(0, 2)) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Gaussian process prior\",\n         subtitle = \"Red line is the median of the covariances\",\n         x = \"distance in thousands of km\",\n         y = \"covariance\")\n  post &lt;- post14_08$sampl |&gt; ggplot(aes(x = dist, y = cov)) +\n    geom_line(aes(group = .draw, color = .draw)) +\n    stat_function(\n      fun = function(x) {\n        post14_08$brms_cov(x = median(post14_08$sampl$dist),\n                           sdgp = mean(post14_08$stats$sdgp_a_gplat_poslon2_pos),\n                          lscale = mean(post14_08$stats$lscale_a_gplat_poslon2_pos))},\n      color = \"firebrick\", linewidth = 1) +\n    scale_x_continuous(breaks = scales::breaks_width(width = 2),\n                       labels = scales::label_number_auto()) +\n    scale_color_paletteer_c(\"pals::ocean.speed\") +\n    coord_cartesian(ylim = c(0, 2)) +\n    theme(legend.position = \"none\") +\n    labs(title = \"Gaussian process posterior\",\n         subtitle = \"Red line is the median of the covariances\",\n         x = \"distance in thousands of km\",\n         y = \"covariance\")\n})\nwrap_plots(plot14_08[c(\"prior\", \"post\")]) +\n  plot_annotation(title = \"Oceanic Gaussian Process\") &\n  theme(title = element_text(color = \"darkblue\"))\n\n\n\n\nFigure 14.11\n\n\n\n\nand to get the matrix of median covariance we simply map the calculation of covariance using the median values of the parameters\n\npost14_08 &lt;- within(post14_08, {\n  # the covariance matrix using the median of the parameters\n  # i.e. the median covariance\n  # The covariance is calculated based on the distance\n  cov &lt;- post14_08$brms_cov(\n    x = dataKline$dist,\n    sdgp = post14_08$stats$sdgp_a_gplat_poslon2_pos,\n    lscale = post14_08$stats$lscale_a_gplat_poslon2_pos)\n  # the correlation matrix\n  cor &lt;- cov2cor(cov) |&gt;\n    round(digits = 2)\n})\n# post14_08$cor\n\nwhich we can illustrate with a heatmap\n\nheatmaply::heatmaply_cor(\n  x = post14_08$cor,\n  cellnote = post14_08$cor,\n  colors = unclass(paletteer::paletteer_c(\"pals::ocean.tempo\", n = 16)),\n  hide_colorbar = TRUE,\n  main = \"Correlations between Oceanic Societies in Thousands of km\")\n\n\n\n\n\nand we add the correlations to our edges to be able to plot them\n\ndataKline &lt;- within(dataKline, {\n  # create the correlation dataframe\n  cor_df &lt;- post14_08$cor\n  # set lower triangle and diagonal to 0 to avoid double values.\n  # zeros can be removed later\n  cor_df[lower.tri(cor_df, diag = FALSE)] &lt;- NA_real_\n  cor_df &lt;- cor_df |&gt;\n    as.data.frame() |&gt;\n    tibble::rownames_to_column(var = \"x\") |&gt;\n    pivot_longer(cols = -x, names_to = \"y\", values_to = \"cor\")\n  \n  # make sure you don't miss a number\n  check &lt;- sum(post14_08$cor[lower.tri(post14_08$cor, diag = TRUE)])\n  stopifnot(sum(cor_df$cor, na.rm = TRUE) - check == 0)\n  \n  # add the correlation to the edges\n  edges_df &lt;- edges_df |&gt;\n    inner_join(y = cor_df, by = c(\"x\" = \"x\", \"y\" = \"y\"))\n  stopifnot(sum(edges_df$cor, na.rm = TRUE) - check == 0)\n  \n})\n\n\ngraph14_08 &lt;- within(graph14_08, {\n  \n  # the edges' colors\n  colrs &lt;- paletteer::paletteer_c(\"oompaBase::bluescale\", n = 16)\n  \n  # create the graph object\n  # the manual layout requires a special treatment when using ggraph\n  # source for manual layout:\n  # https://stackoverflow.com/questions/67756538/plot-ggraph-using-supplied-node-coordinates\n  # must add the x and y coords to the nodes for manual layout to work\n  the_nodes &lt;- dataKline$nodes_df |&gt;\n    select(culture, x = lon2_pos, y = lat_pos, logpop)\n  the_edges &lt;- dataKline$edges_df |&gt;\n    filter(between(cor, 0.01, 1))\n  grf &lt;- tbl_graph(nodes = the_nodes, edges = the_edges, directed = FALSE)\n  \n  # the basic graph\n  # when we use the manual layout, when the x and y have different names to avoid\n  # conflicts. See the source on manual layout mentioned above\n  p1 &lt;- ggraph(grf, layout = data.frame(lx = NA, ly = NA)) +\n    geom_node_point(aes(size = logpop), color = \"purple\") +\n    geom_edge_link(aes(color = cor, width = cor)) +\n    geom_node_text(aes(label = culture), color = \"darkblue\", size = 3, repel = TRUE) +\n    scale_size_continuous(range = c(1, 4)) +\n    scale_edge_width(range = c(0.25, 2)) +\n    scale_edge_color_gradientn(colors = colrs) +\n    theme(legend.position = \"none\") +\n  labs(title = \"Relative positions of societies\",\n       subtitle = \"edges = correlation\",\n       x = \"longitude in thousands of km\", y = \"latitude in thousands of km\")\n  \n  # and we only need to change the nodes for the graph with tools and population\n  the_nodes &lt;- dataKline$nodes_df |&gt;\n    select(culture, x = logpop, y = total_tools)\n  the_edges &lt;- dataKline$edges_df |&gt;\n    filter(between(cor, 0.01, 1))\n  grf &lt;- tbl_graph(nodes = the_nodes, edges = the_edges, directed = FALSE)\n  \n  p2 &lt;- ggraph(grf, layout = data.frame(lx = NA, ly = NA)) +\n    geom_node_point(aes(size = x), color = \"purple\") +\n    geom_edge_link(aes(color = cor, width = cor)) +\n    geom_node_text(aes(label = culture), color = \"darkblue\", size = 3, repel = TRUE) +\n    scale_size_continuous(range = c(1, 4)) +\n    scale_edge_width(range = c(0.25, 2)) +\n    scale_edge_color_gradientn(colors = colrs) +\n    theme(legend.position = \"none\") +\n  labs(title = \"Relations between population and tools\",\n       subtitle = \"edges = correlation\",\n       x = \"log of population\", y = \"total tools\")\n})\nwrap_plots(graph14_08[c(\"p1\", \"p2\")]) +\n  plot_annotation(title = \"Oceanic Gaussian Process\") &\n  theme(title = element_text(color = \"darkblue\"))\n\n\n\n\nFigure 14.12\n\n\n\n\n\n\n\n14.5.2 Phylogenic distance\n\ndagPhylo &lt;- list()\ndagPhylo &lt;- within(dagPhylo, {\n  # ggdag has no built-in method to faciliate using subscripts.\n  # This is one way to do it!\n  \n  # IMPORTANT the nodes labels must be empty\n  the_nodes &lt;- c(\"B1\" = \"\", \n                 \"B2\" = \"\", \n                 \"G1\" = \"\", \n                 \"G2\" = \"\",\n                 \"U1\" = \"\",\n                 \"U2\" = \"\")\n  \n  # IMPORTANT: The text labels must follow the order of the nodes\n  text_labels &lt;- c(expression(B[1]), expression(B[2]),\n                   expression(G[1]), expression(G[2]),\n                   expression(U[1]), expression(U[2]))\n  \n  dag1 &lt;- ggdag::dagify(G2 ~ G1 + U1, B2 ~ G1 + B1 + U1, U2 ~ U1,\n                        outcome = \"B2\",\n                        exposure = c(\"B1\", \"G1\"),\n                        latent = c(\"U1\", \"U2\"),\n                       labels = the_nodes)\n  \n  p1 &lt;-  dag1 |&gt;\n    ggdag::ggdag_status(layout = \"sugiyama\", node = TRUE) +\n    geom_dag_point(aes(color = status)) +\n    geom_dag_text(color = \"white\") +\n    scale_color_paletteer_d(\"ggsci::nrc_npg\", na.value = \"grey\") +\n    ggdag::theme_dag_blank(\n      panel.background = element_rect(fill = \"snow\", color = \"snow\"))\n})\ndagPhylo$p1\n\n\n\n\n\n\n\n\n\ndagPhylo &lt;- within(dagPhylo, {\n  dag2 &lt;- ggdag::dagify(\n    B ~ G + M + U,\n    G ~ M + U,\n    M ~ U,\n    U ~ P,\n    outcome = \"B\",\n    latent = c(\"P\", \"U\"),\n    exposure = c(\"G\", \"M\")\n  )\n  p2 &lt;- dag2 |&gt;\n    ggdag::ggdag_status(layout = \"sugiyama\") +\n        geom_dag_point(aes(color = status)) +\n    geom_dag_text(color = \"white\") +\n    scale_color_paletteer_d(\"ggsci::nrc_npg\", na.value = \"grey\") +\n    ggdag::theme_dag_blank(\n      panel.background = element_rect(fill = \"snow\", color = \"snow\"))\n})\ndagPhylo$p2\n\n\n\n\n\n\n\n\n\n14.5.2.1 Data\nThe data is as follows\n\ndata(Primates301, package = \"rethinking\") \ndata(Primates301_nex)\nstr(Primates301_nex)\n\nList of 4\n $ edge       : int [1:594, 1:2] 302 303 304 305 306 307 308 308 309 310 ...\n $ edge.length: num [1:594] 4.16 22.03 16.81 8.59 6.54 ...\n $ Nnode      : int 294\n $ tip.label  : chr [1:301] \"Allenopithecus_nigroviridis\" \"Cercopithecus_albogularis\" \"Cercopithecus_ascanius\" \"Cercopithecus_campbelli\" ...\n - attr(*, \"class\")= chr \"phylo\"\n - attr(*, \"order\")= chr \"cladewise\"\n\n\nWe standardize the data\n\ndataPrimates &lt;- Primates301 |&gt;\n  mutate(name = as.character(name)) |&gt; \n  drop_na(group_size, body, brain) |&gt; \n  mutate(M = as.vector(scale(log(body))),\n         B = as.vector(scale(log(brain))),\n         G = as.vector(scale(log(group_size))))\n# glimpse(dataPrimates)\n\nand to plot is we use the ggtree package. Details on ggtree can be found at ggtree. It is not available on CRAN at the time of writing this. You can find it in github and install it with devtools::install_github(\"GuangchuangYu/ggtree\").\n\n# devtools::install_github(\"GuangchuangYu/ggtree\")\nggtree::ggtree(tr = Primates301_nex, layout = \"circular\", color = \"firebrick\") +\n  geom_tiplab(size = 5/3, color = \"darkgrey\")\n\n\n\n\n\n\n14.5.2.2 Model\n\\[\n\\begin{align*}\n\\bf{B} &\\sim \\mathcal{MVNormal}(\\mu, \\bf{S})\\\\\n\\mu_i &= \\alpha + \\beta_G G_i + \\beta_M M_i \\\\\n\\bf{S} &= \\sigma^2 \\bf{I}\n\\end{align*}\n\\]\nWe begin by fitting a naive model that does not take into account the phylogenetic covariances.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nfit14_09 &lt;- xfun::cache_rds({\n  brm(\n    data = dataPrimates,\n    family = gaussian,\n    formula =  B ~ M + G,\n    prior = c(\n      prior(normal(0, 1), class = Intercept),\n      prior(normal(0, 0.5), class = b),\n      prior(exponential(1), class = sigma)),\n    iter = 1000, warmup = 500, chains = 2,\n    cores = detectCores(), seed = 1439)},\n  file = \"ch14_fit14_09\", rerun = FALSE)\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.12 sec elapsed\n\n\n\nfit14_09\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: B ~ M + G \n   Data: dataPrimates (Number of observations: 151) \n  Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 1000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.00      0.02    -0.03     0.03 1.01      933      540\nM             0.89      0.02     0.85     0.93 1.00      864      760\nG             0.12      0.02     0.08     0.16 1.00      825      798\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.22      0.01     0.19     0.25 1.00      923      599\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe use the ape package to compute the implied covariance matrix and distance matrix\n\ntreePhylo &lt;- list()\ntreePhylo &lt;- within(treePhylo, {\n  spp_obs &lt;- dataPrimates$name\n  \n  tree_trimmed &lt;- ape::keep.tip(Primates301_nex, spp_obs)\n  Rbm &lt;- ape::corBrownian(phy = tree_trimmed)\n  V &lt;- ape::vcv(Rbm)\n  Dmat &lt;- stats::cophenetic(tree_trimmed)\n})\n\nWarning in Initialize.corPhyl(phy, dummy.df): No covariate specified, species\nwill be taken as ordered in the data frame. To avoid this message, specify a\ncovariate containing the species names with the 'form' argument."
  },
  {
    "objectID": "ch14_covariances.html#summary",
    "href": "ch14_covariances.html#summary",
    "title": "14  Adventures in Covariance",
    "section": "14.6 Summary",
    "text": "14.6 Summary"
  },
  {
    "objectID": "ch14_covariances.html#practice",
    "href": "ch14_covariances.html#practice",
    "title": "14  Adventures in Covariance",
    "section": "14.7 Practice",
    "text": "14.7 Practice\n\n\n\n\nKurz, Solomon. 2020. Statistical Rethinking with Brms. 2nd ed. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://www.taylorandfrancis.com."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Kurz, Solomon. 2020. Statistical Rethinking with Brms. 2nd ed.\nhttps://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in R and Stan. 2nd ed. Boca Raton,\nFlorida: Chapman; Hall/CRC. http://www.taylorandfrancis.com."
  }
]