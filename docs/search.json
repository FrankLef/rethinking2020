[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rethinking2020",
    "section": "",
    "text": "This book is a study project of Statistical Rethinking, second edition, by Richard McElreath (McElreath 2020).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo use the rethinking you must install it as advised by Richard McElreath as described in rethinking. You need to pay attention when it is done, but it works fine. Make sure you verify the installation with example(stan_model, package = \"rstan\", run.dontrun = TRUE) as described in the guides.\n\n\n\n\n\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 22621)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/New_York\n date     2022-10-13\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.3.0   2022-04-25 [1] CRAN (R 4.2.1)\n digest        0.6.29  2021-12-01 [1] CRAN (R 4.2.1)\n evaluate      0.16    2022-08-09 [1] CRAN (R 4.2.1)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.2.1)\n htmltools     0.5.3   2022-07-18 [1] CRAN (R 4.2.1)\n htmlwidgets   1.5.4   2021-09-08 [1] CRAN (R 4.2.1)\n jsonlite      1.8.0   2022-02-22 [1] CRAN (R 4.2.1)\n knitr         1.39    2022-04-26 [1] CRAN (R 4.2.1)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.1)\n rlang         1.0.4   2022-07-12 [1] CRAN (R 4.2.1)\n rmarkdown     2.15    2022-08-16 [1] CRAN (R 4.2.1)\n rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.2.1)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.1)\n stringi       1.7.8   2022-07-11 [1] CRAN (R 4.2.1)\n stringr       1.4.0   2019-02-10 [1] CRAN (R 4.2.1)\n xfun          0.32    2022-08-10 [1] CRAN (R 4.2.1)\n yaml          2.3.5   2022-02-21 [1] CRAN (R 4.2.0)\n\n [1] C:/Users/Ephel/AppData/Local/R/win-library/4.2\n [2] C:/Program Files/R/R-4.2.1/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://www.taylorandfrancis.com."
  },
  {
    "objectID": "ch01_golem.html",
    "href": "ch01_golem.html",
    "title": "1  The Golem of Prague",
    "section": "",
    "text": "This chapter is for reading and very interesting. It involves no actual coding, just a critical and open mind."
  },
  {
    "objectID": "ch02_worlds.html",
    "href": "ch02_worlds.html",
    "title": "2  Small Worlds and Large Worlds",
    "section": "",
    "text": "The foundations of Bayesian statistics."
  },
  {
    "objectID": "ch02_worlds.html#garden-of-forking-data",
    "href": "ch02_worlds.html#garden-of-forking-data",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.1 Garden of forking data",
    "text": "2.1 Garden of forking data\nThere are 2 events: Blue marble represented by B and white marble represented by W\n\nevents <- c(\"B\", \"W\")\n\nThe bag has 4 marbles of 2 colors, blue and white each possibilities, 0 to 4 blue marbles, is a conjecture (sample space).\n\nconjectures <- lapply(0:4, \n                      function(i) c(rep(events[1], times = i), \n                                    rep(events[2], times = 4 - i))\n                      )\nconjectures <- as.data.frame(do.call(rbind, conjectures))\nconjectures\n\n  V1 V2 V3 V4\n1  W  W  W  W\n2  B  W  W  W\n3  B  B  W  W\n4  B  B  B  W\n5  B  B  B  B\n\n\nwe draw 3 marbles from the bag which is the data (event)\n\ndata <- c(\"B\", \"W\", \"B\")\ndata\n\n[1] \"B\" \"W\" \"B\"\n\n\nthe number of ways each conjecures could have generated the data is\n\nways <- apply(X = conjectures, MARGIN = 1, \n              function(x) {\n                  sum(x == data[1]) * sum(x == data[2]) * sum(x == data[3])\n                  })\nways\n\n[1] 0 3 8 9 0\n\n\nSo the number of ways depends on the nb of blue marbles in the bag. Therefore we will assign different plausabiltity to the conjectures depending on the proportion of blue balls. This is an assumtpions on the \\(p\\), other values can be warranted.\n\nprior <- apply(X = conjectures, MARGIN = 1, function(x) sum(x == events[1]) / 4)\nprior\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\nconjectures <- cbind(conjectures, prior, ways)\nconjectures\n\n  V1 V2 V3 V4 prior ways\n1  W  W  W  W  0.00    0\n2  B  W  W  W  0.25    3\n3  B  B  W  W  0.50    8\n4  B  B  B  W  0.75    9\n5  B  B  B  B  1.00    0\n\n\nAnd the plausability of the prior after getting the new data is the data x the prior divided by the sum of all possible event\n\n# the multiplication of likelyhood and prior\nconjectures$post <- conjectures$ways * conjectures$prior\n# the division by the sum to convert to pct summing up to 1\nconjectures$post <- conjectures$post / sum(conjectures$post)  \nconjectures\n\n  V1 V2 V3 V4 prior ways       post\n1  W  W  W  W  0.00    0 0.00000000\n2  B  W  W  W  0.25    3 0.06521739\n3  B  B  W  W  0.50    8 0.34782609\n4  B  B  B  W  0.75    9 0.58695652\n5  B  B  B  B  1.00    0 0.00000000\n\n# verify the total of posterior is 1\nstopifnot(sum(conjectures$post) == 1)\n\n\n\n\n\n\n\nNote\n\n\n\nIf you run the above procedure, wou will find that the posterior is not always the same, it varies quite a bit. That’s why we run lots of sample and investigate their distribution to decide what is the right value for the parameter."
  },
  {
    "objectID": "ch02_worlds.html#building-a-model",
    "href": "ch02_worlds.html#building-a-model",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.2 Building a model",
    "text": "2.2 Building a model\nThe events are that the location on earth is earth represented by \\(L\\) or that the location on earth is water represented by \\(W\\).\n\nevents <- c(\"L\", \"W\")\n\nThe conjecture is the amount of water on earth represented by the parameter p which we try to estimate. This time is a continuous value, we will use a grid to approximate the entire set of possibles conjectures\n\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\nAnd , as priors, we assume that every possibilities of p is uniformely distributed.\n\nprior <- rep(1, times = length(p_grid))\nprior\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nCompute likelihood for each value in the grid\n\nlikelihood <- dbinom(x = 6, size = 9, prob = p_grid)\n\nCompute product of likelihood and prior\n\nunstd.posterior <- likelihood * prior\n\nStandardize the posterior so it sum to 1\n\nposterior <- unstd.posterior / sum(unstd.posterior)"
  },
  {
    "objectID": "ch02_worlds.html#components-of-the-model",
    "href": "ch02_worlds.html#components-of-the-model",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.3 Components of the model",
    "text": "2.3 Components of the model\n\n2.3.1 Likelihood\n\n\n2.3.2 Parameters\n\n\n2.3.3 Priors"
  },
  {
    "objectID": "ch02_worlds.html#making-the-model-go",
    "href": "ch02_worlds.html#making-the-model-go",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.4 Making the model go",
    "text": "2.4 Making the model go\n\n2.4.1 Grid approximation\n\n\n2.4.2 Quadratic approximation\n\nglobe.qa <- rethinking::map(\n    alist(\n        w ~ dbinom(9, p),  # binomial likelihood\n        p ~ dunif(0, 1)  # uniform priors\n    ),\n    data = list(w = 6)\n    )\n\nrethinking::precis(globe.qa)\n\n       mean        sd      5.5%     94.5%\np 0.6666667 0.1571338 0.4155366 0.9177968"
  },
  {
    "objectID": "ch02_worlds.html#summary",
    "href": "ch02_worlds.html#summary",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.5 Summary",
    "text": "2.5 Summary\nGrid approximation can only be used with the simplest problems although if you understand it, you understand the fundamental method behind Bayesian stats.\nQuadratic approximation is a nice trick to remember as, when it can be used, it is very efficient. However I could not find a R package that actually does it.\nMCMC is, seemingly, the most used method in practice. It is the method used by the brms package"
  },
  {
    "objectID": "ch03_sampling.html",
    "href": "ch03_sampling.html",
    "title": "3  Sampling the Imaginary",
    "section": "",
    "text": "We use the example from chapter 2.\nThe grid of \\(p\\) values has a grid size of \\(grid\\_size\\).\nThe prior is uniformly distributed and so, as discussed in Overthinking box of section 2.3.3, p. 35, \\(P(p)=\\frac{1}{1-0}=1\\).\n\nthe_grid <- data.frame(\n prob = seq(from = 0, to = 1, length.out = 1000),\n prior = 1)\n\nand we calculate the posterior using the data. We compute the likelihood using the grid of priors from above, then compute the average likelihood which is the sum of the likelihood.\nThe posterior is defined in detailed in section 2.3.4, p. 37.\n\\[\n\\text{Posterior} =\n\\frac{\\text{Likelihood} \\times \\text{Prior}}\n  {\\text{Average Likelihood}}\n\\]\nNote the code stopifnot(sum(d$posterior) == 1), it is always a good idea to verify this . . . you could be surprised how often you miss the mark.\n\n# the data, see page 28\ndata <- c(\"W\",\"L\",\"W\",\"W\",\"W\",\"L\",\"W\",\"L\",\"W\")\nn_success <- sum(data == \"W\")\nn_trials <- length(data)\n\n# compute the likelihood each value in the grid\nthe_grid <- the_grid |>\n    mutate(\n        likelihood = dbinom(x = n_success, size = n_trials, prob = prob),\n        posterior = likelihood * prior / sum(likelihood))\nassert_that(sum(the_grid$posterior) == 1,\n            msg = \"The total posterior prob. must equal 1.\")\n\n[1] TRUE\n\n\nwhich gives the estimated posterior probability \\(p\\) conditional on the data for each point of a grid.\nGenerate and visualize n_samples samples from the grid with the \\(p\\) values with their respective posterior probability \\(p\\) computed above.\n\nNote: We use dplyr::slice_sample because dplyr::sample_n is deprecated.\n\n\nset.seed(1223)\nthe_samples <- the_grid |>\n  slice_sample(n = 1e4, weight_by = posterior, replace = TRUE) |>\n  # this distance from the mean is used for coloring\n  mutate(dist = abs(prob - mean(prob)))\nthe_samples$id <- seq_len(nrow(the_samples))\n# str(the_samples)\n\nvisualize the sample of water proportion\n\nggplot(data = the_samples, mapping = aes(x = id, y = prob, color = dist)) +\n  geom_point(size = 0.75, alpha = 0.9) +\n  scale_color_gradientn(colors = paletteer_d(palette=\"Manu::Kotare\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = sprintf(\"%d samples\", nrow(the_samples)))\n\n\n\n\nvisualize the density\n\n# show the p density\nggplot(data = the_samples, aes(x = prob)) +\n    geom_density(aes(y=..scaled..), color = \"blue\", size = 1, fill = \"lightblue\") +\n    theme_light() +\n    labs(title = sprintf(\"%d samples\", nrow(the_samples)))"
  },
  {
    "objectID": "ch03_sampling.html#sampling-to-summarize",
    "href": "ch03_sampling.html#sampling-to-summarize",
    "title": "3  Sampling the Imaginary",
    "section": "3.2 Sampling to summarize",
    "text": "3.2 Sampling to summarize\n\n3.2.1 Intervals of defined boundaries\n\nthe_grid |>\n  filter(prob < 0.5) |>\n  summarize(sum = sum(posterior))\n\n        sum\n1 0.1718746\n\n\nand you can obtain the same result using the sampling data by counting the rows\n\nthe_samples |>\n  filter(prob < 0.5) |>\n  count() |>\n  mutate(pct = n / nrow(the_samples)) |>\n  identity()\n\n     n    pct\n1 1744 0.1744\n\n\n\n\n3.2.2 Intervals of defined mass\nBeside the base R quantile function, the mean_qi function from the package ggdist will be used extensively in this project. The benefits of using this package in conjonction with posterior, tidybayes etc. will become obvious in later chapters.\n\nthe_samples |>\n  ggdist::mean_qi(prob, .width = 0.8)\n\n# A tibble: 1 × 6\n   prob .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 0.636  0.444  0.813    0.8 mean   qi       \n\n\nAnd if we redo the sampling with observing 3 \\(W\\) in 3 tosses we have the grid\n\nthe_grid <- data.frame(\n  prob = seq(from = 0, to = 1, length.out = 1000),\n  prior = 1) |>\n  mutate(\n    likelihood = dbinom(x = 3, size = 3, prob = prob),\n    posterior = likelihood * prior / sum(likelihood))\nassert_that(sum(the_grid$posterior) == 1,\n            msg = \"The total posterior prob. must equal 1.\")\n\n[1] TRUE\n\n\nand we use it to resample\n\nset.seed(1223)\nthe_samples <- the_grid |>\n  slice_sample(n = 1e4, weight_by = posterior, replace = TRUE) |>\n  # this distance from the mean is used for coloring\n  mutate(dist = abs(prob - mean(prob)))\nthe_samples$id <- seq_len(nrow(the_samples))\n\n\nthe_samples |>\n  ggdist::mean_qi(prob, .width = 0.5)\n\n# A tibble: 1 × 6\n   prob .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 0.800  0.708  0.931    0.5 mean   qi       \n\n\n\nthe_samples |>\n  ggdist::mean_hdi(prob, .width = 0.5)\n\n# A tibble: 1 × 6\n   prob .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 0.800  0.842      1    0.5 mean   hdi      \n\n\nand we can illustrate the intervals with ggdist as follows\n\nqtl <- c(0.5, 0.8, 0.95, 1)\nx_breaks <- ggdist::mean_qi(.data = the_samples$prob, \n                            .width = qtl) |>\n  select(y, ymin, ymax) |>\n  pivot_longer(cols = c(\"y\", \"ymin\", \"ymax\")) |>\n  distinct(value) |>\n  arrange(value) |>\n  round(digits = 2) |>\n  pull()\nggplot(the_samples, aes(x=prob)) +\n         stat_halfeye(aes(fill=stat(cut_cdf_qi(\n           cdf,\n           .width = qtl,\n           labels = scales::percent_format()\n           )))) +\n  scale_x_continuous(breaks = x_breaks) +\n  scale_fill_paletteer_d(palette = \"Manu::Takahe\", direction = -1,\n                         na.translate = FALSE) +\n  theme_ggdist() +\n  theme(legend.position = c(0.1, 0.75)) +\n  labs(title = \"Intervals of defined mass\",\n       x = \"p_grid\", y = \"prob of p_grid\",fill = \"quantiles\")\n\n\n\n\n\n\n3.2.3 Point estimates (loss function)\nThe linex loss function can be very useful in business analysis. This is to be investigated later."
  },
  {
    "objectID": "ch03_sampling.html#sampling-to-simulate-prediction",
    "href": "ch03_sampling.html#sampling-to-simulate-prediction",
    "title": "3  Sampling the Imaginary",
    "section": "3.3 Sampling to simulate prediction",
    "text": "3.3 Sampling to simulate prediction"
  },
  {
    "objectID": "ch03_sampling.html#summary",
    "href": "ch03_sampling.html#summary",
    "title": "3  Sampling the Imaginary",
    "section": "3.4 Summary",
    "text": "3.4 Summary"
  },
  {
    "objectID": "ch04_linear.html",
    "href": "ch04_linear.html",
    "title": "4  Linear Models",
    "section": "",
    "text": "Gaussian distribution\n\\[\n\\begin{equation}\nP \\left(y \\mid \\mu, \\sigma \\right) =\n\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp{\\left[-\\frac{1}{2}\n\\left(\\frac{y-\\mu}{\\sigma} \\right)^2\n\\right]}\n\\end{equation}\n\\]\ngaussian distribution expressed with \\(precision = \\tau\\) is \\(\\sigma = \\frac{1}{\\sqrt{\\tau}}\\)\n\\[\n\\begin{equation}\nP \\left(y \\mid \\mu, \\tau \\right) =\n\\frac{\\tau}{\\sqrt{2 \\pi}} \\exp{\\left[-\\frac{\\tau}{2}\n\\left(y-\\mu \\right)^2\n\\right]}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "ch04_linear.html#a-language-for-describing-model",
    "href": "ch04_linear.html#a-language-for-describing-model",
    "title": "4  Linear Models",
    "section": "4.2 A language for describing model",
    "text": "4.2 A language for describing model\n\\[\n\\begin{align*}\noutcome_i &\\sim \\mathcal{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta \\times predictor_i \\\\\n\\beta &\\sim \\mathcal{Normal}(0, 10) \\\\\n\\sigma &\\sim \\mathcal{HalfCauchy}(0, 1)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch04_linear.html#a-gaussian-model-of-height",
    "href": "ch04_linear.html#a-gaussian-model-of-height",
    "title": "4  Linear Models",
    "section": "4.3 A Gaussian model of height",
    "text": "4.3 A Gaussian model of height\n\n4.3.1 The data\n\ndata(\"Howell1\")\nd <- Howell1\nrm(Howell1)\n\nwhich we can visualize using skimr\n\nskimr::skim(d) |>\n  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))\n\n\nData summary\n\n\nName\nd\n\n\nNumber of rows\n544\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n0\n1\n138.26\n27.60\n53.98\n125.10\n148.59\n157.48\n179.07\n▁▂▂▇▇\n\n\nweight\n0\n1\n35.61\n14.72\n4.25\n22.01\n40.06\n47.21\n62.99\n▃▂▃▇▂\n\n\nage\n0\n1\n29.34\n20.75\n0.00\n12.00\n27.00\n43.00\n88.00\n▇▆▅▂▁\n\n\nmale\n0\n1\n0.47\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\n\n\n\nselect only the adults\n\nd2 <- d[d$age >= 18, ]\n\n\n\n4.3.2 The model\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu, \\sigma)\\\\\n\\mu &\\sim \\mathcal{N}(178, 20) \\\\\n\\sigma &\\sim \\mathcal{Uniform}(0, 50)\n\\end{align*}\n\\]\nWe do the prior predictive simulation with the prior \\(\\mu \\sim \\mathcal{N}(178, 20)\\)\n\nprior1 <- data.frame(id=seq_len(1e4)) %>%\n  mutate(mu=rnorm(n=nrow(.), mean=178, sd=20),\n         sigma=runif(n=nrow(.), min=0, max=50),\n         height = rnorm(n=nrow(.), mean=mu, sd=sigma))\n\nand we do the prior predictive simulation with the prior \\(\\mu \\sim \\mathcal{N}(178, 100)\\)\n\nprior2 <- data.frame(id=seq_len(1e4)) %>%\n  mutate(mu=rnorm(n=nrow(.), mean=178, sd=100),\n         sigma=runif(n=nrow(.), min=0, max=50),\n         height = rnorm(n=nrow(.), mean=mu, sd=sigma))\n\nand we visualize using ggplot\n\np <- list()\n# NOTE: we use {} with |> to use nrow in  sprintf\np$prior1 <- prior1 %>%\n  {\n    ggplot(data=., aes(x=height)) +\n      geom_density(color = \"slateblue1\", size = 1) +\n      theme_classic() +\n      theme(legend.position = c(0.1, 0.8)) +\n      labs(\n        title = expression(paste(\"h ~ dnorm(\", mu, \",\", sigma ,\")\")),\n        subtitle = sprintf(\"sample size = %d\", nrow(.)),\n        fill = \"quantile\"\n  )\n  }\n# p$prior1\n\n\np$prior2 <- prior2 %>%\n  {\n    ggplot(data=., aes(x = height)) +\n      geom_density(color = \"peru\", size = 1) +\n      geom_vline(xintercept = 0, linetype = \"dotted\", color = \"navy\") +\n      theme_classic() +\n      theme(legend.position = c(0.1, 0.8)) +\n      labs(\n        title = expression(paste(\"h ~ dnorm(\", mu, \",\", sigma ,\")\")),\n        subtitle = sprintf(\"sample size = %d\", nrow(.)),\n        fill = \"quantile\")\n  }\n# p$prior2\n\nand we generate the 4 plots using ggdist::stat_dist_interval() used for analytical distribution\n\np$normal <- data.frame(mean=178, sd=20) %>%\n  {\n    ggplot(data=.) +\n      geom_function(fun=dnorm, args=list(mean=.$mean, sd=.$sd),\n                    color=\"olivedrab4\", size=1) +\n      scale_x_continuous(limits = c(.$mean - 3 * .$sd, .$mean  + 3 * .$sd),\n                     breaks = scales::breaks_width(width = 25)) +\n  theme_classic() +\n  labs(title = bquote(mu ~ .(sprintf(\"~ dnorm(%.0f, %.0f)\", .$mean, .$sd))), \n       x = expression(mu), y = \"density\") \n  }\n# p$normal\n\n\np$uniform <- data.frame(min=0, max=50) %>%\n  {\n    ggplot(data=.) +\n      geom_function(fun=dunif, args=list(min=.$min, max=.$max),\n                    color=\"rosybrown2\", size=1) +\n      scale_x_continuous(limits = c(.$min - 2, .$max  + 2),\n                         breaks = scales::breaks_width(width=10)) +\n      theme_classic() +\n      labs(title = bquote(mu ~ .(sprintf(\"~ dunif(%.0f, %.0f)\", .$min, .$max))), \n           x = expression(sigma), y = \"density\")\n  }\n# p$uniform\n\n\npatchwork::wrap_plots(p)\n\n\n\n\n\n\n4.3.3 Grid approximation of posterior distribution\nFirst create the grid. The name the_grid is used here instead of post, as in the textbook, to emphasize that these are posterior of the grid. They are not the actual posteriors which will be calculated next using a sampling.\n\n# create grid of mu and sigma\npost <- list(n = 200L)\npost$grid <-with(post, {\n  data.frame(\n  mu = seq(from = 140, to = 160, length.out = n),\n  sigma = seq(from = 4, to = 9, length.out = n)) |>\n  expand(mu, sigma)\n})\n\nThen we calculate the likelihood. Since probabilities are percentage this causes a numerical issue as multiple multiplications of percentages will create very small numbers, so small in fact that they will be miscalculated.\nTo resolve this problem, we use logarithms.\nThat is the likelihood function from the model defined in 4.3.2\n\\[\nP(\\mu, \\sigma \\mid h) =\n\\prod_{i=1}^n \\mathcal{N}(y_i \\mid \\mu, \\sigma) \\cdot\n\\mathcal{N}(\\mu \\mid mean = 0, sd = 10) \\cdot\n\\mathcal{U}(\\sigma | min = 0, max = 10)\n\\]\nis transformed to log.\n\nImportant: Read the end note # 73 on page 449. All the explanations, including the usage of max(post$prob) is explained.\n\n\\[\n\\log{P(\\mu, \\sigma \\mid h)} =\n\\sum_{i=1}^n \\left[ \\log{\\mathcal{N}(y_i \\mid \\mu, \\sigma)} +\n\\log{\\mathcal{N}(\\mu \\mid mean = 0, sd = 10)} +\n\\log{\\mathcal{U}(\\sigma | min = 0, max = 10)} \\right]\n\\] and to compute the posterior distribution we compute the likelihood which is the first element of the addition\n\\[\n\\sum_{i=1}^n \\log{\\mathcal{N}(y_i \\mid \\mu, \\sigma)}\n\\] as follows\n\n# The likelihood on the log scale\npost$grid <- post$grid %>%\n  mutate(LL=sapply(seq_len(nrow(.)), \n                   function(i) sum(dnorm(d2$height, \n                                         mean=post$grid$mu[i], \n                                         sd=post$grid$sigma[i], \n                                         log=TRUE))))\n\nthen the remaining 2 elements of the summation are the priors\n\\[\n\\sum_{i=1}^n \\left[\n\\log{\\mathcal{N}(\\mu \\mid mean = 0, sd = 10)} +\n\\log{\\mathcal{U}(\\sigma | min = 0, max = 10)}\n\\right]\n\\] which we add to the likelihood to obtain the posterior distribution on the log scale\n\n# add the the priors to the likelihood  on the log scales to obtain the\n# log of the posterior\npost$grid <- post$grid |>\n  mutate(prob =\n           LL + \n           dnorm(x=mu, mean=178, sd=20, log=TRUE) +\n           dunif(x=sigma, min=0, max=50, log=TRUE))\n\nand to convert the posterior back to the natural scale we exponentiate.\nThe usage of max(the_grid$post) is explained in endnote 73. It is basically used as an approximation to what would be the denominator of the likelihood.\n\\[\n\\sum_{i=1}^n \\left[\n\\log{\\mathcal{N}(\\mu \\mid mean = 0, sd = 10)} +\n\\log{\\mathcal{U}(\\sigma | min = 0, max = 10)}\n\\right]\n\\]\n\\[\n\\exp{\\left[\\log{P(\\mu, \\sigma \\mid h)}\\right]} = P(\\mu, \\sigma \\mid h)\n\\]\n\n# convert back to real scale\n# attention: see endnote 73 on using max(prob)\npost$grid$prob <- with(post$grid, {exp(prob - max(prob))})\n\nplot the results on a heatmap\n\np <- list()\np$heat <- ggplot(data = post$grid, aes(x = mu, y = sigma, fill = prob)) +\n  geom_raster() +\n  scale_x_continuous(limits = c(153, 156)) +\n  scale_y_continuous(limits = c(6.5, 9)) +\n  scale_fill_paletteer_c(\"grDevices::Viridis\") +\n  coord_fixed() +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  labs(title = \"The grid's posterior prob.\",\n       x=expression(mu), y=expression(sigma))\np$heat\n\nWarning: Removed 37129 rows containing missing values (geom_raster).\n\n\n\n\n\n\n\n4.3.4 Sampling from the grid’s posterior\n\npost$samples <- post$grid |>\n  slice_sample(n=1e4, weight_by = prob, replace = TRUE)\n\nand visualizing the density of \\(\\mu\\) and \\(\\sigma\\)\n\n# plot the density of mu\np$mu <- ggplot(data = post$samples, mapping = aes(x = mu)) +\n  geom_density(color = \"blue\", size = 1, fill = \"lightblue\") +\n  theme_minimal() +\n  labs(title = expression(\"distribution of\" ~ mu), x = expression(mu))\np$mu\n\n\n\n\n\n# plot the density of sigma\np$sigma <- ggplot(data = post$samples, mapping = aes(x = sigma)) +\n  geom_density(color = \"darkgreen\", size = 1, fill = \"lightgreen\") +\n  theme_minimal() +\n  labs(title = expression(\"distribution of\" ~ sigma), x = expression(sigma))\n\n\np$mu + p$sigma\n\n\n\n\nor, even, mapping them together using ggExtra\n\np$marg <- ggplot(data = post$samples, mapping = aes(x = mu, y = sigma)) +\n  geom_point(color = \"mediumorchid\", size = 0.8) +\n  geom_jitter(color = \"mediumorchid\", size = 0.8) +\n  theme_minimal() +\n  labs(title = expression(\"distribution of\" ~ mu ~ sigma),\n       x = expression(mu), y = expression(sigma))\np$marg <- ggExtra::ggMarginal(p$marg, \n                    xparams = list(colour = \"blue\", fill = \"lightblue\", size = 1),\n                    yparams = list(colour=\"darkgreen\", fill = \"lightgreen\", size = 1))\np$marg\n\n\n\n\n\n# to see the outut from ggMarginal, an extra code chunk is required\n# Source: https://github.com/daattali/ggExtra\n# grid::grid.newpage()\n# grid::grid.draw(p)\n\n\n\n4.3.5 Finding the posterior distribution with quap and brm()\n\n4.3.5.1 using rethinking::map\nWe now fit the model using rethinking::quap()\n\nSee the overthinking box about list() vs alist() on p. 88 of chapter 4.\n\nThe model is\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu, \\sigma)\\\\\n\\mu &\\sim \\mathcal{N}(178, 20) \\\\\n\\sigma &\\sim \\mathcal{Uniform}(0, 50)\n\\end{align*}\n\\]\nand the fit is\n\n# see Overthinking in section 4.3.5 for the difference between alist() and list()\nm4.1_args <-list(\n flist = alist(\n    height ~ dnorm(mu, sigma),\n    mu ~ dnorm(178, 28),\n    sigma ~ dunif(0, 50)),\n start = list(\n    mu  = mean(d2$height),\n    sigma = sd(d2$height)\n    ))\n\nm4.1 <- xfun::cache_rds(\n  {rethinking::quap(m4.1_args$flist, data = d2, start = m4.1_args$start)},\n  file = \"ch04_m04_01\")\n\nwhich gives us the summary\n\nprecis(m4.1)\n\n            mean        sd       5.5%      94.5%\nmu    154.602156 0.4120367 153.943642 155.260671\nsigma   7.731328 0.2913854   7.265637   8.197018\n\n\nand the variance covariance matrix is\n\nvcov(m4.1)\n\n                mu        sigma\nmu    0.1697742465 0.0001111986\nsigma 0.0001111986 0.0849054742\n\n\nand the correlation matrix\n\ncov2cor(vcov(m4.1))\n\n              mu      sigma\nmu    1.00000000 0.00092618\nsigma 0.00092618 1.00000000\n\n\n\n\n4.3.5.2 Using brms::brm\nThis borrows heavily from Kurz (2020)\nAs mentioned in chapter 8, it is best to use Half-Cauchy distribution for sigma as the tends to work better when using Half Cauchy for sigma when doing a Hamiltonian MCMC with brm().\nTherefore the model is\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu, \\sigma)\\\\\n\\mu &\\sim \\mathcal{N}(178, 20) \\\\\n\\sigma &\\sim \\mathcal{HalfCauchy}(0, 1)\n\\end{align*}\n\\]\n\nSee the overthinking box about half Cauchy distribution in chapter 8 on p. 260.\n\nThis process takes less than a second. It has been save to the rsd file b04_01.rds\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb4.1 <- xfun::cache_rds({\n  brms::brm(data = d2,\n            formula = height ~ 1,\n            family = gaussian,\n            prior = c(prior(normal(178, 20), class = Intercept),\n                      prior(cauchy(0, 1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n            seed = 4)},\n  file = \"ch04_b04_01\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.15 sec elapsed\n\n\n\nplot(b4.1)\n\n\n\n\nwith the summary\n\nsummary(b4.1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: d2 (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.42   153.76   155.41 1.00     3454     2464\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.74      0.28     7.19     8.32 1.00     3956     2623\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nwhich can also be done with tidybayes\n\nb4.1 |>\n  tidybayes::gather_draws(b_Intercept, sigma) |>\n  tidybayes::median_qi(.width = 0.89)\n\n# A tibble: 2 × 7\n  .variable   .value .lower .upper .width .point .interval\n  <chr>        <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 b_Intercept 155.   154.   155.     0.89 median qi       \n2 sigma         7.73   7.31   8.22   0.89 median qi       \n\n\nand to plot the posteriors we need to know the names of the variables\n\ntidybayes::get_variables(b4.1)\n\n [1] \"b_Intercept\"   \"sigma\"         \"lprior\"        \"lp__\"         \n [5] \"accept_stat__\" \"stepsize__\"    \"treedepth__\"   \"n_leapfrog__\" \n [9] \"divergent__\"   \"energy__\"     \n\n\nand we spread the data with one column per variable to be able to plot it. The tidybayes package is particularly useful for this. We will use it extensively from now on.\nIn particular, we can use tidybayes::spread_draws() to put variables in separate columns or tidybayes::gather_draws() to have them in long format```\nand we can visualize with ggdist. it could be done with tidybayes but since tidybayes only export ggsidt we use it directly.\n\np <- list()\n# quantiles used in the plots\np$qtl <- c(0.89, 1)\n# plot the posterior dist for b\np$b <- b4.1 |>\n  tidybayes::spread_draws(b_Intercept, sigma) |>\n  ggplot(aes(x = b_Intercept)) +\n  stat_halfeye(aes(fill=stat(cut_cdf_qi(cdf,.width = p$qtl))),\n           point_interval = mean_qi, .width = p$qtl) +\n   scale_fill_paletteer_d(palette = \"futurevisions::pso\", direction = -1,\n                         na.translate = FALSE) +\n  theme_ggdist() +\n  theme(legend.position = \"none\",\n        plot.background = element_rect(fill = \"palegoldenrod\"),\n        panel.background = element_rect(fill = \"lightblue\")) +\n  labs(title = \"Posterior probability of Intercept\", y = \"density\")\n\n# plot the posterior dist for sigma\np$sigma <- b4.1 |>\n  tidybayes::spread_draws(b_Intercept, sigma) |>\n  ggplot(aes(x = sigma)) +\n  stat_halfeye(aes(fill=stat(cut_cdf_qi(cdf, .width = p$qtl))),\n           point_interval = mean_qi, .width = p$qtl) +\n   scale_fill_paletteer_d(palette = \"futurevisions::pso\",\n                         na.translate = FALSE) +\n  theme_ggdist() +\n  theme(legend.position = \"none\",\n        plot.background = element_rect(fill = \"lightblue\"),\n        panel.background = element_rect(fill = \"palegoldenrod\")) +\n  labs(title = expression(\"Posterior probability of \" ~ sigma), y = \"density\")\np$sigma\n\n\n\np$b + p$sigma +\n  plot_annotation(title = \"Model 4.1\",\n                  theme = theme(title = element_text(color = \"midnightblue\")))\n\n\n\n\n\n\n\n4.3.6 Sampling from a fit\n\n4.3.6.1 Using quap\nSince map is a quadratic approximation, how do we simulate 2 variables, \\(\\mu\\) and \\(\\sigma\\)?\nSimply map gives us the variance covariance. Therefore map can be used to simulation the bivariate normal distribution of \\(\\mu\\) and \\(\\sigma\\)\n\nvcov(m4.1)\n\n                mu        sigma\nmu    0.1697742465 0.0001111986\nsigma 0.0001111986 0.0849054742\n\n\nfrom which we can obtain the correlation matrix\n\ncov2cor(vcov(m4.1))\n\n              mu      sigma\nmu    1.00000000 0.00092618\nsigma 0.00092618 1.00000000\n\n\nso to simulate using rethinking we simply use\n\npost <- extract.samples(m4.1, n = 1e4)\n\nwhich gives us a sample of size 10000 of the posterior distribution which can be summarized with the usual precis()\n\nprecis(post)\n\n            mean        sd       5.5%      94.5%      histogram\nmu    154.600100 0.4131715 153.941841 155.271320        ▁▁▅▇▂▁▁\nsigma   7.735382 0.2924855   7.266365   8.206993 ▁▁▁▁▂▅▇▇▃▁▁▁▁▁\n\n\n\n\n4.3.6.2 Using brm\nUsing brm however we are not given the variance covariance, it is only available for the intercept (first-level parameter)\n\nvcov(b4.1)\n\n          Intercept\nIntercept 0.1750386\n\n\nSo you have to calculate the var-cov matrix by using a sample from the posterior distribution\n\npost <- posterior_samples(b4.1)\n\nWarning: Method 'posterior_samples' is deprecated. Please see ?as_draws for\nrecommended alternatives.\n\nglimpse(post, 5)\n\nRows: 4,000\nColumns: 4\n$ b_Intercept <dbl> …\n$ sigma       <dbl> …\n$ lprior      <dbl> …\n$ lp__        <dbl> …\n\n# compute the cov\ncor(post[, c(\"b_Intercept\", \"sigma\")])\n\n            b_Intercept       sigma\nb_Intercept 1.000000000 0.001090313\nsigma       0.001090313 1.000000000\n\n\n\nSee comment from Kurz (2020) at end of section 4.3.6 to explain that McElreath uses mvnorm() from MASS to simulate using the varcov whereas with brms::posterior_samples() we do it directly.\n\nAlso Kurz (2020) has a nice discussion on how to create summary with histogram."
  },
  {
    "objectID": "ch04_linear.html#linear-predictions",
    "href": "ch04_linear.html#linear-predictions",
    "title": "4  Linear Models",
    "section": "4.4 Linear predictions",
    "text": "4.4 Linear predictions\n\n4.4.1 The linear model strategy\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\beta (x_i - \\bar{x}) \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta &\\sim \\mathcal{N}(0,10) \\\\\n\\sigma &\\sim \\mathcal{Uniform}(0, 50)\n\\end{align*}\n\\]\n\n4.4.1.1 Probability of the data\n\\[\nh_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\n\\] #### Linear model\n\\[\n\\mu_i = \\alpha + \\beta (x_i - \\bar{x})\n\\]\n\n\n4.4.1.2 Priors\n\\[\n\\begin{align*}\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta &\\sim \\mathcal{N}(0,10) \\\\\n\\sigma &\\sim \\mathcal{Uniform}(0, 50)\n\\end{align*}\n\\]\nThe goal is to simulate the heights from the model, using only the prior.\n\nprior <- list()\nprior$n <- 100L\nset.seed(4)\nprior$sim <- with(prior, {\n  data.frame(\n    id = seq_len(n),\n    a = rnorm(n = n, mean = 178, sd = 20),\n    b = rnorm(n = n, mean = 0, sd = 10)) |>\n    expand(nesting(id, a, b), weight = range(d2$weight)) |>\n    mutate(height = a + b * (weight - mean(d2$weight)))\n})\nglimpse(prior$sim)\n\nRows: 200\nColumns: 5\n$ id     <int> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 1…\n$ a      <dbl> 182.3351, 182.3351, 167.1501, 167.1501, 195.8229, 195.8229, 189…\n$ b      <dbl> 6.8480194, 6.8480194, -1.1511351, -1.1511351, -3.5647518, -3.56…\n$ weight <dbl> 31.07105, 62.99259, 31.07105, 62.99259, 31.07105, 62.99259, 31.…\n$ height <dbl> 87.01455, 305.61385, 183.17330, 146.42730, 245.44222, 131.64986…\n\n\nand we plot if\n\nggplot(prior$sim, aes(x = weight, y = height, group = id)) +\n  geom_line(alpha = 1/10) +\n  geom_hline(yintercept = c(0, 272), linetype = c(2, 1), size = 1/3) +\n  coord_cartesian(ylim = c(-100, 400)) +\n  theme_classic() +\n  labs(title = \"b ~ dnorm(0, 10)\")\n\n\n\n\n\n4.4.1.2.1 Adjusting the priors\nsince we know that the effect (\\(\\beta\\)) of the weight on height, i.e. the relation between the 2 should be positive and very large value unlikely we can use the log-normal as a prior on \\(beta\\).\nIn addition, sigma can also very often be better modeled with the exponential or HalfCauchy distribution. See section 9.5.3 in the text. We will use the exponential distribution for \\(\\sigma\\) in this work.\n\np <- list()\np$lnorm <- ggplot(data.frame(x = c(0, 5)), aes(x)) +\n  stat_function(geom = \"line\", fun = dlnorm, args = list(meanlog = 0, sdlog = 1), \n                color = \"slategray\", size = 1.5) +\n  stat_function(geom = \"area\", fun = dlnorm, args = list(meanlog = 0, sdlog = 1), \n                fill = \"slategray1\") +\n  theme_classic() +\n  labs(title = \"log-normal distribution\", x = expression(beta), y = \"density\")\np$exp <- ggplot(data.frame(x = c(0, 5)), aes(x)) +\n  stat_function(geom = \"line\", fun = dexp, args = list(rate = 1), \n                color = \"seagreen\", size = 1.5) +\n  stat_function(geom = \"area\", fun = dexp, args = list(rate = 1), \n                fill = \"seagreen1\") +\n  theme_classic() +\n  labs(title = \"exponential distribution\", x = expression(beta), y = \"density\")\nwrap_plots(p)\n\n\n\n\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\beta (x_i - \\bar{x}) \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta &\\sim \\mathcal{LogNormal}(0,1) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\n\n\n\n\n4.4.2 Fitting the posterior distribution\nAs suggested by the discussion of prior just above, we use a log-normal prior for \\(\\beta\\)\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\beta (x_i - \\bar{x}) \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta &\\sim \\mathcal{LogNormal}(0,1) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\n\n4.4.2.1 Using quap\nWe add the centralized weight to the data\n\ndata(\"Howell1\")\nd <- Howell1\nd2 <- d |>\n  filter(age >= 18) |>\n  mutate(weight_c = scale(weight, center = TRUE, scale = FALSE))\nrm(Howell1)\n\nthen get the fit using rethinking::quap\n\nGiving start values to quap seem to help it significantly and avoiding error, at least when using b ~ dlnorm(0, 1).\n\n\nm4.3_args <-list(\n flist = alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * weight_c,\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)),\n start = list(\n    a  = mean(d2$height),\n    sigma = sd(d2$height)\n    ))\n\n\ntictoc::tic()\nm4.3 <- xfun::cache_rds({\n  quap(\n    flist=m4.3_args$flist,\n    data = d2,\n    start=m4.3_args$start\n    )},\n  file = \"ch04_m04_03\")\ntictoc::toc()\n\n0 sec elapsed\n\n\n\nprecis(m4.3)\n\n             mean         sd        5.5%       94.5%\na     154.6013679 0.27030764 154.1693641 155.0333718\nsigma   5.0718806 0.19115475   4.7663784   5.3773828\nb       0.9032807 0.04192363   0.8362786   0.9702828\n\n\n\n\n4.4.2.2 Using brm\nAgain, we use the exponential distribution as a prior of sigma to facilitate the iterations with brm. There are 2 equivalent ways to run this model. One uses the log-normal distribution of \\(\\beta\\), the other one uses the log transform of \\(\\beta\\) with the normal distribution. The two models are mathematically equivalent\n\n\n4.4.2.3 Using lognormal distribution\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\beta (x_i - \\bar{x}) \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta &\\sim \\mathcal{LogNormal}(0,1) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\nWhen using lognormal for a parameter of class b, you should specify lb and ub (lower bound and upper bound) to avoid error message and accelerate the computations with brm.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nb4.3 <- xfun::cache_rds({\n  brms::brm(\n    data = d2,\n    family = gaussian,\n    formula = height ~ 1 + weight_c,\n    prior = c(\n      prior(normal(178, 20), class = Intercept),\n      prior(lognormal(0, 1), class = b, lb = 0, ub = 3),\n      prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, cores = detectCores(), chains = detectCores(),\n    seed = 4)},\n  file = \"ch04_b04_03\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.16 sec elapsed\n\n\n\nplot(b4.3)\n\n\n\nsummary(b4.3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight_c \n   Data: d2 (Number of observations: 352) \n  Draws: 8 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 8000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.27   154.07   155.12 1.00     7975     6010\nweight_c      0.90      0.04     0.82     0.99 1.00     8445     6252\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.07      0.19     4.71     5.46 1.00     8724     6149\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n4.4.3 Using the log tranformation\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\exp{(log\\_b)} (x_i - \\bar{x}) \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\nlog\\_b &\\sim \\mathcal{N}(0,1) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\n\nglimpse(d2)\n\nRows: 352\nColumns: 5\n$ height   <dbl> 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 1…\n$ weight   <dbl> 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 3…\n$ age      <dbl> 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 4…\n$ male     <int> 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1…\n$ weight_c <dbl[,1]> <matrix[26 x 1]>\n\n\n\nTODO: This brm command does not work. It was in kurtz2020b. Section 5.3.2 and 6.2.1 are supposed to have the solution. We will come back.\n\n\n\n\n\n\n\nTODO\n\n\n\nThis brm command does not work. It was in Kurz (2020). Section 5.3.2 and 6.2.1 are supposed to have the solution. We will come back.\n\n\n\n# tictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\n# b4.3b <- xfun::cache_rds({\n#   brms::brm(\n#   data = d2,\n#   family = gaussian,\n#   formula = bf(height ~ a + exp(lb) * weight_c,\n#                a ~ 1, lb ~ 1, nl = TRUE),\n#   prior = c(\n#     prior(normal(178, 20), class = b, nlpar = a),\n#     prior(normal(0, 1), class = b, nlpar = lb),\n#     prior(exponential(1), class = sigma)),\n#   iter = 2000, warmup = 1000, chains = 4, cores = detectCores(), seed = 4)},\n#   file = \"ch04_b04_03b\")\n# tictoc::toc()\n\n\n# summary(b4.3b)\n\n\n\n4.4.4 Interpreting the posterior distribution\n\n4.4.4.1 Tables of marginal distributions\nUsing rethinking Important, the parameters are correlated here, to avoid this one must do centering of variables. The following uses centered variables.\n\nprecis(m4.3, corr = TRUE)\n\n             mean         sd        5.5%       94.5%\na     154.6013679 0.27030764 154.1693641 155.0333718\nsigma   5.0718806 0.19115475   4.7663784   5.3773828\nb       0.9032807 0.04192363   0.8362786   0.9702828\n\n\n\nround(vcov(m4.3), 3)\n\n          a sigma     b\na     0.073 0.000 0.000\nsigma 0.000 0.037 0.000\nb     0.000 0.000 0.002\n\n\nUsing brm\nNote: lp__ stands for unnormalized log posterior density.\n\nposterior_summary(b4.3, probs = c(0.055, 0.975)) |>\n  round(digits = 2)\n\n            Estimate Est.Error     Q5.5    Q97.5\nb_Intercept   154.60      0.27   154.17   155.12\nb_weight_c      0.90      0.04     0.84     0.99\nsigma           5.07      0.19     4.77     5.46\nlprior        -10.34      0.19   -10.66    -9.98\nlp__        -1081.70      1.22 -1083.99 -1080.29\n\n\nwe get the varcov matrix as follows\n\nposterior_samples(b4.3) |>\n  select(-lp__) |>\n  cov() |>\n  round(digits = 3)\n\nWarning: Method 'posterior_samples' is deprecated. Please see ?as_draws for\nrecommended alternatives.\n\n\n            b_Intercept b_weight_c  sigma lprior\nb_Intercept       0.072      0.000  0.001  0.003\nb_weight_c        0.000      0.002  0.000 -0.002\nsigma             0.001      0.000  0.036 -0.036\nlprior            0.003     -0.002 -0.036  0.038\n\n\nand the correlation matrix\n\nposterior_samples(b4.3) |>\n  select(-lp__) |>\n  cor() |>\n  round(digits = 3)\n\nWarning: Method 'posterior_samples' is deprecated. Please see ?as_draws for\nrecommended alternatives.\n\n\n            b_Intercept b_weight_c  sigma lprior\nb_Intercept       1.000     -0.007  0.024  0.058\nb_weight_c       -0.007      1.000 -0.026 -0.192\nsigma             0.024     -0.026  1.000 -0.973\nlprior            0.058     -0.192 -0.973  1.000\n\n\n\n\n4.4.4.2 Plotting posterior inference against data\nWith brms we use the ggmcmc package to illustrate the results from the markov chain\n\ntidybayes::get_variables(b4.3)\n\n [1] \"b_Intercept\"   \"b_weight_c\"    \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\npost <- list()\npost$long <- b4.3 |>\n  tidybayes::gather_draws(b_Intercept, b_weight_c, sigma)\n\nwith the histogram\n\np <- list()\np$hist <- ggplot(post$long, aes(x = .value)) +\n  geom_histogram(aes(fill = .variable)) +\n  scale_fill_paletteer_d(palette = \"futurevisions::atomic_orange\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_wrap(. ~ .variable, ncol = 1, scales = \"free\")\np$hist\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nand density plots by chains\n\np$dens <- ggplot(post$long, aes(x = .value, color = as.factor(.chain))) +\n  geom_density() +\n  scale_color_paletteer_d(palette = \"futurevisions::atomic_clock\") +\n  theme_minimal() +\n  # theme(legend.position = \"none\") +\n  labs(x = NULL, color = \"chain\") +\n  facet_wrap(. ~ .variable, ncol = 1, scales = \"free\")\np$dens\n\n\n\n\nand the paired plots with ggally\n\npost$wide <- b4.3 |>\n  tidybayes::spread_draws(b_Intercept, b_weight_c, sigma)\n# glimpse(post$wide)\n\n\np$pairs <- GGally::ggscatmat(post$wide, \n                                columns = c(\"b_Intercept\", \"b_weight_c\", \"sigma\"),\n                                color = \".chain\", alpha = 0.8) +\n  scale_color_paletteer_d(palette = \"futurevisions::atomic_clock\") +\n  theme_minimal()\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\np$pairs\n\n\n\n\nand the correlation matrix\n\np$corr <- GGally::ggcorr(post$wide[, c(\"b_Intercept\", \"b_weight_c\", \"sigma\")],\n                            color = \"darkgreen\",\n                            nbreaks = 10, label = TRUE, label_round = 2,\n                            label_color = \"midnightblue\") +\n  scale_fill_paletteer_d(palette = \"futurevisions::venus\") +\n  theme(legend.position = \"none\") +\n  labs(title = \"Correlations between parameters\")\n\nScale for 'fill' is already present. Adding another scale for 'fill', which\nwill replace the existing scale.\n\np$corr\n\n\n\n\nand for added extra, the trace plot\n\np$trace <- ggplot(post$long, aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line() +\n  scale_color_paletteer_d(palette = \"futurevisions::atomic_clock\", direction = 1) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = NULL, y = NULL) +\n  facet_wrap(. ~ .variable, ncol = 1, scales = \"free\")\np$trace"
  },
  {
    "objectID": "ch04_linear.html#curves-from-lines",
    "href": "ch04_linear.html#curves-from-lines",
    "title": "4  Linear Models",
    "section": "4.5 Curves from lines",
    "text": "4.5 Curves from lines\n\n4.5.1 Polynomial regression\n\ndata(\"Howell1\")\nd <- Howell1 |>\n  mutate(weight_s = scale(as.vector(weight)),\n         weight_s2 = weight_s ^ 2)\nrm(Howell1)\n# str(d)\n\n# A function to inverse transform the data to natural scale\n# will be used in plots below. Normally used with `base::scale()`.\n# If the base::scale function was used originally, the attributes contain \n# the center and scale values.\ninv.scale <- function(x, \n                      center = attr(x, which = \"scaled:center\"),\n                      scale = attr(x, which = \"scaled:scale\")) {\n  if(!is.finite(center)) center <- 0L\n  if(!is.finite(scale)) scale <- 1L\n  center + x * scale\n}\n\n\np <- list()\n\n# add geom after because in later plot we need to keep geom_point() again\n# to keep it on top\ncolr <- unclass(paletteer::paletteer_d(\"futurevisions::titan\"))\n# nice trick: we use the pipe with {} to be able to reuse the data within the plot\np$basic <- d %>% {\n  ggplot(d, aes(x = weight_s, y = height, color = age)) +\n  scale_x_continuous(breaks = scales::breaks_extended(n=7),\n                     labels = function(x) {\n                       x <- inv.scale(x, mean(.$weight), sd(.$weight))\n                       label_number(accuracy = 1)(x)\n                     }) +\n  scale_color_gradientn(colors = colr) +\n  theme_classic() +\n  theme(title = element_text(color = \"midnightblue\"),\n        legend.position = c(0.1, 0.8)) +\n  labs(title = \"Census data for the Dobe area !Kung San\",\n       subtitle = sprintf(\"%d individuals\", nrow(.)))\n}\n\np$basic +\n  geom_point(shape = 20, size = 2, alpha = 2/3)\n\n\n\n\nand the model used is\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + \\beta_1 \\cdot weight\\_s_i + \\beta_2 \\cdot weight\\_s^2_i \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta_1 &\\sim \\mathcal{LogNormal}(0,1) \\\\\n\\beta_2 &\\sim \\mathcal{N}(0,1) \\\\\n\\sigma &\\sim \\mathcal{Exponential}(1)\n\\end{align*}\n\\]\nhttps://discourse.mc-stan.org/t/error-with-gamma-prior/16420\n\n\n\n\n\n\nWarning\n\n\n\nThe following code gives a warning about setting lower boundaries. It started to show with R 4.2. Paul Buerkner advises to ignore it. See advice\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb4.5 <- xfun::cache_rds({\n  brm(data = d,\n      family = gaussian,\n      height ~ 1 + weight_s + weight_s2,\n      prior = c(prior(normal(178, 20), class = Intercept),\n                prior(lognormal(0, 1), class = b, coef = \"weight_s\"),\n                prior(normal(0, 1), class = b, coef = \"weight_s2\"),\n                prior(exponential(1), class = sigma)),\n      iter = 4000, warmup = 2000, chains = 4, cores = detectCores(),\n      seed = 4)},\n  file = \"ch04_b04_05\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.47 sec elapsed\n\n\n\nsummary(b4.5)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight_s + weight_s2 \n   Data: d (Number of observations: 544) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   146.05      0.37   145.33   146.78 1.00     6914     5892\nweight_s     21.73      0.29    21.16    22.31 1.00     6999     5915\nweight_s2    -7.80      0.27    -8.33    -7.26 1.00     6755     6110\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.78      0.18     5.44     6.13 1.00     7514     5821\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand to obtain a simplified dataframe we use\n\nbrms::fixef(b4.5)\n\n            Estimate Est.Error       Q2.5      Q97.5\nIntercept 146.052741 0.3694311 145.330484 146.778101\nweight_s   21.732044 0.2926298  21.160968  22.308850\nweight_s2  -7.797295 0.2744356  -8.326718  -7.264454\n\n\n\ntidybayes::get_variables(b4.5)\n\n [1] \"b_Intercept\"   \"b_weight_s\"    \"b_weight_s2\"   \"sigma\"        \n [5] \"lprior\"        \"lp__\"          \"accept_stat__\" \"stepsize__\"   \n [9] \"treedepth__\"   \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\np$dens <- \n  b4.5 |> tidybayes::gather_draws(b_Intercept, b_weight_s, b_weight_s, sigma) |>\n  ggplot(aes(x = .value, color = as.factor(.chain))) +\n  geom_density() +\n  scale_color_paletteer_d(palette = \"futurevisions::mars\", direction = -1) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = NULL, color = \"chain\") +\n  facet_wrap(. ~ .variable, ncol = 1, scales = \"free\")\n# p$dens\n\nand\n\np$trace <- \n  b4.5 |> tidybayes::gather_draws(b_Intercept, b_weight_s, b_weight_s, sigma) |>\n  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) +\n  geom_line() +\n  scale_color_paletteer_d(palette = \"futurevisions::mars\", direction = -1) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = NULL, y = NULL) +\n  facet_wrap(. ~ .variable, ncol = 1, scales = \"free\")\n# b4.5_trace\n\n\np$dens + p$trace\n\n\n\n\nAnd we look at the fitted and predicted values to understand and interpret the result.\nWhat is the difference between fitted and predict? fitted A nice explanation is given by Greg Snow\n\nThe fitted function returns the y-hat values associated with the data used to fit the model. The predict function returns predictions for a new set of predictor variables. If you don’t specify a new set of predictor variables then it will use the original data by default giving the same results as fitted for some models (especially the linear ones), but if you want to predict for a new set of values then you need predict. The predict function often also has options for which type of prediction to return, the linear predictor, the prediction transformed to the response scale, the most likely category, the contribution of each term in the model, etc.\n\nTherefore, if we give the same data to fitted or predict will will obtain sensibly the same results, the difference being caused by the random seed. However, in Bayesian stats, fitted will only provide \\(\\mu_i\\) and its variation whereas predict will give \\(h_i\\) which is \\(h_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\\)\nWe can see it clearly here as fitd_quad gives ans estimate about the same as for predict since they both report the same \\mu_i, but predict has a wider interval since it uses \\(\\sigma\\)\n\nsamples <- list()\n# the new data to use for sampling fitted and pedict\nsamples$newdata <- data.frame(\n  weight_s=seq_range(d$weight_s, n=30L)) |>\n  mutate(weight_s2=weight_s^2)\n\nsamples$fitted <- with(samples, {\n  fitted(b4.5, newdata = newdata) |>\n  data.frame() |>\n  bind_cols(newdata)\n})\n# glimpse(samples$fitted)\n\nsamples$predict <- with(samples, {\n  predict(b4.5, newdata = newdata) |>\n  data.frame() |>\n  bind_cols(newdata)\n})\n# glimpse(samples$predict)\n\nand we can now create the plot.\n\np$model <- p$basic +\n  # scale_x_continuous(labels = function(x) fnc_nat(x)) +\n  geom_ribbon(data = samples$predict,\n              aes(x = weight_s, ymin = Q2.5, ymax = Q97.5), inherit.aes = FALSE,\n              fill = \"lightcyan\", alpha = 1) +\n  geom_smooth(data = samples$fitted,\n              aes(x=weight_s, y = Estimate, ymin = Q2.5, ymax = Q97.5), inherit.aes = FALSE,\n              stat = \"identity\",\n              fill = \"lightcyan3\", color = \"royalblue\", alpha = 1, size = 1/2) +\n  geom_point(shape = 20, size = 2, alpha = 2/3)\np$model\n\n\n\n\n\n\n4.5.2 Splines\n\ndata(\"cherry_blossoms\")\nd <- cherry_blossoms\nrm(cherry_blossoms)\nd |> skimr::skim()\n\n\nData summary\n\n\nName\nd\n\n\nNumber of rows\n1215\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n1408.00\n350.88\n801.00\n1104.50\n1408.00\n1711.50\n2015.00\n▇▇▇▇▇\n\n\ndoy\n388\n0.68\n104.54\n6.41\n86.00\n100.00\n105.00\n109.00\n124.00\n▁▅▇▅▁\n\n\ntemp\n91\n0.93\n6.14\n0.66\n4.67\n5.70\n6.10\n6.53\n8.30\n▃▇▇▂▁\n\n\ntemp_upper\n91\n0.93\n7.19\n0.99\n5.45\n6.48\n7.04\n7.72\n12.10\n▇▇▂▁▁\n\n\ntemp_lower\n91\n0.93\n5.10\n0.85\n0.75\n4.61\n5.14\n5.54\n7.74\n▁▁▆▇▁\n\n\n\n\n# data without NA\nd2 <- d |>\n  drop_na(doy)\n\n\n4.5.2.1 Knots, degree and basis functions\nThe knots used here are based on quantiles, other ways are possible,\n\nknots <- quantile(d2$year, probs = seq(from = 0, to = 1, length.out = 15))\nknots\n\n       0% 7.142857% 14.28571% 21.42857% 28.57143% 35.71429% 42.85714%       50% \n      812      1036      1174      1269      1377      1454      1518      1583 \n57.14286% 64.28571% 71.42857% 78.57143% 85.71429% 92.85714%      100% \n     1650      1714      1774      1833      1893      1956      2015 \n\n\n\ncolr <- unclass(paletteer::paletteer_d(\"futurevisions::cancri\"))\nggplot(d2, aes(x = year, y = doy, color = temp)) +\n  geom_vline(xintercept = knots, color = \"slateblue\", alpha = 1/2) +\n  geom_point(shape = 20, size = 2, alpha = 2/3) +\n  scale_x_continuous(breaks = knots, labels = scales::label_number(big.mark = \"\")) +\n  scale_color_gradientn(colors = colr) +\n  theme_classic() +\n  theme(title = element_text(color = \"midnightblue\"),\n        legend.position = c(0.05, 0.8),\n        axis.text.x = element_text(size = rel(0.9))) +\n  labs(title = \"Cherry Blossom in Japan\",\n       subtitle = sprintf(\"%d observations with %d knots\", nrow(d), length(knots)))\n\n\n\n\nthe code knots[-c(1, nknots)] is required because bs places knots at the boundaries by default, so we have to remove them.\n\nlibrary(splines)\nB <- splines::bs(x = d2$year, knots = knots[-c(1, length(knots))], degree = 3, intercept = TRUE)\n# str(B)\n\nand we plot the basis functions\n\n# this data.frame will be reused below with the posteriors\ndf_bias <- B |>\n  as.data.frame() %>%\n  setNames(sprintf(\"B%02d\", seq_len(ncol(.)))) |>\n  mutate(year = d2$year) |>\n  pivot_longer(cols = -year, names_to = \"bias_func\", values_to = \"bias\")\n# str(df_bias)\n\nclrs <- paletteer::paletteer_c(\"pals::jet\", n = length(unique(df_bias$bias_func)))\nggplot(df_bias, aes(x = year, y = bias, color = bias_func)) +\n  geom_vline(xintercept = knots, color = \"grey60\", linestyle = \"longdash\", alpha = 1/2) +\n  geom_line() +\n  scale_x_continuous(breaks = knots, labels = scales::label_number(big.mark = \"\")) +\n  scale_color_manual(values = clrs) +\n  ggthemes::theme_tufte() +\n  theme(legend.position = \"none\") +\n  labs(\"The bias functions\")\n\nWarning: Ignoring unknown parameters: linestyle\n\n\n\n\n\n\n\n4.5.2.2 Model and fit\n\\[\n\\begin{align*}\ndoy_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\,u_i &= \\alpha + \\sum_{k=1}^Kw_kB_{k, i} \\\\\n\\alpha &\\sim \\mathcal{N}(100, 10) \\\\\nw_j &\\sim \\mathcal{N}(0, 10) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nWe first create append the matrix to the data in one column. See Kurz (2020) on this data structure.\n\nd3 <- d2 |>\n  mutate(B = B)\n# the last column is a matrix column, with same nb of rows as the other\n# columns but with a column including 17 subcolumns (!)\n# glimpse(d3)\n\nand the fit\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb4.8 <- xfun::cache_rds({\n  brm(data = d3,\n      family = gaussian,\n      doy ~ 1 + B,\n      prior = c(prior(normal(100, 10), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(exponential(1), class = sigma)),\n      cores = detectCores(), seed = 4)},\n  file = \"ch04_b04_08\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.22 sec elapsed\n\n\n\nsummary(b4.8)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: doy ~ 1 + B \n   Data: d3 (Number of observations: 827) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   103.51      2.43    98.75   108.31 1.01      657     1083\nB1           -3.14      3.92   -10.74     4.61 1.00     1472     2103\nB2           -1.05      3.93    -8.78     6.66 1.00     1512     2289\nB3           -1.17      3.69    -8.35     6.08 1.00     1121     2198\nB4            4.66      2.94    -1.09    10.40 1.00      950     1799\nB5           -0.98      2.98    -6.65     5.04 1.01      860     1448\nB6            4.17      2.95    -1.68     9.87 1.00      946     1843\nB7           -5.47      2.87   -11.16     0.25 1.00      844     1264\nB8            7.68      2.88     2.15    13.32 1.01      864     1804\nB9           -1.16      2.92    -6.92     4.66 1.00      978     1688\nB10           2.84      2.98    -2.90     8.50 1.00      890     1897\nB11           4.51      2.94    -1.31    10.09 1.00      906     1686\nB12          -0.30      2.92    -5.97     5.41 1.01      910     1429\nB13           5.39      2.93    -0.44    11.13 1.01      883     1521\nB14           0.55      3.05    -5.41     6.42 1.00      938     1691\nB15          -0.97      3.31    -7.40     5.44 1.00     1079     1880\nB16          -7.13      3.43   -13.84    -0.39 1.00     1124     2086\nB17          -7.81      3.28   -14.32    -1.50 1.00     1151     1906\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.95      0.15     5.67     6.24 1.00     4602     2860\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n4.5.2.3 Plot\n\n# get_variables(b4.8)\n\n\n# Source: https://github.com/mjskay/tisdybayes/issues/38\ndf <- tidybayes::gather_draws(b4.8, !!sym(\"^b_B.+\"), regex = TRUE) |>\n  mutate(.variable = as.integer(sub(\"^b_B\", replacement = \"\", x = .variable)),\n         .variable = sprintf(\"B%02d\", .variable)) |>\n  rename(\"bias_func\" = .variable) |>\n  group_by(bias_func) |>\n  summarise(weight = mean(.value)) |>\n  full_join(y = df_bias, by = \"bias_func\")\n# glimpse(df)\n\nclrs <- paletteer::paletteer_c(\"pals::jet\", n = length(unique(df_bias$bias_func)))\nggplot(df, aes(x = year, y = bias * weight, color = bias_func)) +\n  geom_vline(xintercept = knots, color = \"grey60\", linestyle = \"longdash\", alpha = 1/2) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = knots, labels = scales::label_number(big.mark = \"\")) +\n  scale_color_manual(values = clrs) +\n  ggthemes::theme_tufte() +\n  theme(legend.position = \"none\") +\n  labs(title = \"fitted bias functions\")\n\nWarning: Ignoring unknown parameters: linestyle\n\n\n\n\n\n\ndf <- fitted(b4.8) |>\n  as.data.frame() |>\n  bind_cols(d2)\nclrs <- unclass(paletteer::paletteer_d(\"futurevisions::cancri\"))\nggplot(df, aes(x = year, y = doy)) +\n  geom_vline(xintercept = knots[-c(1, length(knots))], color = \"slateblue\", alpha = 1/2) +\n  geom_point(aes(color = temp)) +\n  geom_lineribbon(aes(x = year, y = Estimate, ymin = Q2.5, ymax = Q97.5),\n                  color = \"blueviolet\", fill = \"cornflowerblue\", alpha = 1/2) +\n  scale_x_continuous(breaks = knots, labels = scales::label_number(big.mark = \"\")) +\n  scale_color_gradientn(colors = clrs) +\n  ggthemes::theme_tufte() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Figure 4.12\")\n\n\n\n\n\n\n\n4.5.3 Smooth functions for a smooth world\nSee Kurz (2020) for much more details on this topic."
  },
  {
    "objectID": "ch04_linear.html#summary",
    "href": "ch04_linear.html#summary",
    "title": "4  Linear Models",
    "section": "4.6 Summary",
    "text": "4.6 Summary\nThis was an important chapter. Most of the plots and basic coding tools are exemplified here. It is an important reference chapter. The brms package will be exclusively used from now on.\n\n\n\n\nKurz, Solomon. 2020. Statistical Rethinking with Brms. 2nd ed. https://bookdown.org/content/4857/."
  },
  {
    "objectID": "ch05_multivariate.html",
    "href": "ch05_multivariate.html",
    "title": "5  Multivariate Linear Models",
    "section": "",
    "text": "As mentioned in the summary of chapter 4, from hereon, the brms package will be used instead of rethinking. In addition, we might change the exponential(1) prior for the cauchy(0, 1) when it improves the performance of the fit."
  },
  {
    "objectID": "ch05_multivariate.html#spurious-association",
    "href": "ch05_multivariate.html#spurious-association",
    "title": "5  Multivariate Linear Models",
    "section": "5.1 Spurious association",
    "text": "5.1 Spurious association\nGet the data and standardize the variables\n\n# load data\ndata(\"WaffleDivorce\")\nd <- WaffleDivorce\n# standardize the variables\nd <- d |>\n  mutate(A = round(scale(MedianAgeMarriage), 4),\n         M = round(scale(Marriage), 4),\n         D = round(scale(Divorce), 4))\nskimr::skim(d)\n\n\nData summary\n\n\nName\nd\n\n\nNumber of rows\n50\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nLocation\n0\n1\nFALSE\n50\nAla: 1, Ala: 1, Ari: 1, Ark: 1\n\n\nLoc\n0\n1\nFALSE\n50\nAK: 1, AL: 1, AR: 1, AZ: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPopulation\n0\n1\n6.12\n6.88\n0.56\n1.64\n4.44\n6.68\n37.25\n▇▂▁▁▁\n\n\nMedianAgeMarriage\n0\n1\n26.05\n1.24\n23.20\n25.33\n25.90\n26.75\n29.70\n▂▇▇▂▁\n\n\nMarriage\n0\n1\n20.11\n3.80\n13.50\n17.12\n19.70\n22.10\n30.70\n▅▇▆▂▁\n\n\nMarriage.SE\n0\n1\n1.40\n0.80\n0.39\n0.81\n1.19\n1.77\n3.92\n▇▆▂▂▁\n\n\nDivorce\n0\n1\n9.69\n1.82\n6.10\n8.30\n9.75\n10.90\n13.50\n▂▇▇▅▃\n\n\nDivorce.SE\n0\n1\n0.96\n0.53\n0.24\n0.57\n0.80\n1.26\n2.50\n▇▇▂▂▁\n\n\nWaffleHouses\n0\n1\n32.34\n65.79\n0.00\n0.00\n1.00\n39.75\n381.00\n▇▁▁▁▁\n\n\nSouth\n0\n1\n0.28\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nSlaves1860\n0\n1\n79378.34\n149730.92\n0.00\n0.00\n0.00\n80828.00\n490865.00\n▇▁▁▁▁\n\n\nPopulation1860\n0\n1\n628729.28\n781312.66\n0.00\n43321.00\n407722.00\n920977.00\n3880735.00\n▇▂▁▁▁\n\n\nPropSlaves1860\n0\n1\n0.09\n0.17\n0.00\n0.00\n0.00\n0.09\n0.57\n▇▁▁▁▁\n\n\nA\n0\n1\n0.00\n1.00\n-2.29\n-0.59\n-0.12\n0.56\n2.93\n▂▆▇▂▁\n\n\nM\n0\n1\n0.00\n1.00\n-1.74\n-0.79\n-0.11\n0.52\n2.79\n▅▇▆▂▁\n\n\nD\n0\n1\n0.00\n1.00\n-1.97\n-0.76\n0.03\n0.67\n2.09\n▂▇▇▅▃\n\n\n\n\nstr(d)\n\n'data.frame':   50 obs. of  16 variables:\n $ Location         : Factor w/ 50 levels \"Alabama\",\"Alaska\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ Loc              : Factor w/ 50 levels \"AK\",\"AL\",\"AR\",..: 2 1 4 3 5 6 7 9 8 10 ...\n $ Population       : num  4.78 0.71 6.33 2.92 37.25 ...\n $ MedianAgeMarriage: num  25.3 25.2 25.8 24.3 26.8 25.7 27.6 26.6 29.7 26.4 ...\n $ Marriage         : num  20.2 26 20.3 26.4 19.1 23.5 17.1 23.1 17.7 17 ...\n $ Marriage.SE      : num  1.27 2.93 0.98 1.7 0.39 1.24 1.06 2.89 2.53 0.58 ...\n $ Divorce          : num  12.7 12.5 10.8 13.5 8 11.6 6.7 8.9 6.3 8.5 ...\n $ Divorce.SE       : num  0.79 2.05 0.74 1.22 0.24 0.94 0.77 1.39 1.89 0.32 ...\n $ WaffleHouses     : int  128 0 18 41 0 11 0 3 0 133 ...\n $ South            : int  1 0 0 1 0 0 0 0 0 1 ...\n $ Slaves1860       : int  435080 0 0 111115 0 0 0 1798 0 61745 ...\n $ Population1860   : int  964201 0 0 435450 379994 34277 460147 112216 75080 140424 ...\n $ PropSlaves1860   : num  0.45 0 0 0.26 0 0 0 0.016 0 0.44 ...\n $ A                : num [1:50, 1] -0.606 -0.687 -0.204 -1.41 0.6 ...\n  ..- attr(*, \"scaled:center\")= num 26.1\n  ..- attr(*, \"scaled:scale\")= num 1.24\n $ M                : num [1:50, 1] 0.0226 1.5498 0.049 1.6551 -0.267 ...\n  ..- attr(*, \"scaled:center\")= num 20.1\n  ..- attr(*, \"scaled:scale\")= num 3.8\n $ D                : num [1:50, 1] 1.654 1.544 0.611 2.094 -0.927 ...\n  ..- attr(*, \"scaled:center\")= num 9.69\n  ..- attr(*, \"scaled:scale\")= num 1.82\n\n\nand plot the data\n\nplot_waffles <- function(data, x_var = \"WaffleHouses\", y_var = \"Divorce\",\n                         color_var = \"South\",\n                         the_labs = labs(title = \"Waffle Houses\",\n                                     x = \"Waffle Houses per million\",\n                                     y = \"Divorce rate\",\n                                     color = \"South\")) {\n  ggplot(data, aes(x = .data[[x_var]], y = .data[[y_var]])) +\n    geom_point(aes(color = as.logical(.data[[color_var]]))) +\n    scale_color_paletteer_d(\"fishualize::Bodianus_rufus\", direction = -1) + \n    theme_classic() +\n    theme(title = element_text(color = \"midnightblue\")) +\n    the_labs\n}\n\n\np <- lapply(X = c(\"MedianAgeMarriage\", \"Marriage\", \"WaffleHouses\"),\n            FUN = function(x) {\n              plot_waffles(d, x_var = x) +\n                stat_smooth(method = \"lm\", fullrange = TRUE,\n                            fill = \"darkolivegreen\", color = \"darkgreen\") +\n                labs(x = x)\n            })\nwrap_plots(p, guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nGGally::ggscatmat(d, columns = c(\"A\", \"M\", \"D\")) +\n  # scale_color_paletteer_d(palette = \"calecopal::buow\") +\n  theme_minimal()\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n\n\n\nWe will be using this function in the plots to show the natural scales.\n\n# A function to inverse transform the data to natural scale\n# will be used in plots below. Normally used with `base::scale()`.\n# If the base::scale function was used originally, the attributes contain \n# the center and scale values.\ninv.scale <- function(x, \n                      center = attr(x, which = \"scaled:center\"),\n                      scale = attr(x, which = \"scaled:scale\")) {\n  if(!is.finite(center)) center <- 0L\n  if(!is.finite(scale)) scale <- 1L\n  center + x * scale\n}\n\nThe model for regressing the divorce rate \\(D\\) on the median age \\(A\\) is\n\\[\n\\begin{align*}\nD_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_A \\cdot A_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_A &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand the fit, take note of the argument sample_prior = TRUE which is important for the rest of the exercise\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nb5.1 <- xfun::cache_rds({\n  out <- brm(\n    data = d,\n    formula = D ~ 1 + A,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 0.02), class = Intercept),\n      prior(normal(0, 0.5), class = b),\n      prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4,\n    sample_prior = TRUE,\n    core = detectCores(), seed = 5)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  },\n  file = \"ch05_b05_01\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.19 sec elapsed\n\n\nand we can investigate the priors as follows\n\nb5.1_prior <- brms::prior_draws(b5.1)\nglimpse(b5.1_prior)\n\nRows: 4,000\nColumns: 3\n$ Intercept <dbl> 0.0159007292, 0.0126893038, 0.0260227163, -0.0027384267, -0.…\n$ b         <dbl> -0.220417530, 0.453714302, 0.106990427, 0.701417541, 0.89347…\n$ sigma     <dbl> 0.15096080, 1.60456102, 0.45126734, 5.91524548, 0.59806596, …\n\n\nget the fitted \\(\\mu_i\\) by using a sequence of median age marriage of length 30 from the min(MedianAgeMarriage) to max(MedianAgeMarriage).\n\nfitted <- list()\nfitted$newdata <- data.frame(\n  A = seq_range(d$A, n = 30, pretty = TRUE)\n)\nfitted$data <- fitted$newdata %>%\n  fitted(object = b5.1, newdata = ., probs = c(0.055, 0.945)) |>\n  as_tibble() |>\n  bind_cols(fitted$newdata)\n\nand plot them\n\np <- list()\np$age <- ggplot(data = d, mapping = aes(x = A, y = D)) +\n  geom_smooth(data = fitted$data,\n              aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),\n              stat = \"identity\",\n              fill = \"olivedrab1\", color = \"olivedrab4\", alpha = 1, size = 1/2) +\n  geom_point(aes(color = as.logical(South))) +\n  scale_x_continuous(breaks = scales::breaks_extended(n = 7),\n                     labels = function(x) {\n                       out <- inv.scale(x, \n                                      center = mean(d$MedianAgeMarriage), \n                                      scale = sd(d$MedianAgeMarriage))\n                       scales::label_number(accuracy = 0.1)(out)\n                     }) +\n  scale_y_continuous(breaks = scales::breaks_extended(n = 7),\n                     labels = function(x) {\n                       out <- inv.scale(x, \n                                      center = mean(d$Divorce), \n                                      scale = sd(d$Divorce))\n                       scales::label_number(accuracy = 0.1)(out)\n                     }) +\n  scale_color_paletteer_d(\"calecopal::kelp1\", direction = -1) +\n  theme_minimal() +\n  theme(legend.position = c(0.85, 0.85),\n        title = element_text(color = \"midnightblue\")) +\n  labs(title = \"Divorce rate vs Median Marriage age\",\n       color = \"South\",\n       x = \"Median age\", y = \"Divorce rate\")\n# p$age\n\nThe model for regressing the divorce rate \\(D\\) on the marriage rate \\(M\\) is\n\\[\n\\begin{align*}\nD_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_M \\cdot M_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_A &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nb5.2 <- xfun::cache_rds({\n  out <- brm(\n    data = d,\n    formula = D ~ 1 + M,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 0.02), class = Intercept),\n      prior(normal(0, 0.5), class = b),\n      prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4,\n    sample_prior = TRUE,\n    core = detectCores(), seed = 5)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  },\n  file = \"ch05_b05_02\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.17 sec elapsed\n\n\n\nsummary(b5.2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: D ~ 1 + M \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.02    -0.04     0.04 1.00     3949     2924\nM             0.35      0.12     0.10     0.59 1.00     4286     2557\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.94      0.10     0.78     1.16 1.00     3461     2696\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nfitted <- list()\nfitted$newdata <- data.frame(\n  M = seq_range(d$M, n = 30, pretty = TRUE)\n)\nfitted$data <- fitted$newdata %>%\n  fitted(object = b5.2, newdata = ., probs = c(0.055, 0.945)) |>\n  as_tibble() |>\n  bind_cols(fitted$newdata)\n\nand plot them\n\np$marriage <- ggplot(data = d, mapping = aes(x = M, y = D)) +\n  geom_smooth(data = fitted$data,\n              aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),\n              stat = \"identity\",\n              fill = \"springgreen1\", color = \"springgreen4\", alpha = 1, size = 1/2) +\n  geom_point(aes(color = as.logical(South))) +\n  scale_x_continuous(breaks = scales::breaks_extended(n = 7),\n                     labels = function(x) {\n                       out <- inv.scale(x,\n                                      center = mean(d$Marriage),\n                                      scale = sd(d$Marriage))\n                       scales::label_number(accuracy = 0.1)(out)\n                     }) +\n  scale_y_continuous(breaks = scales::breaks_extended(n = 7),\n                     labels = function(x) {\n                       out <- inv.scale(x,\n                                      center = mean(d$Divorce),\n                                      scale = sd(d$Divorce))\n                       scales::label_number(accuracy = 0.1)(out)\n                     }) +\n  scale_color_paletteer_d(\"calecopal::kelp1\", direction = -1) +\n  theme_minimal() +\n  theme(legend.position = c(0.85, 0.85),\n        title = element_text(color = \"midnightblue\")) +\n  labs(title = \"Divorce rate vs Marriage rate\",\n       color = \"South\",\n       x = \"Marriage rate\", y = \"Divorce rate\")\n# p$marriage\n\n\nwrap_plots(p, guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n5.1.1 Think before you regress\n\np <- list()\np$coord <- data.frame(\n  name = c(\"A\", \"D\", \"M\"),\n  x = c(1, 2, 3),\n  y = c(2, 1, 2)\n)\np$dag1 <- ggdag::dagify(M ~ A,\n                      D ~ A + M,\n                      coords = p$coord) |>\n  ggdag::ggdag(node_size = 8) +\n  ggthemes::theme_solid(fill = \"snow2\")\np$plot1 <- ggplot(p$dag1$data, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name)) +\n  scale_color_paletteer_d(\"calecopal::kelp1\") +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  geom_dag_text(color = \"midnightblue\") +\n  geom_dag_edges(edge_color = \"midnightblue\") +\n  ggthemes::theme_solid(fill = \"snow2\") +\n  theme(legend.position = \"none\")\n\np$dag2 <- ggdag::dagify(M ~ A,\n                      D ~ A,\n                      coords = p$coord) |>\n  ggdag::ggdag(node_size = 8) +\n  ggthemes::theme_solid(fill = \"snow2\")\np$plot2 <- ggplot(p$dag2$data, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name)) +\n  scale_color_paletteer_d(\"calecopal::kelp1\") +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  geom_dag_text(color = \"midnightblue\") +\n  geom_dag_edges(edge_color = \"midnightblue\") +\n  ggthemes::theme_solid(fill = \"snow2\") +\n  theme(legend.position = \"none\")\n\np$plot1 | p$plot2\n\n\n\n\n\n\n5.1.2 Testable implications\n\ndag <- \"dag{ D <- A -> M }\"\ndagitty::dagitty(dag) |>\n  dagitty::impliedConditionalIndependencies()\n\nD _||_ M | A\n\n\n\ndag <- \"dag{ D <- A -> M -> D }\"\ndagitty::dagitty(dag) |>\n  dagitty::impliedConditionalIndependencies()\nmessage(\"This returns NULL because there are no conditional independencies\")\n\nThis returns NULL because there are no conditional independencies\n\n\n\n\n5.1.3 Multiple regression notation\nThe model with median age and marriage rate, both standardized.\n\nThe \\(+\\) in the model can be interpreted as the divorce rate is a function of the marriage rate OR the median age of marriage\n\n\\[\n\\begin{align*}\nD_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_M \\cdot M_i + \\beta_A \\cdot A_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\beta_2 &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(0, 1)\n\\end{align*}\n\\]\n\n\n5.1.4 Approximating the posterior\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nb5.3 <- xfun::cache_rds({\n  out <- brm(\n    data = d,\n    formula = D ~ 1 + M + A,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 0.2), class = Intercept),\n      prior(normal(0, 0.5), class = b, coef = \"M\"),\n      prior(normal(0, 0.5), class = b, coef = \"A\"),\n      prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = detectCores(),\n    core = detectCores(), seed = 5)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  },\n  file = \"ch05_b05_03\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.18 sec elapsed\n\n\n\nsummary(b5.3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: D ~ 1 + M + A \n   Data: d (Number of observations: 50) \n  Draws: 8 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 8000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.00      0.10    -0.20     0.20 1.00     7228     5197\nM            -0.06      0.16    -0.37     0.25 1.00     6010     5565\nA            -0.61      0.16    -0.91    -0.29 1.00     6080     5344\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.83      0.09     0.68     1.02 1.00     6973     5405\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\ntidybayes::get_variables(b5.3)\n\n [1] \"b_Intercept\"   \"b_M\"           \"b_A\"           \"sigma\"        \n [5] \"lprior\"        \"lp__\"          \"accept_stat__\" \"stepsize__\"   \n [9] \"treedepth__\"   \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\nb5.3 |>\n  gather_draws(b_Intercept, b_M, b_A) |>\n  ggplot(aes(x = .value, y = .variable)) +\n  stat_pointinterval(point_interval = mean_qi,\n                     .width = c(0, 0.055, 0.945, 1),\n                     fatten_point = 2,\n                     color = \"tan4\") +\n  ggthemes::theme_hc() +\n  labs(title = deparse1(b5.3$formula$formula),\n       y = NULL)\n\n\n\n\n\n\n5.1.5 Plotting multivariate posteriors\n3 main plots are used\n\nPredictor residual plots\nPosterior prediction plots\nCounterfactual plots\n\n\n5.1.5.1 Predictor residual plots\n\n5.1.5.1.1 Marriage rate residuals\n\n# load data\ndata(\"WaffleDivorce\")\nd <- WaffleDivorce\n# standardize the variables\nd <- d |>\n  mutate(A = scale(as.vector(MedianAgeMarriage)),\n         M = scale(as.vector(Marriage)),\n         D = scale(as.vector(Divorce)))\n# glimpse(d)\n\nWe compute marriage rate in terms of median age of marriage which is the model\n\\[\n\\begin{align*}\nM_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot A_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nfit this model\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb5.4 <- xfun::cache_rds({brm(\n  data = d,\n  formula = M ~ 1 + A,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sigma)\n  ),\n  iter = 2000, warmup = 100, chains = 4, core = 4,\n  seed = 5)\n  },\n  file = \"ch05_b05_04\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.2 sec elapsed\n\n\n\nsummary(b5.4)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: M ~ 1 + A \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 100; thin = 1;\n         total post-warmup draws = 7600\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.09    -0.18     0.18 1.00     9909     5930\nA            -0.69      0.10    -0.89    -0.50 1.00     7018     5473\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.71      0.07     0.58     0.87 1.00     6719     5532\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand again we get the fitted \\(\\mu_i\\) to plot them\n\n# get the fit on the original data and\n# add columns to original data to prepare for the plot\nfitted <- list()\nfitted$data <- fitted(b5.4) |>\n  as.data.frame() |>\n  bind_cols(d[, c(\"D\", \"A\", \"M\")]) |>\n  mutate(resid = M - Estimate)\n# str(fitted$data)\n\nand plot the divorce against the marriage rate residuals, free from the age effect\n\np <- list()\ncolr <- unclass(paletteer::paletteer_d(\"calecopal::buow\"))\np$A <- ggplot(data = fitted$data, aes(x = A, y = M)) +\n  geom_point(aes(color = D)) +\n  geom_segment(aes(xend = A, yend = Estimate), size = 0.5, color = \"pink\") +\n  geom_line(aes(y = Estimate), color = \"pink4\") +\n  scale_color_gradientn(colors = colr) +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Median age (standardized)\",\n       y = \"Marriage rate (standardized)\",\n       title = \"Residual plot:  Marriage rate vs Median age\")\n# p$A\n\nso now we use the residuals of the marriage rate to represent marriage free of any influence by the median age of marriage\nWe use the residuals() function which is an alias of predictive_error() but don’t use predictive_error() as it return the error for every posterior sample and every observation. We just want the expected for every initial observations of \\(D\\).\nand so we can now plot the divorce against the marriage rate residuals\n\ncolr <- unclass(paletteer::paletteer_d(\"calecopal::buow\"))\np$B <- ggplot(data = fitted$data, aes(x = resid, y = D)) +\n  stat_smooth(method = \"lm\", fullrange = TRUE, fill = \"wheat\") +\n  geom_point(aes(color = D)) +\n  scale_color_gradientn(colors = colr) +\n  scale_x_continuous(breaks = scales::breaks_width(width = 0.5)) +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"residual of marriage rate\",\n       y = \"divorce\",\n       title = \"Predictor residual plot: Divorce vs Marriage rate\")\n# p$B\n\nwhich allows us to conclude that the *marriage rate seems to have little impact on the divorce rate.\n\n# cowplot::plot_grid(b5.4_pA , b5.4_pB)\n(p$A | p$B) +\n  plot_annotation(\"Fig 5.4: Divorce vs Residual marriage rate\") &\n  theme(title = element_text(color = \"midnightblue\"))\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n5.1.5.2 Posterior prediction plots\nThe textbook mentions posterior predictions but the plot in figure 5.5 sems to have the intervals of the fitted values.\n\npredict <- list()\npredict$data <- predict(b5.3) |>\n  as.data.frame() |>\n  mutate(D = d$D)\n# str(predict$data)\n\n\nggplot(data = predict$data, aes(x = D, y = Estimate)) +\n  geom_point(color = \"firebrick4\") +\n  geom_linerange(aes(ymin = Estimate - Est.Error, ymax = Estimate + Est.Error),\n                 color = \"firebrick3\") +\n  geom_abline(slope = 1, linetype = \"dashed\", color = \"darkorchid\") +\n  theme_classic() +\n  labs(title = \"Posterior Predictive Plot: Divorce rate\",\n       x = \"observed divorce\",\n       y = \"predicted divorce\")\n\n\n\n\n\n\n5.1.5.3 Counterfactual plots\nWe are using the same DAG as above\n\np <- list()\np$coord <- data.frame(\n  name = c(\"A\", \"D\", \"M\"),\n  x = c(1, 2, 3),\n  y = c(2, 1, 2)\n)\np$dag <- ggdag::dagify(M ~ A,\n                       D ~ A + M,\n                      coords = p$coord) |>\n  ggdag::ggdag(node_size = 8) +\n  ggthemes::theme_solid(fill = \"snow2\")\nggplot(p$dag$data, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name)) +\n  scale_color_paletteer_d(\"calecopal::kelp1\") +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  geom_dag_text(color = \"midnightblue\") +\n  geom_dag_edges(edge_color = \"midnightblue\") +\n  ggthemes::theme_solid(fill = \"aliceblue\") +\n  theme(legend.position = \"none\")\n\n\n\n\nIMPORTANT: The full model implies 2 effects, \\(A\\) on \\(M\\) and \\(D\\) and \\(A\\) on \\(M\\). In other words 2 structural equations are involved or expressed differently, 2 formulas in brms.\n\nd_model <- brms::bf(D ~ 1 + A + M)\nm_model <- brms::bf(M ~ 1 + A)\n\nThe set_rescor(FALSE) indicates that we do not want brms to add the residual correlation between \\(D\\) and \\(M\\).\nAlso the argument resp is used to identify the response.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb5.3a <- xfun::cache_rds({brm(\n  data = d, \n  family = gaussian,\n  d_model + m_model + set_rescor(FALSE),\n      prior = c(prior(normal(0, 0.2), class = Intercept, resp = D),\n                prior(normal(0, 0.5), class = b, resp = D),\n                prior(exponential(1), class = sigma, resp = D),\n                prior(normal(0, 0.2), class = Intercept, resp = M),\n                prior(normal(0, 0.5), class = b, resp = M),\n                prior(exponential(1), class = sigma, resp = M)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 5)\n  }, file = \"ch05_b05_03a\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.23 sec elapsed\n\n\n\nsummary(b5.3a)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: D ~ 1 + A + M \n         M ~ 1 + A \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nD_Intercept    -0.00      0.10    -0.20     0.20 1.00     5757     3111\nM_Intercept     0.00      0.09    -0.17     0.17 1.00     5386     2960\nD_A            -0.60      0.16    -0.91    -0.29 1.00     3381     2726\nD_M            -0.06      0.16    -0.36     0.26 1.00     3535     2749\nM_A            -0.69      0.10    -0.88    -0.49 1.00     5972     2664\n\nFamily Specific Parameters: \n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_D     0.83      0.09     0.68     1.02 1.00     5459     3301\nsigma_M     0.71      0.07     0.58     0.88 1.00     5342     2919\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe manipulate \\(M\\) \\(M=0\\) and predicting \\(D\\) with \\(A\\) with this new counterfactual \\(M\\).\n\npredict <- list()\npredict$newdata <- data.frame(\n  A = seq_range(d$A, n = 30L, pretty = TRUE),\n  M = 0)\n# NOTE: the argument resp = \"D\" is important to select the right model\npredict$data <- predict(b5.3a, resp = \"D\", newdata = predict$newdata) |>\n  data.frame() |>\n  bind_cols(predict$newdata)\n# str(predict$data)\np <- list()\np$D <- ggplot(predict$data,\n              aes(x = A, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  # coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) +\n  geom_smooth(stat = \"identity\", fill = \"palegoldenrod\", color = \"peru\") +\n  # coord_cartesian(ylim = c(-2, 2)) +\n  ggthemes::theme_clean() +\n  labs(title = \"Total counterfactual effect of A on D\",\n       x = \"Manipulated A\", y = \"Counterfactual D\")\np$D\n\n\n\n\nTo estimate the influence of \\(A\\) on \\(M\\) we only need to regress \\(A\\) on \\(M\\) as there is no other known factor influencing \\(M\\).\n\npredict <- list()\npredict$newdata <- data.frame(A = seq_range(d$A, n = 30L, pretty = TRUE))\npredict$data <- predict(b5.3a, resp = \"M\", newdata = predict$newdata) |>\n  data.frame() |>\n  bind_cols(predict$newdata)\np$M <- ggplot(predict$data, aes(x = A, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_smooth(stat = \"identity\", fill = \"khaki\", color = \"khaki4\") +\n  # coord_cartesian(ylim = c(-2, 2)) +\n  ggthemes::theme_clean() +\n  labs(title = \"Single effect of A on M\",\n       x = \"A\", y = \"M\") \n\n\nwrap_plots(p)"
  },
  {
    "objectID": "ch05_multivariate.html#masked-relationship",
    "href": "ch05_multivariate.html#masked-relationship",
    "title": "5  Multivariate Linear Models",
    "section": "5.2 Masked relationship",
    "text": "5.2 Masked relationship\nLoad data and look at the pair plot. We use GGally::pairs() which gives better information and formatting options.\nBut first, as mentioned on p. 136, we need to remove missing values which cause problems when plotting and in modeling.\n\ndata(milk)\nd <- milk |>\n  as.data.frame() |>\n  tidyr::drop_na() |>\n  mutate(K = as.vector(scale(kcal.per.g)),\n         N = as.vector(scale(neocortex.perc)),\n         M = as.vector(scale(log(mass))))\nskimr::skim(d)\n\n\nData summary\n\n\nName\nd\n\n\nNumber of rows\n17\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nclade\n0\n1\nFALSE\n4\nNew: 7, Ape: 6, Old: 3, Str: 1\n\n\nspecies\n0\n1\nFALSE\n17\nA p: 1, Alo: 1, Cal: 1, Cal: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nkcal.per.g\n0\n1\n0.66\n0.17\n0.46\n0.49\n0.62\n0.80\n0.97\n▇▁▃▂▃\n\n\nperc.fat\n0\n1\n36.06\n14.71\n3.93\n27.18\n37.78\n50.49\n55.51\n▂▅▅▇▇\n\n\nperc.protein\n0\n1\n16.26\n5.60\n7.37\n11.68\n15.80\n20.85\n25.30\n▆▇▇▃▇\n\n\nperc.lactose\n0\n1\n47.68\n13.59\n27.09\n37.80\n46.88\n55.20\n70.77\n▇▇▃▇▆\n\n\nmass\n0\n1\n16.64\n23.58\n0.12\n1.55\n5.25\n33.11\n79.43\n▇▁▂▁▁\n\n\nneocortex.perc\n0\n1\n67.58\n5.97\n55.16\n64.54\n68.85\n71.26\n76.30\n▃▃▆▇▆\n\n\nK\n0\n1\n0.00\n1.00\n-1.14\n-0.97\n-0.22\n0.82\n1.81\n▇▁▃▂▃\n\n\nN\n0\n1\n0.00\n1.00\n-2.08\n-0.51\n0.21\n0.62\n1.46\n▃▃▆▇▆\n\n\nM\n0\n1\n0.00\n1.00\n-1.87\n-0.55\n0.08\n1.03\n1.49\n▂▃▇▁▇\n\n\n\n\n# it should give us a dataframe with 17 rows\nstopifnot(nrow(d) == 17)\n# glimpse(d)\n\n\nGGally::ggpairs(d[, c(\"K\", \"N\", \"M\")]) +\n  ggthemes::theme_fivethirtyeight()\n\n\n\n\nthe model\n\n5.2.1 Model 5.5\n\\[\n\\begin{align*}\nK &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_N \\cdot N_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_N &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand the fit is\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb5.5 <- xfun::cache_rds({brm(\n  data = d,\n  formula = K ~ 1 + N,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sigma)\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n  seed = 5)\n  }, file = \"ch05_b05_05\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.23 sec elapsed\n\n\n\nprint(b5.5, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: K ~ 1 + N \n   Data: d (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept   -0.001     0.154   -0.297    0.298 1.002     3748     2970\nN            0.119     0.233   -0.348    0.572 1.000     3479     2764\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    1.039     0.192    0.749    1.487 1.000     3448     2054\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand show the coefficient plot\n\ntidybayes::get_variables(b5.5)\n\n [1] \"b_Intercept\"   \"b_N\"           \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\npost <- list()\npost$long <- b5.5 |>\n  tidybayes::gather_draws(b_Intercept, b_N, sigma)\nggplot(post$long, aes(x = .value)) +\n  stat_halfeye(aes(fill = .variable), point_interval = mean_hdi, .width = 0.89) +\n  scale_fill_paletteer_d(palette = \"calecopal::dudleya\") +\n  scale_x_continuous(breaks = scales::breaks_width(width = 0.25)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_wrap(. ~ .variable, ncol = 1, scales = \"fixed\")\n\n\n\n\nfit for different confidence intervals\n\nfitted <- list()\nfitted$newdata <- data.frame(N = seq_range(d$N, n = 30L, pretty = TRUE))\nfitted$data <- fitted$newdata %>%\n  fitted(object = b5.5, newdata = ., probs = c(0.025, 0.975, 0.25, 0.75)) |>\n  as_tibble() |>\n  bind_cols(fitted$newdata)\n\nand plot the fits with different variables and counterfactuals. We use this plotting function\n\nplot_milk <- function(original, fitted, x_var = \"N\", y_var = \"Estimate\",\n                      clrs = list(fill1 = \"steelblue1\", fill2 = \"steelblue\",\n                                  line = \"steelblue4\", point = \"plum4\")) {\n  ggplot(fitted, aes(x = .data[[x_var]], y = .data[[y_var]])) +\n    geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = clrs$fill1) +\n    geom_ribbon(aes(ymin = Q25, ymax = Q75), fill = clrs$fill2) +\n    geom_line(color = clrs$line, size = 1) +\n    geom_point(data = original, aes(x = .data[[x_var]], y = K), \n             color = clrs$point, size = 2) +\n    scale_x_continuous(breaks = scales::breaks_width(width = 0.5)) +\n    scale_y_continuous(breaks = scales::breaks_width(width = 0.5)) +\n    coord_fixed(ratio = 9/16) +\n    ggthemes::theme_few()\n  }\n\n\np <- list()\np$N <- plot_milk(original = b5.5$data, fitted = fitted$data, x_var = \"N\",\n                 clrs = list(fill1 = \"steelblue1\", fill2 = \"steelblue\",\n                                  line = \"steelblue4\", point = \"plum4\")) +\n  labs(title = \"Model 5.5\", subtitle = deparse1(b5.5$formula$formula),\n       x = \"neocrotex percent (std)\", y = \"kilocal per g (std)\")\n\n\n\n5.2.2 Model 5.6\n\\[\n\\begin{align*}\nK &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_M \\cdot M_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_N &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand the fit is\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb5.6 <- xfun::cache_rds({brm(\n  data = d,\n  formula = K ~ 1 + M,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n  seed = 5)\n  }, file = \"ch05_b05_06\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.23 sec elapsed\n\n\n\nprint(b5.6, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: K ~ 1 + M \n   Data: d (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept    0.001     0.155   -0.301    0.301 1.001     3730     2997\nM           -0.280     0.227   -0.719    0.164 1.001     3431     2721\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.994     0.190    0.702    1.439 1.000     3272     2669\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand the plot for the kilo vs mass\n\ntidybayes::get_variables(b5.6)\n\n [1] \"b_Intercept\"   \"b_M\"           \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\nfitted <- list()\nfitted$newdata <- data.frame(M = seq_range(d$M, n = 30L, pretty = TRUE))\nfitted$data <- fitted$newdata %>%\n  fitted(object = b5.6, newdata = ., probs = c(0.025, 0.975, 0.25, 0.75)) |>\n  as_tibble() |>\n  bind_cols(fitted$newdata)\n\np$M <- plot_milk(original = b5.6$data, fitted = fitted$data, x_var = \"M\",\n                 clrs = list(fill1 = \"plum1\", fill2 = \"plum\",\n                                  line = \"plum4\", point = \"steelblue4\")) +\n    labs(title = \"Model 5.6\", subtitle = deparse1(b5.6$formula$formula),\n       x = \"body mass (std of log)\", y = \"kilocal per g (std)\")\n\n\n\n5.2.3 Model 5.7\nwe now add neocortex and log mass together to see their mutual effect,\n\\[\n\\begin{align*}\nK_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_N \\cdot N_i + \\beta_M \\cdot log(M_i) \\\\\n\\alpha &\\sim \\mathcal{N}(0, 0.2) \\\\\n\\beta_N &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\beta_M &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand the fit is\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nb5.7 <- xfun::cache_rds({brm(\n  data = d,\n  formula = K ~ 1 + N + M,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b),\n    prior(cauchy(0, 0.5), class = sigma)\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n  seed = 5)\n  }, file = \"ch05_b05_07\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.32 sec elapsed\n\n\n\nprint(b5.7, digits = 3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: K ~ 1 + N + M \n   Data: d (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept    0.001     0.139   -0.277    0.267 1.001     3021     2364\nN            0.577     0.263    0.030    1.057 1.002     2108     2531\nM           -0.680     0.262   -1.162   -0.131 1.002     2085     2112\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.789     0.164    0.541    1.192 1.001     2204     2234\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nget the conterfactual data holding \\(M = 0\\)\n\nfitted <- list()\nfitted$newdata <- data.frame(\n  N = seq_range(d$N, n = 30L, pretty = TRUE),\n  M = 0\n)\nfitted$data <- fitted$newdata %>%\n  fitted(object = b5.7, newdata = ., probs = c(0.025, 0.975, 0.25, 0.75)) |>\n  as_tibble() |>\n  bind_cols(fitted$newdata)\n\nand the plot\n\np$N_counter <- plot_milk(original = b5.7$data, fitted = fitted$data, x_var = \"N\",\n                 clrs = list(fill1 = \"steelblue1\", fill2 = \"steelblue\",\n                                  line = \"steelblue4\", point = \"plum4\")) +\n  labs(title = \"Model 5.7 - Holding M = 0\", \n       subtitle = deparse1(b5.7$formula$formula),\n       x = \"neocrotex percent (std)\", y = \"kilocal per g (std)\")\n\nget the conterfactual data holding \\(N = 0\\)\n\nfitted <- list()\nfitted$newdata <- data.frame(\n  M = seq_range(d$M, n = 30L, pretty = TRUE),\n  N = 0\n)\nfitted$data <- fitted$newdata %>%\n  fitted(object = b5.7, newdata = ., probs = c(0.025, 0.975, 0.25, 0.75)) |>\n  as_tibble() |>\n  bind_cols(fitted$newdata)\n\nand the plot\n\np$M_counter <- plot_milk(original = b5.7$data, fitted = fitted$data, x_var = \"M\",\n                 clrs = list(fill1 = \"plum1\", fill2 = \"plum\",\n                                  line = \"plum4\", point = \"steelblue4\")) +\n    labs(title = \"Model 5.7 - Holding N = 0\", subtitle = deparse1(b5.7$formula$formula),\n       x = \"body mass (std of log)\", y = \"kilocal per g (std)\")\n\n\nwrap_plots(p)"
  },
  {
    "objectID": "ch05_multivariate.html#categorical-variables",
    "href": "ch05_multivariate.html#categorical-variables",
    "title": "5  Multivariate Linear Models",
    "section": "5.3 Categorical variables",
    "text": "5.3 Categorical variables\n\n5.3.1 Binary categories\nLoad data\n\ndata(\"Howell1\")\nd <- Howell1 |>\n  mutate(sex = factor(male))\nglimpse(d)\n\nRows: 544\nColumns: 5\n$ height <dbl> 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149…\n$ weight <dbl> 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.…\n$ age    <dbl> 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.…\n$ male   <int> 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, …\n$ sex    <fct> 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, …\n\n\nthe model\n\\[\n\\begin{align*}\nh_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{sex[i]} \\\\\n\\alpha &\\sim \\mathcal{N}(178, 20) \\\\\n\\beta_m &\\sim \\mathcal{N}(0, 10) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb5.8 <- xfun::cache_rds({brm(\n  data = d,\n  formula = height ~ 0 + sex,\n  family = gaussian,\n  prior = c(\n    prior(normal(178, 20), class = b),\n    prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n  seed = 5)\n  }, file = \"ch05_b05_08\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.36 sec elapsed\n\n\n\nsummary(b5.8)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 0 + sex \n   Data: d (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsex0   134.90      1.57   131.83   137.92 1.00     4095     3000\nsex1   142.56      1.66   139.33   145.86 1.00     4365     3015\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    26.77      0.79    25.27    28.34 1.00     4864     2451\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nRead the important comment in section 5.3.1 when using \\(\\alpha\\). \\(\\alpha\\) now represents the average of women and the male heights is more variable because it relates to 2 parameters instead of 1.\n\n\n\n5.3.2 Many categories\n\nSee the overthinking box in section 5.3.2 on how to reparametrize. Very nice.\n\nLoad data, standardize and make sure clade is a factor. > With brms there is no need to create the contrasts. We simply make sure that *clade* is a factor.brms` will create the contrasts by itself.\n\ndata(\"milk\")\nd <- milk |> \n  mutate(K = as.vector(scale(kcal.per.g)))\n# skimr::skim(d)\nstopifnot(is.factor(d$clade))\n\nthe model\n\\[\n\\begin{align*}\nK_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{clade[i]} \\\\\n\\alpha_j &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nb5.9 <- xfun::cache_rds({brm(\n  data = d,\n  formula = K ~ 0 + clade,\n  family = gaussian,\n  prior = c(\n    prior(normal(0, 0.5), class = b),\n    prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n  seed = 5)\n  }, file = \"ch05_b05_09\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.32 sec elapsed\n\n\n\nsummary(b5.9)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: K ~ 0 + clade \n   Data: d (Number of observations: 29) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ncladeApe               -0.46      0.24    -0.92     0.02 1.00     4302     2750\ncladeNewWorldMonkey     0.35      0.25    -0.15     0.83 1.00     4573     2474\ncladeOldWorldMonkey     0.64      0.28     0.08     1.15 1.00     5032     3030\ncladeStrepsirrhine     -0.55      0.30    -1.12     0.05 1.00     4299     2852\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.80      0.12     0.61     1.07 1.00     3575     2858\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\ntidybayes::get_variables(b5.9)\n\n [1] \"b_cladeApe\"            \"b_cladeNewWorldMonkey\" \"b_cladeOldWorldMonkey\"\n [4] \"b_cladeStrepsirrhine\"  \"sigma\"                 \"lprior\"               \n [7] \"lp__\"                  \"accept_stat__\"         \"stepsize__\"           \n[10] \"treedepth__\"           \"n_leapfrog__\"          \"divergent__\"          \n[13] \"energy__\"             \n\n\nand plot the result\n\ntidybayes::gather_draws(b5.9, b_cladeApe, b_cladeNewWorldMonkey, \n                        b_cladeOldWorldMonkey, b_cladeStrepsirrhine) |>\n  mean_hdi(.width = 0.89) |>\n  ggplot(aes(x = .value, xmin = .lower, xmax = .upper, y = .variable)) +\n  geom_vline(xintercept = 0, colour = \"darkgoldenrod\") +\n  geom_pointinterval(fatten_point = 2.5, color = \"yellowgreen\", size = 5) +\n  ggthemes::theme_clean() + \n  labs(title = \"Expected kcal by clade\", x = \"kcal (std)\", y = NULL)"
  },
  {
    "objectID": "ch05_multivariate.html#other-ways-to-modify-categorical-variables",
    "href": "ch05_multivariate.html#other-ways-to-modify-categorical-variables",
    "title": "5  Multivariate Linear Models",
    "section": "5.4 Other ways to modify categorical variables",
    "text": "5.4 Other ways to modify categorical variables\nSee Kurz (2020)"
  },
  {
    "objectID": "ch05_multivariate.html#summary",
    "href": "ch05_multivariate.html#summary",
    "title": "5  Multivariate Linear Models",
    "section": "5.5 Summary",
    "text": "5.5 Summary\n\n\n\n\nKurz, Solomon. 2020. Statistical Rethinking with Brms. 2nd ed. https://bookdown.org/content/4857/."
  },
  {
    "objectID": "ch06_scm.html",
    "href": "ch06_scm.html",
    "title": "6  Structural Causal Models",
    "section": "",
    "text": "This is a ver y important point, in the intro to chapter 6."
  },
  {
    "objectID": "ch06_scm.html#multicollinearity",
    "href": "ch06_scm.html#multicollinearity",
    "title": "6  Structural Causal Models",
    "section": "6.1 Multicollinearity",
    "text": "6.1 Multicollinearity\nMulticollinearity means a very strong association between 2 or more predictor variables.\n\n6.1.1 Multicollinear legs\nCreate the data\n\nn <- 100\nset.seed(6)\n\nd <- \n  tibble(height   = rnorm(n, mean = 10, sd = 2),\n         leg_prop = runif(n, min = 0.4, max = 0.5)) |> \n  mutate(leg_left  = leg_prop * height + rnorm(n, mean = 0, sd = 0.02),\n         leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02))\n\nwhich has the following correlations\n\nGGally::ggcorr(d[, c(\"leg_left\", \"leg_right\")],\n               color = \"darkgreen\", nbreaks = 10, label = TRUE, \n               label_round = 4, label_color = \"midnightblue\", direction = -1) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Correlations between parameters\")\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nWarning: Ignoring unknown parameters: direction\n\n\n\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"110 secs.\"))\nb6.1 <- xfun::cache_rds({brm(data = d, \n      family = gaussian,\n      height ~ 1 + leg_left + leg_right,\n      prior = c(prior(normal(10, 100), class = Intercept),\n                prior(normal(2, 10), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 6)},\n  file = \"ch06_b06_01\")\ntictoc::toc()\n\nrun time of 110 secs., use the cache.: 0.14 sec elapsed\n\n\n\nsummary(b6.1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + leg_left + leg_right \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.81      0.30     0.21     1.40 1.00     4054     2945\nleg_left      1.32      2.21    -3.17     5.73 1.00     1269     1478\nleg_right     0.71      2.22    -3.71     5.23 1.00     1268     1483\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.63      0.05     0.55     0.73 1.00     2265     2249\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\ntidybayes::get_variables(b6.1)\n\n [1] \"b_Intercept\"   \"b_leg_left\"    \"b_leg_right\"   \"sigma\"        \n [5] \"lprior\"        \"lp__\"          \"accept_stat__\" \"stepsize__\"   \n [9] \"treedepth__\"   \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\n\ntidybayes::gather_draws(b6.1, b_Intercept, b_leg_left, b_leg_right, sigma) |>\n  mean_hdi(.width = 0.95) |>\n  ggplot(aes(x = .value, xmin = .lower, xmax = .upper, y = .variable, color = .variable)) +\n  geom_pointinterval() +\n  ggrepel::geom_text_repel(aes(label = round(.value, 2))) +\n  scale_color_paletteer_d(\"Manu::Kereru\") +\n  ggthemes::theme_few() +\n  theme(title = element_text(color = \"midnightblue\"),\n        legend.position = \"none\",\n        panel.border = element_blank()) +\n  labs(title = \"Leg model\",\n       x = \"value\", y = NULL)\n\n\n\n\n\n\n6.1.2 Multicollinear milk\n\ndata(milk)\nd <- milk\nd <- d |>\n  mutate(K = as.vector(scale(kcal.per.g)),\n         `F` = as.vector(scale(perc.fat)),\n         L = as.vector(scale(perc.lactose)))\n\n\nGGally::ggscatmat(d, columns = c(\"K\", \"F\", \"L\")) +\n  # scale_color_paletteer_d(palette = \"Manu::Kereru\") +\n  theme_minimal()\n\n\n\n\n\n# k regressed on f\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nb6.3 <- xfun::cache_rds({brm(data = d, \n      family = gaussian,\n      K ~ 1 + `F`,\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 6)},\n  file = \"ch06_b06_03\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.19 sec elapsed\n\n\n\n# k regressed on l\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"10 secs.\"))\nb6.4 <- xfun::cache_rds({update(b6.3,\n         newdata = d,\n         formula = K ~ 1 + L, seed = 6)},\n  file = \"ch06_b06_04\")\ntictoc::toc()\n\nrun time of 10 secs., use the cache.: 0.11 sec elapsed\n\n\nand the coefficients are\n\nposterior_summary(b6.3) |> round(digits = 2)\n\n            Estimate Est.Error   Q2.5  Q97.5\nb_Intercept     0.00      0.08  -0.17   0.17\nb_F             0.86      0.09   0.67   1.03\nsigma           0.49      0.07   0.37   0.64\nlprior         -1.59      0.34  -2.31  -1.01\nlp__          -22.09      1.28 -25.37 -20.64\n\n\n\nposterior_summary(b6.4) |> round(digits = 2)\n\n            Estimate Est.Error   Q2.5  Q97.5\nb_Intercept     0.00      0.07  -0.14   0.14\nb_L            -0.90      0.08  -1.06  -0.75\nsigma           0.41      0.06   0.32   0.55\nlprior         -1.65      0.30  -2.30  -1.12\nlp__          -17.38      1.25 -20.63 -15.91\n\n\nand the multivariate which shows that each variable has now a much larger variance caused by the colinearity.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nb6.5 <- xfun::cache_rds({brm(data = d,\n      family = gaussian,\n      K ~ 1 + `F` + L,\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 6)},\n  file = \"ch06_b06_05\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.25 sec elapsed\n\n\n\nposterior_summary(b6.5) |> round(digits = 2)\n\n            Estimate Est.Error   Q2.5  Q97.5\nb_Intercept     0.00      0.07  -0.15   0.14\nb_F             0.25      0.20  -0.13   0.66\nb_L            -0.67      0.20  -1.05  -0.27\nsigma           0.41      0.06   0.32   0.55\nlprior         -1.42      0.42  -2.50  -0.88\nlp__          -17.28      1.50 -20.89 -15.42"
  },
  {
    "objectID": "ch06_scm.html#post-treatment-bias",
    "href": "ch06_scm.html#post-treatment-bias",
    "title": "6  Structural Causal Models",
    "section": "6.2 Post-treatment bias",
    "text": "6.2 Post-treatment bias\n\n# how many plants would you like?\nn <- 100\nset.seed(7)\nd <- \n  tibble(h0        = rnorm(n, mean = 10, sd = 2), \n         treatment = rep(0:1, each = n / 2),\n         fungus    = rbinom(n, size = 1, prob = .5 - treatment * 0.4),\n         h1        = h0 + rnorm(n, mean = 5 - 3 * fungus, sd = 1))\nskimr::skim(d)\n\n\nData summary\n\n\nName\nd\n\n\nNumber of rows\n100\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nh0\n0\n1\n10.28\n1.92\n6.43\n8.88\n10.21\n11.44\n15.43\n▃▇▇▃▂\n\n\ntreatment\n0\n1\n0.50\n0.50\n0.00\n0.00\n0.50\n1.00\n1.00\n▇▁▁▁▇\n\n\nfungus\n0\n1\n0.30\n0.46\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nh1\n0\n1\n14.38\n2.41\n7.93\n13.12\n14.69\n15.87\n20.53\n▁▃▇▇▁\n\n\n\n\n\n\n6.2.1 A prior is born\n\nIf we center our prior for \\(p\\) on 1, that implies an expectation of no change in height. That is less than we know. But we would allow \\(p\\) to be less than 1, in case the experiment ges wrong. We also want to ensure \\(p>0\\).\n\nTherefore we use \\(p\\) with log-normal distribution\n\\[\n\\begin{align*}\nh_{1,i} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= h_{0,i} \\times p \\\\\np &\\sim \\mathcal{LogNormal}(0, 0.25) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nb6.6 <- xfun::cache_rds({\n  out <- brm(\n    data = d, \n    family = gaussian,\n    h1 ~ 0 + h0,\n    prior = c(prior(lognormal(0, 0.25), class = b, lb = 0),\n              prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 6)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))\n  out\n  },\n  file = \"ch06_b06_06\")\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 0.23 sec elapsed\n\n\n\nbrms::posterior_summary(b6.6)\n\n          Estimate  Est.Error        Q2.5       Q97.5\nb_h0      1.383651 0.01654117    1.351387    1.415997\nsigma     1.742914 0.12775331    1.508492    2.006601\nlprior   -2.444556 0.14628704   -2.760598   -2.184604\nlp__   -199.131304 0.98776814 -201.919589 -198.132103\n\n\nSo the increase is 1.38 relative to \\(h_0\\).\nNow including the treatment and fungus we have\n\\[\n\\begin{align*}\nh_{1,i} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= h_{0,i} \\times p \\\\\np &\\sim \\alpha + \\beta_1 treatment_i + \\beta_2 fungus_i \\\\\n\\alpha &\\sim \\mathcal{LogNormal}(0, 0.25) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\beta_2 &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nb6.7 <- xfun::cache_rds({\n  out <- brm(\n    data = d, \n    family = gaussian,\n    bf(h1 ~ h0 * (a + t * treatment + f * fungus),\n       a + t + f ~ 1, \n       nl = TRUE),\n    prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0),\n                prior(normal(0, 0.5), nlpar = t),\n                prior(normal(0, 0.5), nlpar = f),\n                prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 6)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch06_b06_07\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.44 sec elapsed\n\n\n\nbrms::posterior_summary(b6.7)\n\n                   Estimate  Est.Error          Q2.5        Q97.5\nb_a_Intercept    1.42326211 0.02424137    1.37512847    1.4694897\nb_t_Intercept    0.04446936 0.02957687   -0.01249662    0.1044173\nb_f_Intercept   -0.20371935 0.03277750   -0.26847845   -0.1413819\nsigma            1.33384353 0.09512225    1.15778260    1.5330524\nlprior          -3.09812524 0.20213247   -3.51192788   -2.7164356\nlp__          -173.07112190 1.39597643 -176.64959452 -171.3354407\n\n\nNow the effect of the treatment is almost non existent.\n\n\n6.2.2 Blocked by consequence\nThe problem is that the fungus is part of a chain between the treatment and the growth.\n\ndag_coord <- data.frame(\n  name = c(\"h0\", \"h1\", \"F\", \"T\"),\n  x = c(1, 2, 3, 4),\n  y = c(1, 1, 1, 1)\n)\ndag <- ggdag::dagify(h1 ~ h0, h1 ~ `F`, `F` ~ `T`,\n                      coords = dag_coord) |>\n  ggdag::ggdag(node_size = 8, text_col = \"yellow\") +\n  ggthemes::theme_solid(fill = \"snow2\")\ndag\n\n\n\n\nso now we redo the model but without the fungus effect.\n\\[\n\\begin{align*}\nh_{1,i} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= h_{0,i} \\times p \\\\\np &\\sim \\alpha + \\beta_1 treatment_i \\\\\n\\alpha &\\sim \\mathcal{LogNormal}(0, 0.25) \\\\\n\\beta_1 &\\sim \\mathcal{N}(0, 0.5) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nb6.8 <- xfun::cache_rds({\n  out <- brm(\n    data = d, \n    family = gaussian,\n    bf(h1 ~ h0 * (a + t * treatment),\n      a + t ~ 1, nl = TRUE),\n    prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0),\n                prior(normal(0, 0.5), nlpar = t),\n                prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 6)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch06_b06_08\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.22 sec elapsed\n\n\n\nsummary(b6.8)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: h1 ~ h0 * (a + t * treatment) \n         a ~ 1\n         t ~ 1\n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_Intercept     1.31      0.02     1.27     1.35 1.00     2050     2024\nt_Intercept     0.15      0.03     0.09     0.21 1.00     2138     2360\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.57      0.11     1.37     1.81 1.00     2699     2476\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand we now see more treatment effect."
  },
  {
    "objectID": "ch06_scm.html#collider-bias",
    "href": "ch06_scm.html#collider-bias",
    "title": "6  Structural Causal Models",
    "section": "6.3 Collider bias",
    "text": "6.3 Collider bias\n\n6.3.1 Collider of false sorrow\n\nd <- rethinking::sim_happiness(seed = 1977, N_years = 1000)\n# select age > 17 and rescale to [0, 1] and create indexed factor\n# creating factor makes it easer with brms\nd2 <- d |>\n  filter(age > 17) |>\n  mutate(A = scales::rescale(age, to = c(0, 1)),\n         mid = factor(married + 1, labels = c(\"single\", \"married\")))\nglimpse(d2)\n\nRows: 960\nColumns: 5\n$ age       <dbl> 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, …\n$ married   <dbl> 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, …\n$ happiness <dbl> -2.0000000, -1.7894737, -1.5789474, -1.3684211, -1.1578947, …\n$ A         <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.000…\n$ mid       <fct> single, single, married, single, single, single, married, ma…\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nb6.9 <- xfun::cache_rds({\n  out <- brm(\n    data = d2, \n    family = gaussian,\n    happiness ~ 0 + mid + A,\n    prior = c(prior(normal(0, 1), class = b, coef = midmarried),\n                prior(normal(0, 1), class = b, coef = midsingle),\n                prior(normal(0, 2), class = b, coef = A),\n                prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 6)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch06_b06_09\")\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 0.2 sec elapsed\n\n\n\nbrms::posterior_summary(b6.9)\n\n                  Estimate  Est.Error          Q2.5         Q97.5\nb_midsingle     -0.2360886 0.06276554    -0.3585190    -0.1139693\nb_midmarried     1.2576981 0.08489173     1.0925441     1.4227535\nb_A             -0.7470218 0.11251484    -0.9648072    -0.5258873\nsigma            0.9922772 0.02270052     0.9486625     1.0377774\nlprior          -5.3379200 0.11727085    -5.5848227    -5.1304462\nlp__         -1360.3803066 1.44212127 -1364.0379724 -1358.6465647\n\n\nThe fit finds that the effect of age on happiness is negative\nnow lets do it without the marriage factor\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"90 secs.\"))\nb6.10 <- xfun::cache_rds({\n  out <- brm(\n    data = d2,\n    family = gaussian,\n    happiness ~ 1 + A,\n    prior = c(prior(normal(0, 1), class = Intercept),\n                prior(normal(0, 2), class = b),\n                prior(exponential(1), class = sigma)),\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 6)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch06_b06_10\")\ntictoc::toc()\n\nrun time of 90 secs., use the cache.: 0.2 sec elapsed\n\n\n\nbrms::posterior_summary(b6.10)\n\n                 Estimate  Est.Error          Q2.5         Q97.5\nb_Intercept -7.860119e-04 0.07794572    -0.1488609     0.1563472\nb_A          2.175651e-03 0.13191281    -0.2591360     0.2579227\nsigma        1.216523e+00 0.02774757     1.1638998     1.2719130\nlprior      -3.750499e+00 0.02806359    -3.8070398    -3.6975066\nlp__        -1.553357e+03 1.19787955 -1556.4143153 -1551.9710955\n\n\nNow the age has no effect on happiness! When we include marriage, we include a spurious association.\n\n\n6.3.2 The haunted DAG"
  },
  {
    "objectID": "ch06_scm.html#confronting-counfounding",
    "href": "ch06_scm.html#confronting-counfounding",
    "title": "6  Structural Causal Models",
    "section": "6.4 Confronting counfounding",
    "text": "6.4 Confronting counfounding\nSee Overthinking box in section 6.4.3. Confounding occurs when\n\\[\nPr(Y \\mid X) \\neq Pr(Y \\mid do(X))\n\\]"
  },
  {
    "objectID": "ch06_scm.html#summary",
    "href": "ch06_scm.html#summary",
    "title": "6  Structural Causal Models",
    "section": "6.5 Summary",
    "text": "6.5 Summary"
  },
  {
    "objectID": "ch07_information.html",
    "href": "ch07_information.html",
    "title": "7  Ulysses’ Compass",
    "section": "",
    "text": "An important point to remember is mentioned in McElreath (2020), introduction of chapter 7."
  },
  {
    "objectID": "ch07_information.html#the-problem-with-parameters",
    "href": "ch07_information.html#the-problem-with-parameters",
    "title": "7  Ulysses’ Compass",
    "section": "7.1 The problem with parameters",
    "text": "7.1 The problem with parameters\n\\(R^2\\) is not the right way to do it.\n\\[\n\\begin{align*}\nR^2 &= \\frac{var(outcome) - var(residuals)}{var(outcome)} =\n1 - \\frac{var(residuals)}{var(outcome)} \\\\\n&= 1- \\frac{SSR}{SST}\n\\end{align*}\n\\]\n\n7.1.1 More parameters always improve fit OVERFITTING\nGet the data and standardize it. In section 7.1.1 McElreath (2020) exolains that we rescale brain size instead of standardizing it because we wnat to preserve zero as a reference point.\n\nd <- tibble(\n    species = c(\"afarensis\", \"africanus\", \"habilis\", \"boisei\",\n                \"rudolfensis\", \"ergaster\", \"sapiens\"),\n    brain = c(438, 452, 612, 521, 752, 871, 1350),\n    mass = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)) |>\n    mutate(brain_r = scales::rescale(brain),\n           mass_s = scale(as.vector(mass)))\n\nplot the raw data\n\ncolr <- data.frame(colr = seq(from = min(d$mass) - 5, to = max(d$mass) + 5, \n                              length.out = 100))\nggplot(d, aes(x = mass, y = brain, label = species)) +\n    geom_segment(data = colr, aes(x = colr, xend = colr, y = -Inf, yend = Inf,\n                                  color = colr),\n                 inherit.aes = FALSE, size = 3) +\n    geom_point(color = \"gold\", size = 3) +\n    ggrepel::geom_text_repel(color = \"yellow\") +\n    scale_color_paletteer_c(\"scico::berlin\") +\n    theme_minimal() +\n    theme(panel.grid = element_blank(),\n          legend.position = \"none\") +\n    labs(title = \"Average brain volume vs body mass for 6 hominin species\",\n         x = \"body mass in kg\", y = \"brain volume in cc\")\n\n\n\n\nSee hadley for very nice discussion on how to process several models in one dataframe.\n\n\n7.1.2 Too few parameters hurts UNDERFITTING\nSee Rethinking box in section 7.1.2 which explains the Bias-variance trade-off.\nBias relates to underfitting and variance to over-fitting."
  },
  {
    "objectID": "ch07_information.html#entropy-and-accuracy",
    "href": "ch07_information.html#entropy-and-accuracy",
    "title": "7  Ulysses’ Compass",
    "section": "7.2 Entropy and accuracy",
    "text": "7.2 Entropy and accuracy\n\n7.2.1 Firing the weatherperson\n\n# the emoji used in his section\nsun <- emo::ji(\"sun\")\nrain <- emo::ji(\"cloud_with_rain\")\numbrella <- emo::ji(\"closed_umbrella\")\n\nThe currently emplyoed weather person has the following data\n\nweather <- data.frame(\n    day = 1:10,\n    predicted = rep(c(1, 0.6), times = c(3, 7)),\n    observed = rep(c(rain, sun), times = c(3, 7))\n    ) |>\n    t() |>\n    as.data.frame() |>\n    tibble::rownames_to_column()\n\nweather |> gt::gt(rowname_col = \"rowname\") |>\n    gt::tab_options(column_labels.hidden = TRUE)\n\n\n\n\n\n  \n  \n  \n    day\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n    predicted\n1.0\n1.0\n1.0\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n    observed\n🌧\n🌧\n🌧\n☀️\n☀️\n☀️\n☀️\n☀️\n☀️\n☀️\n  \n  \n  \n\n\n\n\nThe new weather person has this data\n\nweather_new <- data.frame(\n    day = 1:10,\n    predicted = 0,\n    observed = rep(c(rain, sun), times = c(3, 7))\n    ) |>\n    t() |>\n    as.data.frame() |>\n    tibble::rownames_to_column()\n\nweather_new |> gt::gt(rowname_col = \"rowname\") |>\n    gt::tab_options(column_labels.hidden = TRUE)\n\n\n\n\n\n  \n  \n  \n    day\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n    predicted\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n    observed\n🌧\n🌧\n🌧\n☀️\n☀️\n☀️\n☀️\n☀️\n☀️\n☀️\n  \n  \n  \n\n\n\n\nwould be, i.e. the expected nb of correct predictions,\n\n3 * 1 + 7 * 0.4\n\n[1] 5.8\n\n\nwhich gives a frequency per day (probability) of\n\n(3 * 1 + 7 * 0.4) / 10\n\n[1] 0.58\n\n\n\n\n7.2.2 Information and uncertainty\n\\[\nH(p)=-E(\\log{p_i})=-\\sum_{i=1}^n{p_i \\cdot \\log{p_i}}\n\\]\nthe entropy for the above is\n\np <- c(0.3, 0.7)\n-sum(p * log(p))\n\n[1] 0.6108643\n\n\nbut in Abu Dhabi it is\n\np <- c(0.01, 0.99)\n-sum(p * log(p))\n\n[1] 0.05600153\n\n\n\n\n7.2.3 From entropy to accuracy\n\\[\nD_{KL}(p,q) = H(p,q) - H(p) = -\\sum{p_i \\cdot (\\log{p_i} - \\log{q_i})} =\n-\\sum{p_i \\cdot \\frac {\\log{p_i}} {\\log{q_i}}}\n\\]\n\n\n7.2.4 Estimating divergence\nThe whole point here is the if we have 2 models, with 2 different probability distributions \\(q\\) and \\(r\\)\nthen their respective divergence is\n\\[\nD_{KL}(p,q) = H(p,q) - H(p) = E(\\log{q}) - E(\\log{p})\n\\]\nand\n\\[\nD_{KL}(p,r) = H(p,r) - H(p) = E(\\log{r}) - E(\\log{p})\n\\]\nand therefore their relative divergence between each other is\n\\[\n\\begin{align*}\nD_{KL}(p,q) - D_{KL}(p,r) &= [H(p,q) - H(p)] - [H(p,r) - H(p)] \\\\\n&= H(p,q) - H(p,r)  \\\\ &= E(\\log{q}) - E(\\log{r})\n\\end{align*}\n\\]\nand the relative value of the \\(D_{KL}(p,q)\\) and \\(D_{KL}(p,r)\\) is approximated with their deviance\n\\[\nD(q) = -2 \\sum_i{\\log{q_i}} \\\\\nD(r) = -2 \\sum_i{\\log{r_i}}\n\\]\nor, even more simply we could use the total score\n\\[\nS(q) = \\sum_i{\\log{q_i}} \\\\\nS(r) = \\sum_i{\\log{r_i}}\n\\]\n\nImportant: Since the deviance / total score represente the relative distance from the target, it does not mean anything by itself. It means something only when comparing models with each other.\n\nThe Bayesian version of the log-probability score is called the log-pointwise-predictive-density (lppd) and is defined as\nThe log-pointwise-predictive-density\n\\[\nlppd(y, \\Theta) = \\sum_{i=1}^N{\\log{Pr(y_i)}} =\n\\sum_{i=1}^N{\\log{\\frac{1}{S}\\sum_{s=1}^SPr(y_i \\mid \\Theta)}}\n\\]\n\n\n7.2.5 Scoring the right data\nNote the the total score, or deviance used just previously suffers from the same flaw as \\(R^2\\). That is, the more parameters (i.e. complexity), the better fit we obtain regardless of the relevance of such a complexity."
  },
  {
    "objectID": "ch07_information.html#golem-taming-regularization",
    "href": "ch07_information.html#golem-taming-regularization",
    "title": "7  Ulysses’ Compass",
    "section": "7.3 Golem taming: regularization",
    "text": "7.3 Golem taming: regularization\n\\[\n\\begin{align*}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x_i \\\\\n\\alpha &\\sim \\mathcal{N}(0, 100) \\\\\n\\beta &\\sim \\mathcal{N}(0, 1) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch07_information.html#predicting-predictive-accuracy",
    "href": "ch07_information.html#predicting-predictive-accuracy",
    "title": "7  Ulysses’ Compass",
    "section": "7.4 Predicting predictive accuracy",
    "text": "7.4 Predicting predictive accuracy\nThere are 2 families of strategies to evaluate models\n\nCross-validation\nInformation citeria\n\n\n7.4.1 Cross-validation\nThe method suggested here is to use Leave-one-out cross validation (LOOCV) coupled with Pareto-smoothed importance sampling cross-validation (PSIS) to approximate the LOOCV’s score.\nThe best feature of PSIS is that it provides feed back about its own reliability.\n\n\n7.4.2 Information criteria\nThe difference in deviance between in-sample and out-of-sample is always \\(2 p\\) where p is the number of parameters.\nThe is the basis for the \\(AIC\\), the Akaike Information Criteria\n\\[\nAIC = D_{train} + 2 \\cdot p = -2\\cdot lppd + 2 \\cdot p\n\\] AIC is used when\n\nPriors are flat or overwhelmed by the likelihood\nThe posterior distribution is approximately multivariate Gaussian\nThe sample size \\(N\\) is much greater than the number of parameters \\(k\\)\n\n\n7.4.2.1 DIC (Deviance Information criteria)\nIt assumes a posterior distribution is approximately multivariate Gaussian like AIC which means it can be very wrong if the distribution is skewed.\n\n\n7.4.2.2 WAIC (Widely Applicable Information Criteria)\nThe penalty term is based on \\(V(y_i)\\) which is the variance in log-likelihood for the observation \\(i\\) in the sample. (See section 6.4.1 of the first edition of McElreath)\n\\[\np_{WAIC} = \\sum_{i=1}^N{V(y_i)} = \\sum_{i=1}^N{var_{\\theta} \\log{p(y_i \\mid \\theta)}}\n\\]\n\\[\nWAIC = -2 (lppd - p_{WAIC})\n\\] The penalty term is also called the effective number of parameters which is really not the right mathematical way of writing it. See discussion in section 7.4.2.\n\n\n\n7.4.3 Comparing CV, PSIS and WAIC\nPSIS and WAIC perform very similarly in the context of prdinary linear models.\nEstimation aside, PSIS has the distinct advantage of warning the user when it is unreliable."
  },
  {
    "objectID": "ch07_information.html#model-comparison",
    "href": "ch07_information.html#model-comparison",
    "title": "7  Ulysses’ Compass",
    "section": "7.5 Model comparison",
    "text": "7.5 Model comparison\n\nfn <- list.files(path=here::here(\"cache\"), pattern=\"ch06_b06_06_.*[.]rds$\")\nstopifnot(length(fn) == 1)\nb6.6 <- readRDS(here::here(\"cache\", fn))\nfn <- list.files(path=here::here(\"cache\"), pattern=\"ch06_b06_07_.*[.]rds$\")\nstopifnot(length(fn) == 1)\nb6.7 <- readRDS(here::here(\"cache\", fn))\nfn <- list.files(path=here::here(\"cache\"), pattern=\"ch06_b06_08_.*[.]rds$\")\nstopifnot(length(fn) == 1)\nb6.8 <- readRDS(here::here(\"cache\", fn))\n\n\n7.5.1 Model mis-selection\n\nloo::waic(b6.7)\n\n\nComputed from 4000 by 100 log-likelihood matrix\n\n          Estimate   SE\nelpd_waic   -172.5  6.6\np_waic         3.7  0.7\nwaic         345.1 13.1\n\n1 (1.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n# can also use this function if you need to manipulate the data\nloo::waic(b6.7)$estimates\n\n             Estimate         SE\nelpd_waic -172.527626  6.5710413\np_waic       3.663018  0.7004132\nwaic       345.055251 13.1420826\n\n\nThe waic is \\(-2 * elpd\\) which is not what Kurtz says in his version of the textbook. I think he is mistaken. McElreath on the other is conistent with his previous definitions.\n\nnear(loo::waic(b6.7)$estimates[\"elpd_waic\", \"Estimate\"] * -2,\n     loo::waic(b6.7)$estimates[\"waic\", \"Estimate\"])\n\n[1] TRUE\n\n\nand comparing the 3 models\n\nw <- loo::loo_compare(b6.6, b6.7, b6.8, criterion = \"waic\")\n\nand for more details\n\nprint(w, simplify = FALSE)\n\n     elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nb6.7    0.0       0.0  -172.5       6.6          3.7    0.7     345.1   13.1 \nb6.8  -15.8       4.8  -188.4       6.3          2.7    0.5     376.7   12.7 \nb6.6  -25.8       6.3  -198.4       5.5          1.6    0.3     396.7   11.1 \n\n\nKurtz make some mistake in describing this data. Be careful when reading him. McElreath is more precise.\nTo get the corresponding WAIC we simply mutliply by -2\n\ncbind(waic_diff = w[, \"elpd_diff\"] * -2, waic_se = w[, \"se_diff\"] * 2)\n\n     waic_diff   waic_se\nb6.7   0.00000  0.000000\nb6.8  31.68303  9.630005\nb6.6  51.69095 12.562092\n\n\nand we can also compare using loo which gives the same results.\n\nl <- loo::loo_compare(b6.6, b6.7, b6.8, criterion = \"loo\")\nprint(l, simplify = FALSE)\n\n     elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nb6.7    0.0       0.0  -172.5      6.6         3.7    0.7    345.1   13.2  \nb6.8  -15.8       4.8  -188.4      6.4         2.7    0.5    376.8   12.7  \nb6.6  -25.8       6.3  -198.4      5.5         1.6    0.3    396.8   11.1  \n\n\n\nw <- loo::loo_compare(b6.6, b6.7, b6.8, criterion = \"waic\") |> \n    data.frame() |>\n    tibble::rownames_to_column(\"model_name\") |>\n    mutate(model_name = forcats::fct_reorder(model_name, waic, .desc = TRUE))\nggplot(w, aes(x = waic, y = model_name, xmin = waic - se_waic, xmax = waic + se_waic)) +\n    geom_pointrange(shape = 19, size = 0.75, color = \"mediumseagreen\") +\n    ggrepel::geom_text_repel(aes(label = round(waic, 0))) +\n    ggthemes::theme_few() +\n    theme(title = element_text(color = \"midnightblue\"),\n          panel.border = element_blank()) +\n    labs(title = \"WAIC plot\", x = \"waic\", y = NULL)\n\n\n\n\nA last point about model comparison is that comparing the pointwise weights. See the useful comments by McElreath at the end of section 7.5.1.\n\\[\nw_i = \\frac{exp(-0.5 \\Delta_i)}{\\sum_j exp(-0.5 \\Delta_j)}\n\\] which can be obtained with brms::model_weights\n\nbrms::model_weights(b6.6, b6.7, b6.8) |>\n    round(digits = 4)\n\n  b6.6   b6.7   b6.8 \n0.0213 0.9787 0.0000 \n\n\n\n\n7.5.2 Outliers and other illusions\n\ndata(\"WaffleDivorce\")\nd <- WaffleDivorce |>\n    mutate(D = scale(as.vector(Divorce)),\n           M = scale(as.vector(Marriage)),\n           A = scale(as.vector(MedianAgeMarriage)))\n\n\nfn <- list.files(path=here::here(\"cache\"), pattern=\"ch05_b05_01_.*[.]rds$\")\nstopifnot(length(fn) == 1)\nb5.1 <- readRDS(here::here(\"cache\", fn))\nfn <- list.files(path=here::here(\"cache\"), pattern=\"ch05_b05_02_.*[.]rds$\")\nstopifnot(length(fn) == 1)\nb5.2 <- readRDS(here::here(\"cache\", fn))\nfn <- list.files(path=here::here(\"cache\"), pattern=\"ch05_b05_03_.*[.]rds$\")\nstopifnot(length(fn) == 1)\nb5.3 <- readRDS(here::here(\"cache\", fn))\n\n\nl <- loo::loo_compare(b5.1, b5.2, b5.3, criterion = \"loo\")\nprint(l, simplify = FALSE)\n\n     elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic\nb5.1   0.0       0.0   -62.3      6.4         3.1   1.7    124.5  12.8   \nb5.3  -1.5       0.4   -63.8      6.4         4.7   1.9    127.5  12.8   \nb5.2  -6.6       4.7   -68.9      4.9         2.2   0.8    137.7   9.9   \n\n\n\ndp <- tibble(pareto_k = b5.3$criteria$loo$diagnostics$pareto_k,\n       p_waic   = b5.3$criteria$waic$pointwise[, \"p_waic\"],\n       Loc      = d$Loc,\n       South = d$South)\nggplot(dp, aes(x = pareto_k, y = p_waic, color = Loc == \"ID\")) +\n    geom_vline(xintercept = .5, linetype = 2, color = \"black\", alpha = 1/2) +\n    geom_point(aes(shape = Loc == \"ID\")) +\n    geom_text(data = . %>% filter(p_waic > 0.5),\n              aes(x = pareto_k - 0.03, label = Loc),\n              hjust = 1) +\n    scale_color_manual(values = c(\"darkgreen\", \"violetred\")) +\n    scale_shape_manual(values = c(19, 19)) +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    labs(title = \"Gaussian model (b5.3)\",\n         subtitle = deparse1(b5.3$formula$formula),\n         caption = \"Different than McElreath but ok with Kurtz (same conclusions for both)\")\n\n\n\n\nTherefore we have ID which has too much influence. The solution is to use robust regression, a wonderful solution described by McElreath at the end of section 7.5.2.\nSee Kurtz on how to do it with brms as follows. We use \\(\\nu = 2\\), same as McElreath.\nMake sure you read McElreath and Kurtz on the t-distribution. A few things are important to remember. e.g. the parameter \\(\\sigma\\) is not the standard deviation in t-distribution.\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb5.3t <- xfun::cache_rds({\n  out <- brm(data = d, \n      family = student,\n      bf(D ~ 1 + M + A, nu = 2),\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 5)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch07_b05_03t\")\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 0.16 sec elapsed\n\n\n\nsummary(b5.3t)\n\n Family: student \n  Links: mu = identity; sigma = identity; nu = identity \nFormula: D ~ 1 + M + A \n         nu = 2\n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.10    -0.17     0.22 1.00     3682     2931\nM             0.05      0.20    -0.32     0.46 1.00     3994     2884\nA            -0.70      0.15    -0.98    -0.41 1.00     3878     2980\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.58      0.09     0.43     0.76 1.00     3529     2814\nnu        2.00      0.00     2.00     2.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand now we have a more robust results, i.e. where the influence from ID and ME is lessened.\n\ndp <- tibble(pareto_k = b5.3t$criteria$loo$diagnostics$pareto_k,\n       p_waic   = b5.3t$criteria$waic$pointwise[, \"p_waic\"],\n       Loc      = d$Loc,\n       South = d$South)\nggplot(dp, aes(x = pareto_k, y = p_waic, color = Loc == \"ID\")) +\n    geom_vline(xintercept = .5, linetype = 2, color = \"black\", alpha = 1/2) +\n    geom_point(aes(shape = Loc == \"ID\")) +\n    geom_text(data = . %>% filter(Loc %in% c(\"ID\", \"ME\")),\n              aes(x = pareto_k - 0.01, label = Loc),\n              hjust = 1) +\n    scale_color_manual(values = c(\"darkgreen\", \"violetred\")) +\n    scale_shape_manual(values = c(19, 19)) +\n    theme_minimal() +\n    theme(legend.position = \"none\",\n          title = element_text(color = \"midnightblue\")) +\n    labs(title = \"Student-t model (b5.3)\",\n         subtitle = \"using brms (see Kurtz)\",\n         caption = \"See Kurtz comments\")"
  },
  {
    "objectID": "ch07_information.html#summary",
    "href": "ch07_information.html#summary",
    "title": "7  Ulysses’ Compass",
    "section": "7.6 Summary",
    "text": "7.6 Summary\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://www.taylorandfrancis.com."
  },
  {
    "objectID": "ch08_interactions.html",
    "href": "ch08_interactions.html",
    "title": "8  Conditional Manatees",
    "section": "",
    "text": "Load the data, log transform the gdp measure, remove incomplete cases and create a character column for Africa or Not Africa.\n\ndata(rugged)\nd <- rugged |>\n  filter(complete.cases(rgdppc_2000)) |>\n  mutate(log_gdp = log(rgdppc_2000),\n         is_africa = if_else(cont_africa == 1, \"Africa\", \"Not Africa\"),\n         is_africa = as.factor(is_africa))\nrm(rugged)\n# NOTE: Make sure as.vector() is outside of scale().\n#       Otherwise it keeps the vector as an array and causes all sort of little\n#       problems.  In particular, a very obscure, fine error message\n#       in doing brms fit for b8.2.\ndd <- d |>\n  drop_na(rgdppc_2000) |>\n  mutate(log_gdp_s = log_gdp / mean(log_gdp),\n         rugged_s = scales::rescale(rugged),\n         rugged_sc = as.vector(scale(rugged_s, center = TRUE, scale = FALSE)))\n# glimpse(dd)\n\nand we use the following DAG, see overthinking box in introduction of section 8.1 for another possible DAG.\n\ndag <- list()\ndag$coords <- tibble(name = c(\"C\", \"G\", \"R\", \"U\"),\n                     x = c(3, 2, 1, 2),\n                     y = c(2, 2, 2, 1))\ndag$g <- dagify(\n  G ~ C + R + U,\n  R ~ U,\n  latent = \"U\",\n  outcome = \"G\",\n  coords = dag$coords)\np <- list()\np$g <- dag$g |>\n  ggdag_status(aes(color = status), as_factor = TRUE, node_size = 14,\n               text_size = 4, text_col = \"midnightblue\") +\n  # geom_dag_point(aes(color = status), size = 3, text = FALSE) +\n  # geom_dag_text(color = \"midnightblue\") +\n  scale_color_paletteer_d(\"khroma::light\", \n                          na.value = \"honeydew3\",\n                          direction = 1) +\n  theme_dag() +\n  theme(legend.position = c(0.8, 0.2)) +\n  labs(title = \"African nations\", \n       subtitle = \"Section 8.1\")\np$g\n\n\n\n\n\np1 <- dd |>\n  filter(grepl(\"^africa$\", x = is_africa, ignore.case = TRUE)) |>\n  ggplot(aes(x = rugged_s, y = log_gdp_s)) +\n  geom_smooth(method = \"lm\", formula = y ~ x, fill = \"lightblue\", color = \"royalblue\") +\n  geom_point(color = \"burlywood4\") +\n  theme_minimal() +\n  labs(title = \"African nations\", x = \"ruggedness (rescale)\", \n       y = \"log GDP (prop of mean)\")\np2 <- dd |>\n  filter(!grepl(\"^africa$\", x = is_africa, ignore.case = TRUE)) |>\n  ggplot(aes(x = rugged_s, y = log_gdp_s)) +\n  geom_smooth(method = \"lm\", formula = y ~ x, fill = \"burlywood1\", color = \"burlywood4\") +\n  geom_point(color = \"royalblue\") +\n  theme_minimal() +\n  labs(title = \"Non-African nations\", x = \"ruggedness (rescale)\",\n       y = \"log GDP (prop of mean)\")\nmsg <- \"Figure 8.2. Separate linear regressions inside and outside of Africa\"\np1 + p2 + plot_annotation(title = msg)\n\n\n\n\n\n\nand split the data into countries from Africa and not.\n\nlst <- d |>\n  split(d$is_africa)\n# str(lst)\n\nand now creating a simple univariate model\n\\[\n\\begin{align*}\n\\log{(log\\_gdp\\_s_i)} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot rugged\\_sc_i \\\\\n\\alpha &\\sim \\mathcal{N}(1, 1) \\\\\n\\beta &\\sim \\mathcal{N}(0, 1) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nNow fit the model. Get the prior samples by using sample_prior = TRUE\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb8.1a <- xfun::cache_rds({\n  out <- brm(\n    data = dd,\n    family = gaussian,\n    log_gdp_s ~ 1 + rugged_sc,\n    prior = c(\n      prior(normal(1, 1), class = Intercept),\n      prior(normal(0, 1), class = b),\n      prior(exponential(1), class = sigma)),\n    sample_prior = TRUE,\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 8)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_b08_01a\")\n\nCompiling Stan program...\n\n\nStart sampling\n\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 78.51 sec elapsed\n\n\n\nposterior_summary(b8.1a)\n\n                    Estimate   Est.Error        Q2.5      Q97.5\nb_Intercept      1.000081472 0.010482916  0.97889128  1.0203198\nb_rugged_sc      0.002595453 0.057174334 -0.11293461  0.1170604\nsigma            0.138208672 0.007877334  0.12389342  0.1547571\nprior_Intercept  0.979374473 0.995637434 -0.99669567  2.9484634\nprior_b         -0.002211324 1.015265447 -1.98742024  1.9703342\nprior_sigma      1.024724984 1.034587795  0.02633572  3.7691684\nlprior          -1.977778085 0.008367267 -1.99533339 -1.9628471\nlp__            91.741521467 1.286736578 88.29902430 93.1966649\n\n\nthe estimates are described in section 8.1.1 of McElreath but he seems to have\n\nset.seed(8)\nb8.1a_prior <- prior_samples(b8.1a)\n\nWarning: 'prior_samples' is deprecated. Please use 'prior_draws' instead.\n\npd <-\n  b8.1a_prior |>\n  slice_sample(n = 50) |>\n  tibble::rownames_to_column() |>\n  expand(nesting(rowname, Intercept, b), rugged_sc = c(-2, 2)) |>\n  mutate(log_gdp_s = Intercept + b * rugged_sc,\n         rugged_s  = rugged_sc + mean(dd$rugged_s))\n# glimpse(pd)\n\npd_estimate_fixed <- min(dd$log_gdp_s)\npd_estimate_b <- diff(range(dd$log_gdp_s))\n\np1 <- ggplot(pd, aes(x = rugged_s, y = log_gdp_s, group = rowname)) +\n  geom_line(color = \"lavender\") +\n  geom_hline(yintercept = range(dd$log_gdp_s), \n             size = 1, linetype = 2, color = \"royalblue\") +\n  geom_abline(intercept = pd_estimate_fixed, slope = pd_estimate_b,\n              color = \"purple\", size = 1) +\n  coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) +\n  labs(\n    subtitle = \"Intercept ~ dnorm(1, 1)\\nb ~ dnorm(0, 1)\",\n    x = \"ruggedness (rescaled)\",\n    y = \"log GDP (prop of mean)\") +\n  ggthemes::theme_hc()\n# p1\n\nNow using the prior where we want the intercept to be around 1 with extremes from 0.8 to 1.2 (i.e. a mean of 1 and sd of 0.1) and the slope to have extremes about \\(\\pm 0.6\\), that is a mean of 0 with sd of 0.3 (i.e. 2 sd with sd = 3 from a mean of 0).\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb8.1b <- xfun::cache_rds({\n  out <- update(\n    b8.1a,\n    newdata = dd,\n    prior = c(\n      prior(normal(1, 0.1), class = Intercept),\n      prior(normal(0, 0.3), class = b),\n      prior(exponential(1), class = sigma)),\n    sample_prior = TRUE,\n    seed = 8)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_b08_01b\")\n\nThe desired updates require recompiling the model\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 1:                0.031 seconds (Sampling)\nChain 1:                0.065 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.031 seconds (Warm-up)\nChain 2:                0.028 seconds (Sampling)\nChain 2:                0.059 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.031 seconds (Warm-up)\nChain 3:                0.024 seconds (Sampling)\nChain 3:                0.055 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 7e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 4:                0.031 seconds (Sampling)\nChain 4:                0.064 seconds (Total)\nChain 4: \n\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 58.73 sec elapsed\n\n\n\nposterior_summary(b8.1b)\n\n                    Estimate  Est.Error        Q2.5      Q97.5\nb_Intercept      1.000166075 0.01040321  0.98023034  1.0213012\nb_rugged_sc      0.001615194 0.05591031 -0.10967419  0.1118792\nsigma            0.138231472 0.00744684  0.12489902  0.1537012\nprior_Intercept  1.003443969 0.09940477  0.80845829  1.1954058\nprior_b          0.005170575 0.30228072 -0.56945691  0.5975727\nprior_sigma      0.971864509 0.98132613  0.02340771  3.5326634\nlprior           1.507661382 0.02871990  1.42927885  1.5384032\nlp__            95.316575288 1.25356563 92.00799294 96.7069674\n\n\n\nset.seed(8)\nb8.1b_prior <- prior_samples(b8.1b)\n\nWarning: 'prior_samples' is deprecated. Please use 'prior_draws' instead.\n\npd <-\n  b8.1b_prior |>\n  slice_sample(n = 50) |>\n  tibble::rownames_to_column() |>\n  expand(nesting(rowname, Intercept, b), rugged_sc = c(-2, 2)) |>\n  mutate(log_gdp_s = Intercept + b * rugged_sc,\n         rugged_s  = rugged_sc + mean(dd$rugged_s))\n# glimpse(pd)\n\np2 <- ggplot(pd, aes(x = rugged_s, y = log_gdp_s, group = rowname)) +\n  geom_line(color = \"lavender\") +\n  geom_hline(yintercept = range(dd$log_gdp_s), \n             size = 1, linetype = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) +\n  labs(\n    subtitle = \"Intercept ~ dnorm(0, 0.1)\\nb ~ dnorm(0, 0.3)\",\n    x = \"ruggedness (rescaled)\",\n    y = \"log GDP (prop of mean)\") +\n  ggthemes::theme_hc()\n# p2\n\n\nmsg <- \"Figure 8.3. Simulating different priors to evaluate their fit\"\np1 + p2 + plot_annotation(title = msg)\n\n\n\n\n\n\n\nWe add the cid variable to identify the continent.\n\ndd <- dd |>\n  mutate(cid = as.factor(if_else(cont_africa == 1, \"1\", \"2\")))\n\nand fitting the data to the following model\n\\[\n\\begin{align*}\n\\log{(log\\_gdp\\_s_i)} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha[cid] + \\beta \\cdot rugged\\_sc_i \\\\\n\\alpha &\\sim \\mathcal{N}(1, 0.1) \\\\\n\\beta &\\sim \\mathcal{N}(0, 0.3) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"60 secs.\"))\nb8.2 <- xfun::cache_rds({\n  out <- brm(\n    data = dd,\n    family = gaussian,\n    log_gdp_s ~ 0 + cid + rugged_sc,\n    prior = c(\n      prior(normal(1, 0.1), class = b, coef = cid1),\n      prior(normal(1, 0.1), class = b, coef = cid2),\n      prior(normal(0, 0.3), class = b, coef = rugged_sc),\n      prior(exponential(1), class = sigma)),\n    sample_prior = TRUE,\n    iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n    seed = 8)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_b08_02\")\n\nCompiling Stan program...\n\n\nStart sampling\n\n\nWarning: \n1 (0.6%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\ntictoc::toc()\n\nrun time of 60 secs., use the cache.: 69.67 sec elapsed\n\n\n\nsummary(b8.2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_gdp_s ~ 0 + cid + rugged_sc \n   Data: dd (Number of observations: 170) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ncid1          0.88      0.02     0.85     0.91 1.00     4031     2847\ncid2          1.05      0.01     1.03     1.07 1.00     4281     2812\nrugged_sc    -0.05      0.05    -0.14     0.04 1.00     4083     2820\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.11      0.01     0.10     0.13 1.00     4126     2962\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nand measuring the models’ performance\n\nloo::loo_compare(b8.1b, b8.2, criterion = \"waic\") |> print(simplify = FALSE)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nb8.2     0.0       0.0   126.1       7.4          4.1    0.8    -252.3   14.8 \nb8.1b  -31.7       7.3    94.5       6.5          2.5    0.3    -189.0   13.0 \n\n\nwith thew model weights\n\nbrms::model_weights(b8.1b, b8.2) |>\n  round(digits = 2)\n\nb8.1b  b8.2 \n 0.03  0.97 \n\n\nand create the fitted data used for the plot\n\ntidybayes::get_variables(b8.2)\n\n [1] \"b_cid1\"            \"b_cid2\"            \"b_rugged_sc\"      \n [4] \"sigma\"             \"prior_b_cid1\"      \"prior_b_cid2\"     \n [7] \"prior_b_rugged_sc\" \"prior_sigma\"       \"lprior\"           \n[10] \"lp__\"              \"accept_stat__\"     \"stepsize__\"       \n[13] \"treedepth__\"       \"n_leapfrog__\"      \"divergent__\"      \n[16] \"energy__\"         \n\n\n\nb8.2_seq <- crossing(cid = as.factor(1:2),\n                     rugged_sc = seq(from = -0.2, to = 1.2, length.out = 30)) |>\n  mutate(rugged_sc = as.vector(scale(rugged_sc)))\n# glimpse(b8.2_seq)\nb8.2_fitted <- fitted(b8.2, newdata = b8.2_seq, probs = c(0.015, 0.985)) |>\n  data.frame() |>\n  bind_cols(b8.2_seq) |>\n  mutate(is_africa = if_else(cid == 1, \"Africa\", \"Not Africa\")) |>\n  mutate(is_africa = as.factor(is_africa))\n# glimpse(b8.2_fitted)\n\n\nggplot(dd, aes(x = rugged_sc, y = log_gdp_s, fill = is_africa, color = is_africa)) +\n  geom_smooth(data = b8.2_fitted, aes(x = rugged_sc, y = Estimate, ymin = Q1.5, ymax = Q98.5),\n              stat = \"identity\",\n              alpha = 1/4, size = 1/2) +\n  geom_point(size = 1) +\n  scale_fill_manual(values = c(\"Africa\" = \"springgreen2\", \"Not Africa\" = \"violet\")) +\n  scale_color_manual(values = c(\"Africa\" = \"springgreen2\", \"Not Africa\" = \"violet\")) +\n  coord_cartesian(xlim = c(0, 1)) +\n  theme_minimal() +\n  theme(legend.position = c(.80, .90),\n        legend.title = element_blank()) +\n  labs(title = \"Figure 8.4\",\n       subtitle = \"model b8.2\",\n       x = \"ruggedness (standardized)\",\n       y = \"log GDP (as proportion of mean)\")\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\n\\log{(log\\_gdp\\_s_i)} &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{[cid]} + \\beta_{[cid]} \\cdot rugged\\_sc_i \\\\\n\\alpha &\\sim \\mathcal{N}(1, 0.1) \\\\\n\\beta &\\sim \\mathcal{N}(0, 0.3) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"70 secs.\"))\nb8.3 <- xfun::cache_rds({\n  out <- brm(data = dd,\n      family = gaussian,\n      formula = bf(log_gdp_s ~ 0 + a + b * rugged_sc,\n         a ~ 0 + cid,\n         b ~ 0 + cid,\n         nl = TRUE),\n      prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a),\n                prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a),\n                prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b),\n                prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 8)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_b08_03\")\n\nCompiling Stan program...\n\n\nStart sampling\n\n\nWarning: \n2 (1.2%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\ntictoc::toc()\n\nrun time of 70 secs., use the cache.: 75.64 sec elapsed\n\n\n\nsummary(b8.3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_gdp_s ~ 0 + a + b * rugged_sc \n         a ~ 0 + cid\n         b ~ 0 + cid\n   Data: dd (Number of observations: 170) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_cid1     0.89      0.02     0.85     0.92 1.00     4926     3441\na_cid2     1.05      0.01     1.03     1.07 1.00     5650     3326\nb_cid1     0.13      0.08    -0.02     0.28 1.00     4922     3327\nb_cid2    -0.14      0.06    -0.25    -0.03 1.00     4294     3031\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.11      0.01     0.10     0.13 1.00     4670     3244\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nloo::loo_compare(b8.1b, b8.2, b8.3, criterion = \"waic\") |> print(simplify = FALSE)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nb8.3     0.0       0.0   129.6       7.3          4.9    0.8    -259.2   14.6 \nb8.2    -3.5       3.2   126.1       7.4          4.1    0.8    -252.3   14.8 \nb8.1b  -35.1       7.5    94.5       6.5          2.5    0.3    -189.0   13.0 \n\n\nwith thew model weights\n\nbrms::model_weights(b8.1b, b8.2, b8.3) |>\n  round(digits = 2)\n\nb8.1b  b8.2  b8.3 \n 0.00  0.13  0.87 \n\n\n\n\n\n\nb8.3_seq <- crossing(cid = as.factor(1:2),\n                     rugged_sc = seq(from = -0.2, to = 1.2, length.out = 30)) |>\n  mutate(rugged_sc = as.vector(scale(rugged_sc)))\nglimpse(b8.3_seq)\n\nRows: 60\nColumns: 2\n$ cid       <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ rugged_sc <dbl> -1.66122768, -1.54666026, -1.43209283, -1.31752540, -1.20295…\n\nb8.3_fitted <- fitted(b8.3, newdata = b8.3_seq, probs = c(0.015, 0.985)) |>\n  data.frame() |>\n  bind_cols(b8.3_seq) |>\n  mutate(is_africa = if_else(cid == 1, \"Africa\", \"Not Africa\")) |>\n  mutate(is_africa = as.factor(is_africa))\nglimpse(b8.3_fitted)\n\nRows: 60\nColumns: 7\n$ Estimate  <dbl> 0.6668773, 0.6820424, 0.6972076, 0.7123727, 0.7275378, 0.742…\n$ Est.Error <dbl> 0.12461280, 0.11599999, 0.10739953, 0.09881465, 0.09024980, …\n$ Q1.5      <dbl> 0.4035901, 0.4376822, 0.4716878, 0.5043084, 0.5376742, 0.570…\n$ Q98.5     <dbl> 0.9424758, 0.9393188, 0.9344113, 0.9301145, 0.9271121, 0.923…\n$ cid       <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ rugged_sc <dbl> -1.66122768, -1.54666026, -1.43209283, -1.31752540, -1.20295…\n$ is_africa <fct> Africa, Africa, Africa, Africa, Africa, Africa, Africa, Afri…\n\n\n\nggplot(dd, aes(x = rugged_sc, y = log_gdp_s, fill = is_africa, color = is_africa)) +\n  geom_smooth(data = b8.3_fitted, aes(x = rugged_sc, y = Estimate, ymin = Q1.5, ymax = Q98.5),\n              stat = \"identity\",\n              alpha = 1/4, size = 1/2) +\n  geom_point(size = 1) +\n  scale_fill_manual(values = c(\"Africa\" = \"springgreen2\", \"Not Africa\" = \"violet\")) +\n  scale_color_manual(values = c(\"Africa\" = \"springgreen2\", \"Not Africa\" = \"violet\")) +\n  coord_cartesian(xlim = c(0, 1)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Figure 8.5\",\n       subtitle = \"model b8.3\",\n       x = \"ruggedness (standardized)\",\n       y = \"log GDP (as proportion of mean)\") +\n  facet_wrap(~ is_africa)"
  },
  {
    "objectID": "ch08_interactions.html#symmetry-of-interactions",
    "href": "ch08_interactions.html#symmetry-of-interactions",
    "title": "8  Conditional Manatees",
    "section": "8.2 Symmetry of interactions",
    "text": "8.2 Symmetry of interactions"
  },
  {
    "objectID": "ch08_interactions.html#continuous-interactions",
    "href": "ch08_interactions.html#continuous-interactions",
    "title": "8  Conditional Manatees",
    "section": "8.3 Continuous interactions",
    "text": "8.3 Continuous interactions\n\n8.3.1 A winter flower\n\ndata(tulips, package = \"rethinking\")\nd <- tulips |>\n  mutate(blooms_r = scales::rescale(blooms),\n         water_c = as.vector(scale(water, scale = FALSE)),\n         shade_c = as.vector(scale(shade, scale = FALSE)))\nrm(tulips)\n\n\n\n8.3.2 The models\n\n8.3.2.1 Calibrating the priors\nOur preliminary model, as a first jest in terms of prior is\n\\[\n\\begin{align*}\nblooms\\_r_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_W \\cdot water\\_c_i+ \\beta_S \\cdot shade\\_c_i \\\\\n\\alpha &\\sim \\mathcal{N}(0.5, 1) \\\\\n\\beta_W &\\sim \\mathcal{N}(0, 1) \\\\\n\\beta_S &\\sim \\mathcal{N}(0, 1) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nWhen looking at the data with skimr::skim() to evaluate the priors we obtain\n\nskimr::skim(d)\n\n\nData summary\n\n\nName\nd\n\n\nNumber of rows\n27\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nbed\n0\n1\nFALSE\n3\na: 9, b: 9, c: 9\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nwater\n0\n1\n2.00\n0.83\n1\n1.00\n2.00\n3.00\n3.00\n▇▁▇▁▇\n\n\nshade\n0\n1\n2.00\n0.83\n1\n1.00\n2.00\n3.00\n3.00\n▇▁▇▁▇\n\n\nblooms\n0\n1\n128.99\n92.68\n0\n71.12\n111.04\n190.30\n361.66\n▅▇▂▂▁\n\n\nblooms_r\n0\n1\n0.36\n0.26\n0\n0.20\n0.31\n0.53\n1.00\n▅▇▂▂▁\n\n\nwater_c\n0\n1\n0.00\n0.83\n-1\n-1.00\n0.00\n1.00\n1.00\n▇▁▇▁▇\n\n\nshade_c\n0\n1\n0.00\n0.83\n-1\n-1.00\n0.00\n1.00\n1.00\n▇▁▇▁▇\n\n\n\n\n\nWe see that blooms_r must be between 0 and 1. The prior used assigns most probability outside of that range\n\nm <- 0.5\ns <- 1\npnorm(q = 0, mean = m, sd = s) + pnorm(q = 1, mean = m, sd = s, lower.tail = FALSE)\n\n[1] 0.6170751\n\n\nlets say that we we want only 5% of the values outside the range (2.5% on each side) then, going with trial an error, the boundaries would be about\n\nm <- 0.5\ns <- 0.25\npnorm(q = 0, mean = m, sd = s) + pnorm(q = 1, mean = m, sd = s, lower.tail = FALSE)\n\n[1] 0.04550026\n\n\nTherefore we will use\n\\[\n\\alpha \\sim \\mathcal{N}(0.5, 0.25)\n\\] and since the range for water_c and shade_c is -1 to 1 then we can use the same logic for both as follows\n\nm <- 0\ns <- 0.25\npnorm(q = -1, mean = m, sd = s) + pnorm(q = 1, mean = m, sd = s, lower.tail = FALSE)\n\n[1] 6.334248e-05\n\n\nwhich means virtually almost all values will be between -1 and 1. When looking at the skimr::skim() summary we see that there are many extreme values so this prior covers this situation well.\nTherefore, our model with a little more informative priors is\n\\[\n\\begin{align*}\nblooms\\_r_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_W \\cdot water\\_c_i+ \\beta_S \\cdot shade\\_c_i \\\\\n\\alpha &\\sim \\mathcal{N}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\mathcal{N}(0, 0.25) \\\\\n\\beta_S &\\sim \\mathcal{N}(0, 0.25) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand we fit that model to the data\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"80 secs.\"))\nb8.4 <- xfun::cache_rds({\n  out <- brm(data = d,\n      family = gaussian,\n      formula = blooms_r ~ 1 + water_c + shade_c,\n      prior = c(prior(normal(0.5, 0.25), class = Intercept),\n                prior(normal(0, 0.25), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 8)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_b08_04\")\n\nCompiling Stan program...\n\n\nStart sampling\n\n\nWarning: \n4 (14.8%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\ntictoc::toc()\n\nrun time of 80 secs., use the cache.: 69.62 sec elapsed\n\n\n\nsummary(b8.4)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: blooms_r ~ 1 + water_c + shade_c \n   Data: d (Number of observations: 27) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.36      0.03     0.29     0.43 1.00     4474     2850\nwater_c       0.21      0.04     0.12     0.29 1.00     4288     2729\nshade_c      -0.11      0.04    -0.19    -0.04 1.00     4074     2872\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.18      0.03     0.13     0.24 1.00     3076     2732\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n8.3.2.2 Adding an interaction\nUsing the notation \\(\\gamma_{W, i} = \\beta_W+\\beta_{WS} \\cdot shade\\_c_i\\) we get the new model with interactions\n\\[\n\\begin{align*}\nblooms\\_r_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &=\n\\alpha + \\gamma_{W, i} \\cdot water\\_c_i+ \\beta_S \\cdot shade\\_c_i =\n\\alpha + \\beta_W \\cdot water\\_c_i + \\beta_S \\cdot shade\\_c_i + \\beta_{WS} \\cdot shade\\_c_i \\cdot water\\_c_i\n\\\\\n\\alpha &\\sim \\mathcal{N}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\mathcal{N}(0, 0.25) \\\\\n\\beta_S &\\sim \\mathcal{N}(0, 0.25) \\\\\n\\beta_WS &\\sim \\mathcal{N}(0, 0.25) \\\\\n\\sigma &\\sim \\mathcal{Exp}(1)\n\\end{align*}\n\\]\nand now fitting the model with interaction\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"90 secs.\"))\nb8.5 <- xfun::cache_rds({\n  out <- brm(data = d,\n      family = gaussian,\n      formula = blooms_r ~ 1 + water_c + shade_c + water_c:shade_c,\n      prior = c(prior(normal(0.5, 0.25), class = Intercept),\n                prior(normal(0, 0.25), class = b, coef = water_c),\n                prior(normal(0, 0.25), class = b, coef = shade_c),\n                prior(normal(0, 0.25), class = b, coef = \"water_c:shade_c\"),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n      seed = 8)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_b08_05\")\n\nCompiling Stan program...\n\n\nStart sampling\n\n\nWarning: \n3 (11.1%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\ntictoc::toc()\n\nrun time of 90 secs., use the cache.: 69.16 sec elapsed\n\n\n\nsummary(b8.5)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: blooms_r ~ 1 + water_c + shade_c + water_c:shade_c \n   Data: d (Number of observations: 27) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           0.36      0.03     0.30     0.41 1.00     5577     2814\nwater_c             0.21      0.03     0.14     0.27 1.00     5021     2549\nshade_c            -0.11      0.03    -0.18    -0.05 1.00     4408     2719\nwater_c:shade_c    -0.14      0.04    -0.22    -0.06 1.00     5722     3001\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.14      0.02     0.11     0.20 1.00     3104     2545\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n8.3.3 Plotting posterior predictions\nSee the very good details in Kurtz\n\n# augment the data\npoints <-\n  d |>\n  expand(fit = c(\"b8.4\", \"b8.5\"),\n         nesting(shade_c, water_c, blooms_r)) |>\n  mutate(x_grid = paste(\"shade_c =\", shade_c),\n         y_grid = fit)\n\n# redefine `nd`\nnd <- crossing(shade_c = -1:1, \n               water_c = c(-1, 1))\n\n# use `fitted()`\nset.seed(8)\n\ndf <- rbind(fitted(b8.4, newdata = nd, summary = F, nsamples = 20),\n            fitted(b8.5, newdata = nd, summary = F, nsamples = 20)) |>\n  data.frame() |>\n  rlang::set_names(mutate(nd, name = paste(shade_c, water_c, sep = \"_\")) |> \n                     pull()) |>\n  mutate(row = 1:n(),\n         fit = rep(c(\"b8.4\", \"b8.5\"), each = n() / 2)) |>\n  pivot_longer(-c(row:fit), values_to = \"blooms_r\") |>\n  separate(name, into = c(\"shade_c\", \"water_c\"), sep = \"_\") |>\n  mutate(shade_c = shade_c |> as.double(),\n         water_c = water_c |> as.double()) |>\n  # these will come in handy for `ggplot2::facet_grid()`\n  mutate(x_grid = paste(\"shade_c =\", shade_c),\n         y_grid = fit)\n\nWarning: Argument 'nsamples' is deprecated. Please use argument 'ndraws'\ninstead.\n\nWarning: Argument 'nsamples' is deprecated. Please use argument 'ndraws'\ninstead.\n\n# glimpse(df)\n  \n\nggplot(df, aes(x = water_c, y = blooms_r)) +\n  geom_line(aes(group = row), color = \"steelblue\", alpha = 1/5, size = 1/2) +\n  geom_point(data = points, color = \"steelblue\") +\n  scale_x_continuous(breaks = c(-1, 0, 1)) +\n  scale_y_continuous(breaks = c(0, .5, 1)) +\n  coord_cartesian(xlim = c(-1, 1), ylim = c(0, 1)) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        strip.background = element_rect(fill = \"pink\")) +\n  labs(title = \"Posterior predicted blooms\",\n       x = \"Water (centered)\", y = \"Blooms (recaled)\") +\n  facet_grid(y_grid ~ x_grid)\n\n\n\n\n\n\n8.3.4 Plotting prior predictions\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"20 secs.\"))\nb8.4p <- xfun::cache_rds({\n  out <- update(b8.4,\n         sample_prior = \"only\",\n         iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n         seed = 8)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_b08_04p\")\n\nStart sampling\n\n\nWarning: \n27 (100.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\nWarning: Found 27 observations with a pareto_k > 0.7 in model 'out'. It is\nrecommended to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\ntictoc::toc()\n\nrun time of 20 secs., use the cache.: 21.86 sec elapsed\n\n\n\ntictoc::tic(msg = sprintf(\"run time of %s, use the cache.\", \"20 secs.\"))\nb8.5p <- xfun::cache_rds({\n  out <- update(b8.5,\n         sample_prior = \"only\",\n         iter = 2000, warmup = 1000, chains = 4, cores = detectCores(),\n         seed = 8)\n  out <- brms::add_criterion(out, criterion = c(\"waic\", \"loo\"))},\n  file = \"ch08_b08_05p\")\n\nStart sampling\n\n\nWarning: \n27 (100.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\nWarning: Found 27 observations with a pareto_k > 0.7 in model 'out'. It is\nrecommended to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\ntictoc::toc()\n\nrun time of 20 secs., use the cache.: 18.87 sec elapsed\n\n\n\nset.seed(8)\n\ndf <- rbind(fitted(b8.4p, newdata = nd, summary = F, nsamples = 20),\n            fitted(b8.5p, newdata = nd, summary = F, nsamples = 20)) |>\n  data.frame() |>\n  rlang::set_names(mutate(nd, name = paste(shade_c, water_c, sep = \"_\")) |> pull()) |>\n  mutate(row = rep(1:20, times = 2),\n         fit = rep(c(\"b8.4\", \"b8.5\"), each = n() / 2)) |>\n  pivot_longer(-c(row:fit), values_to = \"blooms_r\") |>\n  separate(name, into = c(\"shade_c\", \"water_c\"), sep = \"_\") |>\n  mutate(shade_c = shade_c |> as.double(),\n         water_c = water_c |> as.double()) |>\n  # these will come in handy for `ggplot2::facet_grid()`\n  mutate(x_grid = paste(\"shade_c =\", shade_c),\n         y_grid = fit)\n\nWarning: Argument 'nsamples' is deprecated. Please use argument 'ndraws'\ninstead.\n\nWarning: Argument 'nsamples' is deprecated. Please use argument 'ndraws'\ninstead.\n\n# glimpse(df)\n\nggplot(df, aes(x = water_c, y = blooms_r, group = row)) +\n  geom_hline(yintercept = 0:1, linetype = 2) +\n  geom_line(aes(alpha = row == 1, size = row == 1),\n          color = \"steelblue\") +\n  scale_size_manual(values = c(1/2, 1)) +\n  scale_alpha_manual(values = c(1/3, 1)) +\n  scale_x_continuous(breaks = c(-1, 0, 1)) +\n  scale_y_continuous(breaks = c(0, .5, 1)) +\n  coord_cartesian(xlim = c(-1, 1),\n                ylim = c(-0.5, 1.5)) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        strip.background = element_rect(fill = \"pink\")) +\n  labs(title = \"Posterior predicted blooms\",\n       x = \"Water (centered)\", y = \"Blooms (recaled)\") +\nfacet_grid(y_grid ~ x_grid)"
  },
  {
    "objectID": "ch08_interactions.html#summary",
    "href": "ch08_interactions.html#summary",
    "title": "8  Conditional Manatees",
    "section": "8.4 Summary",
    "text": "8.4 Summary"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Kurz, Solomon. 2020. Statistical Rethinking with Brms. 2nd ed.\nhttps://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in R and Stan. 2nd ed. Boca Raton,\nFlorida: Chapman; Hall/CRC. http://www.taylorandfrancis.com."
  }
]