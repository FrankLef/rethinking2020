# Adventures in Covariance {#Covariance}

```{r}
#| include: false
library(dplyr)
library(tidyr)
library(tidybayes)
library(rethinking)
library(brms)
library(loo)
library(modelr)
library(skimr)
library(simstudy)
library(posterior)
library(scales)
library(dagitty)
library(ggplot2)
library(ggdag)
library(ggnetwork)
library(bayesplot)
library(patchwork)
library(paletteer)
```

Some options to facilitate the computations

```{r}
#  For execution on a local, multicore CPU with excess RAM
options(mc.cores = parallel::detectCores())
#  To avoid recompilation of unchanged Stan programs
rstan_options(auto_write = TRUE)
```

The default theme used by `ggplot2`

```{r}
theme_set(ggthemes::theme_stata(base_size = 11, base_family = "sans", 
                                scheme = "s2color"))
```


## Varying slopes by construction

### Simulate the population

```{r}
simCafes <- list()
simCafes <- within(simCafes, {
  # a := average morning wait time
  # b := average difference afternoon wait time
  Mu <- c("a" = 3.5, "b" = -1)
  
  # a := std dev of intercepts
  # b := std dev of slopes
  sigmas <- c("a" = 1, "b" = 0.5)
  
  # correlation between intercepts and slopes
  rho <- -0.7
  
  cov_ab <- prod(sigmas) * rho
})
```

McElreath mentions a difficulty using the `matrix` function. He misses the 
argument `byrow` which resolve this.

```{r}
# use byrow = TRUE to solve McElrath's issue
matrix(1:4, nrow = 2, ncol = 2, byrow = TRUE)
```

we get the covariance matrix `sigma` as follows

```{r}
simCafes <- within(simCafes, {
  # matrix of correlation
  Rho <- matrix(c(1, rho, rho, 1), nrow = 2)
  
  # covariance matrix
  Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
})
```

and we simulate the bivariate normal distribution

```{r}
simCafes <- within(simCafes, {
  n_cafes <- 20

  set.seed(1409)
  vary_effects <- MASS::mvrnorm(n = n_cafes, mu = Mu, Sigma = Sigma) |>
    as.data.frame() |>
    mutate(cafe = seq_len(n_cafes)) |>
    relocate(cafe)
})
# glimpse(simCafes$vary_effects)
```

and we plot the simulated data which represents the intercept and slope

```{r}
#| fig-cap: "Figure 14.2"
simCafes$vary_effects |>
  ggplot(aes(x = a, y = b)) +
  geom_point(shape = 1, size = 3, color = "purple") +
  lapply(X = 1:5 / 5, FUN = function(x) {
      stat_ellipse(type = "norm", level = x, linetype = "dotted", size = 0.25)}) +
  theme(legend.position = "none") +
  labs(title = sprintf("Distribution of intercept and slopes for %d cafes", 
                       simCafes$n_cafes),
       x = "intercepts (a_cafe)", y = "slope (b_cafe)")
```

### Simulate the observations (visits by cafe)

Now using the simulated intercepts and slopes, we create the simulated visits to each cafe.

```{r}
simCafes <- within(simCafes, {
  n_visits <- 10  # nb of visits to each cafe by robot
  sigma <- 0.5  # std dev within cafes
  
  set.seed(1409)
  data <- vary_effects |>
    expand(nesting(cafe, a, b), visit = seq_len(n_visits)) |>
    mutate(afternoon = rep(0:1, times = n()/2)) |>
    mutate(mu = a + b * afternoon) |>
    mutate(wait = rnorm(n = n(), mean = mu, sd = sigma))
})
# glimpse(simCafes$vary_effects)
# glimpse(simCafes$data)
```

and plot the simulated observations.

```{r}
simCafes$data |>
  mutate(afternoon = if_else(afternoon == 0, "M", "A"),
         day = rep(rep(1:5, each = 2), times = simCafes$n_cafes),
         label = paste("cafe", simCafes$data$cafe)) |>
  filter(cafe %in% c(1, 5)) |>
  ggplot(aes(x = visit, y = wait, group = day)) +
  geom_point(aes(color = afternoon), size = 2) +
  geom_line(color = "green") +
  scale_color_manual(values = c("M" = "royalblue", "A" = "hotpink")) +
  theme(legend.position = "none") +
  labs(title = "Varying slopes simulation") +
  facet_wrap(~ label, ncol = 1)
```

### The varying slopes model

#### The model

$$
\begin{align*}
wait_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha_{cafe[i]} + \beta_{cafe[i]} \cdot afternoon_i \\
\begin{bmatrix}
\alpha_{cafe} \\
\beta_{cafe}
\end{bmatrix}
&\sim
\mathcal{MVNormal}(
\begin{bmatrix}
\alpha \\
\beta
\end{bmatrix}
,
\bf{\Sigma}
) \\
\bf{\Sigma} &=
\begin{bmatrix}
\sigma_{\alpha} & 0 \\
0 & \sigma_{\beta}
\end{bmatrix}
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
\begin{bmatrix}
\sigma_{\alpha} & 0 \\
0 & \sigma_{\beta}
\end{bmatrix} \\
\alpha &\sim \mathcal{N}(0, 10) \\
\beta &\sim \mathcal{N}(0, 10) \\
\sigma &\sim \mathcal{HalfCauchy}(0, 1) \\
\sigma_{\alpha} &\sim \mathcal{HalfCauchy}(0, 1) \\
\sigma_{\beta} &\sim \mathcal{HalfCauchy}(0, 1) \\
\rho
&\sim \mathcal{LKJcorr}(K=2)
\end{align*}
$$

#### LKJ prior

We use the `ggdist` package to illustrate the LKJ distribution.

```{r}
#| fig-cap: "Figure 14.3"
lkj_dist <- list()
lkj_dist <- within(lkj_dist, {
  df <- crossing(K = 2:4, eta = 1:3, x = seq(from = -1, to = 1, by = 0.05)) |>
    mutate(
      id = K^eta,
      label_K = paste0("K==", K),
      label_eta = paste0("eta==", eta),
      dens = ggdist::dlkjcorr_marginal(x = x, K = K, eta = eta)
    )
  p <- df |>
    ggplot(aes(x = x, y = dens, color = id)) +
    geom_line(size = 1) +
    scale_y_continuous(breaks = c(0, 0.5, 1)) +
    scale_color_paletteer_c("palr::sst_pal") +
    theme(legend.position = "none") +
    facet_grid(facets = label_eta ~ label_K, scales= "fixed", labeller = label_parsed) +
    labs(title = "Lewandowski-Kurowicka-Joe Distribution",
         x = "correlation", y = "density")
})
# glimpse(lkj_dist$df)
lkj_dist$p
```



```{r ch14_fit14_01}
tictoc::tic(msg = sprintf("run time of %s, use the cache.", "60 secs."))
fit14_01 <- xfun::cache_rds({
  out <- brm(
    data = simCafes$data,
    family = gaussian,
    formula = wait ~ 1 + afternoon + (1 + afternoon | cafe),
    prior = c(
      prior(normal(5, 2), class = Intercept),
      prior(normal(-1, 0.5), class = b),
      prior(exponential(1), class = sd),
      prior(exponential(1), class = sigma),
      prior(lkj(2), class = cor)),
    sample_prior = TRUE,
    iter = 1000, warmup = 500, chains = 2,
    cores = detectCores(), seed = 1423)
  add_criterion(out, c("loo", "waic"))},
  file = "ch14_fit14_01", rerun = FALSE)
tictoc::toc()
```



```{r ch14_post14_01}
post14_01 <- list()
post14_01 <- within(post14_01, {
  prior <- prior_draws(x = fit14_01)
  
  post <- tidy_draws(model = fit14_01)
  
  # dataframe of correlations to plot
  corr = data.frame("value" = c(prior$cor_cafe, post$cor_cafe__Intercept__afternoon),
                    "id" = c(rep("prior", nrow(prior)), rep("post", nrow(post))))

  coefs <- fit14_01 |>
    spread_draws(b_Intercept, b_afternoon, r_cafe[cafe, term]) |>
    pivot_wider(id_cols = c("b_Intercept", "b_afternoon", "cafe"),
                names_from = "term", values_from = "r_cafe") |>
    group_by(cafe) |>
    summarize(b_Intercept = mean(b_Intercept), b_afternoon = mean(b_afternoon),
              r_afternoon = mean(afternoon), r_Intercept = mean(Intercept)) |>
    mutate(Intercept = b_Intercept + r_Intercept,
           afternoon = b_afternoon + r_afternoon) |>
    select(cafe, Intercept, afternoon) |>
    identity()
  all <- simCafes$vary_effects |>
    rename("Intercept" = a, "afternoon" = b)
  
  all <- bind_rows("real" = all, "post" = coefs, .id = "id")
})
# glimpse(post14_01$post)
# glimpse(post14_01$coefs)
# glimpse(post14_01$all)
```

```{r ch14_plot14_01}
#| fig-cap: "Figure 14.4"
plot14_01 <- list()
plot14_01 <- within(plot14_01, {
  cor <- post14_01$corr |> 
    ggplot(aes(x = value, color = id, linetype = id)) +
    geom_density(size = 1, adjust = 0.75) +
    scale_color_manual(values = c("prior" = "black", "post" = "blue")) +
    scale_linetype_manual(values = c("prior" = "longdash", "post" = "solid")) +
    theme(legend.position = c(0.8, 0.8), legend.title = element_blank()) +
    labs(title = "Posterior and Prior distribution of the correlation",
         x = "correlation")
})
plot14_01$cor
```



```{r ch14_pred14_01}
pred14_01 <- list()
pred14_01 <- within(pred14_01, {
  pred_df <- simCafes$data |>
    group_by(cafe, afternoon) |>
    summarise(mwait = mean(wait)) |>
    add_predicted_draws(object = fit14_01) |>
    mean_qi(.width = 0.89) |>
    mutate(term = if_else(afternoon == 0, "Intercept", "afternoon"))
  
  real <- pred_df |>
    select(cafe, term, mwait) |>
    pivot_wider(id_cols = cafe, names_from = term, values_from = mwait)
  
  pred <- pred_df |>
    select(cafe, term, .prediction) |>
    pivot_wider(id_cols = cafe, names_from = term, values_from = .prediction)
  
  all <- bind_rows("real" = real, "pred" = pred, .id = "id")
})
# glimpse(pred14_01$all)
# glimpse(simCafes$data)
```



```{r}
#| fig-cap: "Figure 14.5"
plot14_01 <- within(plot14_01, {
  coefs <- post14_01$all |>
    ggplot(mapping = aes(x = Intercept, y = afternoon, group = cafe, color = id)) +
    lapply(X = 1:5 / 5, FUN = function(x) {
      stat_ellipse(data = post14_01$all, mapping = aes(x = Intercept, y = afternoon),
                   inherit.aes = FALSE,
                   geom = "polygon", type = "norm", level = x, linewidth = 1/5,
                 color = "dodgerblue", fill = "transparent")}) +
    geom_point() +
    geom_line(color = "black") +
    scale_color_paletteer_d("awtools::spalette", direction = 1) +
    theme(legend.position = c(0.2, 0.2),
          legend.title = element_blank()) +
    labs(title = "Coefficients and shrinkage",
         x = "Intercept", y = "Slope")
    
  
  wait <- pred14_01$all |>
    ggplot(mapping = aes(x = Intercept, y = afternoon, group = cafe, color = id)) +
    lapply(X = 1:5 / 5, FUN = function(x) {
      stat_ellipse(data = pred14_01$all, mapping = aes(x = Intercept, y = afternoon),
                   inherit.aes = FALSE,
                   geom = "polygon", type = "norm", level = x, linewidth = 1/5,
                 color = "dodgerblue", fill = "transparent")}) +
    geom_point() +
    geom_line(color = "black") +
    scale_color_paletteer_d("awtools::spalette", direction = -1) +
    theme(legend.position = c(0.8, 0.2),
          legend.title = element_blank()) +
    labs(title = "Waiting time and shrinkage",
         x = "morning wait", y = "afternoon wait")
})
# plot14_01$coefs
wrap_plots(plot14_01[c("coefs", "wait")]) +
  plot_annotation(title = "Shrinkage in two dimensions")

```

## Advanced varying slopes


```{r ch14_dataChimp}
data(chimpanzees)
dataChimp <- chimpanzees |>
  mutate(block = factor(block),
         actor = factor(actor),
         treatment = factor(1 + prosoc_left + 2 * condition, levels = 1:4,
                            labels = c("AR", "AL", "PR", "PL")))
rm(chimpanzees)
dataChimp |>
  skim() |>
  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))
```

#### The model

$$
\begin{align*}
L_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &\sim \gamma_{treatment[i]} + \alpha_{actor[i], treatment[i]} + \beta_{block[i], treatment[i]} \\
\gamma_{treatment[i]} &\sim \mathcal{N}(0, 1), \, \text{for } i = 1 \ldots 4 \\


\begin{bmatrix}
\alpha_{j, 1} \\
\alpha_{j, 2} \\
\alpha_{j, 3} \\
\alpha_{j, 4}
\end{bmatrix}
&\sim
\mathcal{MVNormal}(
\begin{bmatrix}
0 \\
0 \\
0 \\
0\\
\end{bmatrix}
,
\bf{\Sigma_{actor}}
) \\


\begin{bmatrix}
\beta_{j, 1} \\
\beta_{j, 2} \\
\beta_{j, 3} \\
\beta_{j, 4}
\end{bmatrix}
&\sim
\mathcal{MVNormal}(
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
\end{bmatrix}
,
\bf{\Sigma_{block}}
) \\



\bf{\Sigma_{actor}} &=
\begin{bmatrix}
\sigma_{factor} & 0 \\
0 & \sigma_{factor}
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & \rho_{factor} \\
\rho_{factor} & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
\sigma_{factor} & 0 \\
0 & \sigma_{factor}\\
\end{bmatrix} \\



\bf{\Sigma_{block}} &=
\begin{bmatrix}
\sigma_{block} & 0 \\
0 & \sigma_{block}
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & \rho_{block} \\
\rho_{block} & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
\sigma_{block} & 0 \\
0 & \sigma_{block}\\
\end{bmatrix} \\



\sigma_{actor, j} &\sim \mathcal{Exponential}(1), \, \text{for } i = 1 \ldots 4 \\
\sigma_{block, j} &\sim \mathcal{Exponential}(1), \, \text{for } i = 1 \ldots 4 \\


\rho_{actor}, \rho_{block} &\sim \mathcal{LKJcorr}(2)

\end{align*}
$$

We don't do model m14.2 since it is only done to illustrate centralized vs
non-centralized parametrization and that `brms` uses only non-centralized
parametrization.


```{r ch14_fit14_03}
tictoc::tic(msg = sprintf("run time of %s, use the cache.", "80 secs."))
fit14_03 <- xfun::cache_rds({
  out <- brm(
    data = dataChimp,
    family = bernoulli,
    formula = bf(pulled_left ~ 0 + treatment + (0 + treatment | actor) + (0 + treatment | block)),
    prior = c(
      prior(normal(0, 1), class = b),
      prior(exponential(1), class = sd, group = actor),
      prior(exponential(1), class = sd, group = block),
      prior(lkj(2), class = cor, group = actor),
      prior(lkj(2), class = cor, group = block)),
    iter = 1000, warmup = 500, chains = 2,
    cores = detectCores(), seed = 1427)
  add_criterion(out, c("loo", "waic"))},
  file = "ch14_fit14_03", rerun = FALSE)
tictoc::toc()
```
The plot is slightly different than what McElreath has. The open circle
represent the actual results and the solid circles are the predicted mean.

```{r ch14_plot14_03}
#| fig-cap: "Figure 14.7"
plot14_03 <- list()
plot14_03 <- within(plot14_03, {

  df <- dataPredicted <- dataChimp |>
    group_by(actor, treatment, block) |>
    summarize(prop = mean(pulled_left)) |>
    ungroup() |>
    add_epred_draws(fit14_03) |>
    summarize(prop = mean(prop),
              mean_qi(.epred, .width = 0.89) ) |>
    rename(".epred" = y, ".lower" = ymin, ".upper" = ymax)
  
  # every block is different but, for plotting, we use the average of the blocks
  df <- df |>
    group_by(actor, treatment) |>
    summarize(
      prop = mean(prop),
      .epred = mean(.epred),
      .lower = mean(.lower),
      .upper = mean(.upper)) |>
    mutate(label = paste("actor", actor)) |>
    mutate(condition = if_else(substring(treatment, 1, 1) == "A", "alone", "partner"),
           condition = as.factor(condition),
           prosoc_left = if_else(substring(treatment, 2, 2) == "R", "right", "left"),
           prosoc_left = as.factor(prosoc_left)) |>
    mutate(label = paste("actor", actor))
    
    
    
  p <- df |>
    ggplot(aes(x = treatment, y = .epred,
                             group = prosoc_left, color = prosoc_left,
                             fill = prosoc_left)) +
    geom_line(linetype = "solid", size = 1) +
    geom_point(shape = 16, size = 3) +
    geom_line(aes(y = prop), linetype = "solid", size = 1) +
    geom_point(aes(y = prop), shape = 1, size = 3) +
    geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 1/3) +
    geom_hline(yintercept = 0.5, color = "brown", linetype = 2) +
    scale_y_continuous(labels = scales::label_percent()) +
    scale_color_paletteer_d("jcolors::pal9") +
    coord_cartesian(ylim = c(0, 1)) +
    theme(legend.position = "bottom") +
    labs(title = "Posterior expected predictions with 89% CI",
         subtitle = "Open circles are actual results, solid circles are mean predictions.",
         x = NULL, y = "proportion pulled left") +
    facet_grid(. ~ label)
})
# glimpse(plot14_03$df)
plot14_03$p
```

## Instruments and Causal designs

```{r}
ggdag::dagify(E ~ U, W ~ U, W ~ E) |>
    ggdag::ggdag_classic(layout = "sugiyama", text_col = "royalblue") +
    ggdag::theme_dag_blank(
      panel.background = element_rect(fill = "snow", color = "snow"))
```
> In causal terms, an instrument variable is a variable that acts like a natural
experiment  on the exposure $E$.

In mathematical terms the instrumental variable $Q$ is characterized as follows:

1. Independent of $U$, i.e. $Q \perp\!\!\!\perp U$
2. Not independent of $E$, i.e. $Q \not\!\perp\!\!\!\perp E$
3. Has no effect on $W$ except through $E$, also called the **exclusion condition**


> The exclusion restriction cannot be tested, and it is often implausible.

In the education and wage example, the simplest instrument variable $Q$ would be
as follows


```{r}
ggdag::dagify(E ~ U + Q, W ~ U, W ~ E) |>
    ggdag::ggdag_classic(layout = "sugiyama", text_col = "royalblue") +
    ggdag::theme_dag_blank(
      panel.background = element_rect(fill = "snow", color = "snow"))
```

We now use a simulation to illustrate.

> With real data, you never know what the right anser is. This is h=why studying
simulated examples is so important.

```{r chap14_simInstrument}
simInstrument <- list()
simInstrument <- within(simInstrument, {
  n = 500L
  U <- rnorm(n = n)
  Q <- sample(x = 1:4, size = n, replace = TRUE)
  E <- rnorm(n = n, mean = U + Q)
  # we assume that the true influence of E (education)
  #  on W (wage) is zero. Just for the sake of the example.
  W <- rnorm(n = n, mean = U + 0 * E)
  df <- data.frame(
    "W" = scale(W),
    "E" = scale(E),
    "Q" = scale(Q)
  )
})
# glimpse(simInstrument$df)
```



```{r}
#| echo: false
#| output: false
get_prior(
  data = simInstrument$df,
  formula = W ~ 1 + E,
  family = gaussian
)
```


```{r ch14_fit14_04}
tictoc::tic(msg = sprintf("run time of %s, use the cache.", "60 secs."))
fit14_04 <- xfun::cache_rds({
  out <- brm(
    data = simInstrument$df,
    family = gaussian,
    formula = W ~ 1 + E,
    prior = c(
      prior(normal(0, 0.2), class = Intercept),
      prior(normal(0, 0.5), class = b),
      prior(exponential(1), class = sigma)),
    iter = 1000, warmup = 500, chains = 2,
    cores = detectCores(), seed = 1429)
  add_criterion(out, c("loo", "waic"))},
  file = "ch14_fit14_04", rerun = FALSE)
tictoc::toc()
```
which gives us the results

```{r}
summarize_draws(fit14_04, "mean", "sd", ~quantile(.x, probs = c(0.055, 0.945)),
                default_convergence_measures()) |>
  filter(!grepl("^lp", x = variable)) |>
  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),
         across(.cols = starts_with("ess"), .fns = as.integer))
```

The value $b_E$ should have been close to zero. The oncorrect value is caused 
by the confounding effect of $U$.

Now, lets see what happens when we include the instrumental variable $Q$.



```{r ch14_fit14_05}
tictoc::tic(msg = sprintf("run time of %s, use the cache.", "60 secs."))
fit14_05 <- xfun::cache_rds({
  out <- brm(
    data = simInstrument$df,
    family = gaussian,
    formula = W ~ 1 + E + Q,
    prior = c(
      prior(normal(0, 0.2), class = Intercept),
      prior(normal(0, 0.5), class = b),
      prior(exponential(1), class = sigma)),
    iter = 1000, warmup = 500, chains = 2,
    cores = detectCores(), seed = 1429)
  add_criterion(out, c("loo", "waic"))},
  file = "ch14_fit14_05", rerun = FALSE)
tictoc::toc()
```


which gives us the results

```{r}
summarize_draws(fit14_05, "mean", "sd", ~quantile(.x, probs = c(0.055, 0.945)),
                default_convergence_measures()) |>
  filter(!grepl("^lp", x = variable)) |>
  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),
         across(.cols = starts_with("ess"), .fns = as.integer))
```

The results are now even more confusing as the influence of $Q$ makes the effect
of $E$ difficult to evaluate on its own.

So lets use the instrumental variable $Q$ again but taking into account the
covariance of $E$ and $Q$. That is, we express the model as a *multivariate 
statistical model as follows

$$
\begin{align*}
\begin{bmatrix}
W_i \\
E_i \\
\end{bmatrix}
&\sim
\mathcal{MVNormal}(
\begin{bmatrix}
\mu_{W, i} \\
\mu_{E, i}
\end{bmatrix}, 
\bf{\Sigma}
) \\
\mu_{W, i} &= \alpha_W + \beta_{EW}E_i \\
\mu_{E, i} &= \alpha_E + \beta_{QE}Q_i \\
\bf{\Sigma} &= 
\begin{bmatrix}
\sigma_W & 0 \\
0 & \sigma_E
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
\sigma_W & 0 \\
0 & \sigma_E
\end{bmatrix}
\\
\alpha_W, \alpha_E &\sim \mathcal{N}(0, 0.2) \\
\beta_{EW}, \beta_{QE} &\sim \mathcal{N}(0, 0.5) \\
\sigma_W, \sigma_E &\sim \mathcal{Exponential}(1) \\
\rho &\sim \mathcal{LKJ}(2)
\end{align*}
$$


```{r}
#| echo: false
#| output: false
get_prior(
  data = simInstrument$df,
  formula = bf(W ~ 1 + E) + bf(E ~ 1 + Q) + set_rescor(TRUE),
  family = gaussian
)
```
and the final fit is

```{r ch14_fit14_06}
tictoc::tic(msg = sprintf("run time of %s, use the cache.", "90 secs."))
fit14_06 <- xfun::cache_rds({
  out <- brm(
    data = simInstrument$df,
    family = gaussian,
    formula = bf(W ~ 1 + E) + bf(E ~ 1 + Q),
    prior = c(
      prior(normal(0, 0.2), class = Intercept, resp = E),
      prior(normal(0, 0.5), class = b, resp = E),
      prior(exponential(1), class = sigma, resp = E),
      prior(normal(0, 0.2), class = Intercept, resp = W),
      prior(normal(0, 0.5), class = b, resp = W),
      prior(exponential(1), class = sigma, resp = W),
      prior(lkj(2), class = rescor)),
    iter = 1000, warmup = 500, chains = 2,
    cores = detectCores(), seed = 1429)
  add_criterion(out, c("loo", "waic"))},
  file = "ch14_fit14_06", rerun = FALSE)
tictoc::toc()
```


which gives us the results

```{r}
summarize_draws(fit14_06, "mean", "sd", ~quantile(.x, probs = c(0.055, 0.945)),
                default_convergence_measures()) |>
  filter(!grepl("^lp", x = variable)) |>
  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),
         across(.cols = starts_with("ess"), .fns = as.integer))
```


## Social relations as correlated varying effects

### Data

The research can be find in this [paper](https://research-information.bris.ac.uk/ws/portalfiles/portal/41942591/koster2014sn.pdf).
However I could not find what the variable $dlndist$ is. I suppose it it the log
of some distance . . but why is it negative? It doesn't matter, it is only used
to color the edges of the network and not in the analysis per se.

```{r ch14_dataKL}
data(KosterLeckie, package = "rethinking")
dataKL <- list(
  "dyads" = kl_dyads,
  "houses" = kl_households)
rm(list = c("kl_dyads", "kl_households"))
dataKL$dyads <- dataKL$dyads |>
  # we only use these variable
  select(hidA, hidB, did, giftsAB, giftsBA, offset, dlndist) |>
  mutate(hidA = factor(hidA),
         hidB = factor(hidB),
         did = factor(did))
```

The data set can be summarized with the `skimr` package

```{r}
names(dataKL$dyads)
dataKL$dyads |> skim() |>
  select(-n_missing, -complete_rate) |>
  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))
```

We can visualize the distribution of dyadic gifts using a scatter plot

```{r}
#| fig-cap: "Figure 14.8"
dataKL$dyads |> ggplot(aes(x = giftsAB, y = giftsBA)) +
  geom_point(color = "darkblue") +
  geom_abline(slope = 1, intercept = 0, linetype = 2, color = "royalblue") +
  coord_equal(ratio = 1, xlim = c(0, 120), ylim = c(0, 120)) +
  labs(title = "Distribution of dyadic gifts")
```

and since this a social network then we can visualize the network with the
`ggnetwork` package.For details see [ggnetwork](https://cran.r-project.org/web/packages/ggnetwork/vignettes/ggnetwork.html).

First we need to create an igraph object. See [igraph](https://igraph.org/r/doc/graph_from_data_frame.html)
for details.


```{r}
# first we need to create an igraph
netKL <- list()
netKL <- within(netKL, {
  # the igraph object
  g <- dataKL$dyads |>
    mutate(giftsSum = abs(giftsAB - giftsBA)) |>
    igraph::graph_from_data_frame(directed = TRUE,
                                  vertices = dataKL$houses)
  # create the network dataframe
  df <- ggnetwork(g, layout = igraph::layout_in_circle(g))
  # plot the network
  p <- ggplot(df, aes(x = x, y = y, xend = xend, yend = yend)) +
    geom_edges(aes(color = dlndist)) +
    geom_nodes(aes(size = hwealth), color = "brown") +
    scale_color_paletteer_c("oompaBase::jetColors", direction = -1) +
    scale_size_continuous(range = c(1, 6)) +
    theme_blank() +
    labs(title = "The KosterLeckie Network")
})
# glimpse(netKL$df)
netKL$p
```


### Model

$$
\begin{align*}
\begin{bmatrix}
y_{a \rightarrow b} \\
y_{b \rightarrow a} \\
\end{bmatrix}
&\sim
\begin{bmatrix}
\mathcal{Poisson}(\lambda_{AB}) \\
\mathcal{Poisson}(\lambda_{BA}) \\
\end{bmatrix} \\
\log{\lambda_{AB}} &= \alpha + g_A + r_B + d_{AB} \\
\log{\lambda_{BA}} &= \alpha + g_B + r_A + d_{BA} \\
\begin{bmatrix}
g_i \\
r_i \\
\end{bmatrix}
&\sim
\mathcal{MVNormal}(
\begin{bmatrix}
0 \\
0
\end{bmatrix},
\Sigma_{gr}
) \\
\begin{bmatrix}
d_{ij} \\
d_{ji} \\
\end{bmatrix}
&\sim
\mathcal{MVNormal}(
\begin{bmatrix}
0 \\
0
\end{bmatrix},
\Sigma_d
) \\

\Sigma_{gr} &= 
\begin{bmatrix}
\sigma_g & 0 \\
0 & \sigma_r &
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & \rho_{gr} \\
\rho_{gr} & 1 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
\sigma_g & 0 \\
0 & \sigma_r \\
\end{bmatrix} \\

\Sigma_d &= 
\begin{bmatrix}
\sigma_d & 0 \\
0 & \sigma_d &
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & \rho_d \\
\rho_d & 1 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
\sigma_d & 0 \\
0 & \sigma_d \\
\end{bmatrix} \\

\sigma_d, \sigma_g, \sigma_r &\sim \mathcal{Exponential}(1) \\


\rho_{gr}, \rho_d &\sim \mathcal{LKJ}(4)
\end{align*}
$$

Kurtz says that there is no known way to use `brms`. The package `bisonR` 
that specializes in social networks and uses `brms`.
It can be found at [bison](https://github.com/JHart96/bisonR) with a useful 
vignette at [vignette](https://jordanhart.co.uk/bisonR/articles/getting_started.html).

::: {.callout-note}
This section is skipped. But one day, it might be interesting to do it with the
`bisonR` package.
:::

## Continuous categories and the Gaussian process

### Spatial autocorrelation in Oceanic tools

#### Data

The observed data is as follows

```{r}
data(Kline)
dataKline <- list()
dataKline <- within(dataKline, {
  df <- Kline |>
    mutate(log_pop = log(population),
           log_pop_s = scale(log_pop),
           cid = factor(contact, levels = c("low", "high")))
  })
rm(Kline)
dataKline$df |>
  skim() |>
  select(-n_missing, -complete_rate) |>
  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2))
```

and the distance matrix is as follows, with a visualization as a heatmap with
the 

```{r}
data(islandsDistMatrix)
dataKline <- within(dataKline, {
  dist <- islandsDistMatrix
  })
rm(islandsDistMatrix)
# dataKline$dist |>
#   round(digits = 1)

heatmaply::heatmaply(
  x = dataKline$dist,
  cellnote = round(dataKline$dist, 1),
  colors = unclass(paletteer::paletteer_c("pals::ocean.speed", n = 16)),
  hide_colorbar = TRUE,
  main = "Distances between Oceanic Societies in Thousands of km")
```

and we will also use a matrix of the longitudinal and lagitudinal relative
positions of the societies. This is an approximation only as it used the
km per degree at the equator which is 111.32 km per degree at the equator.
This gives the following relative position matrix. The longitudes and latitudes
can be found in the `Kline2` dataset.

```{r}
data(Kline2)
dataKline <- within(dataKline, {
  pos <- Kline2 |>
    mutate(lat_pos = lat * 0.11132,
           lon2_pos = lon2 * 0.11132)
  })
rm(Kline2)
```

```{r}
#| fig-align: "center"
dataKline$pos |>
  ggplot(aes(x = lon2_pos, y = lat_pos, size = log(population), color = log(population))) +
  geom_point() +
  ggrepel::geom_text_repel(aes(label = culture), size = 4) +
  scale_color_paletteer_c("pals::isol") +
  theme(legend.position = "none") +
  labs(title = "Relative positions of societies",
       x = "longitude in km", y = "latitude in km")
```


#### Model

We use the scientific model. See section 11.2.1, p. 356, in the Overthinking
box.

$$
\begin{align*}
T_i &\sim \mathcal{Poisson}(\lambda_i) \\
\lambda_i &= \frac{\alpha P_i^\beta}{\gamma}
\end{align*}
$$

and since we need a varying intercept we had one in a *multiplicative form*. We
could simply add it but then it could become negative.


$$
\begin{align*}
T_i &\sim \mathcal{Poisson}(\lambda_i) \\
\lambda_i &= \exp(k_{society}) \frac{\alpha P_i^\beta}{\gamma}
\end{align*}
$$
Here $k_{society}$ is the varying intercept. These intercept are part of
a multivariate distribution. The multivariate intercept prior is defined as

$$
\begin{align*}
\begin{bmatrix}
k_1 \\
k_2 \\
k_3 \\
\vdots \\
k_{10}
\end{bmatrix}
&\sim
\mathcal{MVNormal}(
\begin{bmatrix}
0 \\
0 \\
0 \\
\vdots \\
0
\end{bmatrix}
,
\bf{K}
) \\
\bf{K} &= \eta^2 \exp(-\rho^2D_{ij}^2) + \delta_{ij}\sigma^2
\end{align*}
$$

::: {.callout-note}
The rest of the section comes from @kurtz2020b who did such a fantastic work
at adapting the wonderful *rethinking* to `brms`. I am so grateful for the 
opportunity to enjoy all this.
:::

::: {.callout-important}
There is a lot more details and info at @kurtz2020b. Please read it to get the
full picture.
:::

We could have used $D_{ij}$ instead of $D_{ij}^2$ for $\bf{K}$. $D_{ij}^2$
because of its half-Gaussian shape which is more sigmoidal in shape. As
illustrated just below.

```{r}
#| fig-cap: "Figure 14.10"
#| fig-align: "center"
ggplot(data.frame(x = c(0, 4)), aes(x = x)) +
  stat_function(aes(color = "-exp(D)"), fun = function(x) exp(-1 * x), size = 2) +
  stat_function(aes(color = "-exp(D^2)"), fun = function(x) exp(-1 * x^2), size = 2) +
  scale_color_paletteer_d("ggsci::alternating_igv") +
  theme(legend.position = c(0.8, 0.8),
        legend.title = element_blank()) +
  labs(title = "Shape of function relating distance to covariance",
       x = "distance", y = "correlation")
```

#### Fit

Since our model is non-linear (scientific formula) then the fit with `brms`
uses the non-linear form.

```{r}
#| echo: false
#| output: false
get_prior(data = dataKline$pos,
          formula = bf(total_tools ~ exp(a) * population^b / g,
                  a ~ 1 + gp(lat_pos, lon2_pos, scale = FALSE),
                  b + g ~ 1,
                  nl = TRUE),
          family = poisson)
```


```{r ch14_fit14_08}
tictoc::tic(msg = sprintf("run time of %s, use the cache.", "90 secs."))
fit14_08 <- xfun::cache_rds({
  brm(
    data = dataKline$pos,
    family = poisson(link = "identity"),
    formula =  bf(total_tools ~ exp(a) * population^b / g,
                  a ~ 1 + gp(lat_pos, lon2_pos, scale = FALSE),
                  b + g ~ 1,
                  nl = TRUE),
    prior = c(
      prior(normal(0, 1), nlpar = a),
      prior(exponential(1), nlpar = b, lb = 0),
      prior(exponential(1), nlpar = g, lb = 0),
      prior(inv_gamma(2.874624, 2.941204), class = lscale, coef = gplat_poslon2_pos, nlpar = a),
      prior(exponential(1), class = sdgp, coef = gplat_poslon2_pos, nlpar = a)),
    iter = 2000, warmup = 1000, chains = 2,
    sample_prior = TRUE,
    cores = detectCores(), seed = 1433)},
  file = "ch14_fit14_08", rerun = FALSE)
tictoc::toc()
```
which gives the results

```{r}
fit14_08
```


and to have a summary similar to McElreath on p. 472 we can use

```{r}
fit14_08 |>
  summarize_draws("mean", "sd", ~quantile(.x, probs = c(0.055, 0.945)),
                  default_convergence_measures()) |>
  filter(!grepl("^lp", x = variable)) |>
  mutate(across(.cols = where(is.numeric), .fns = round, digits = 2),
         across(.cols = starts_with("ess"), .fns = as.integer))
```

or with `posterior_summary` which is simpler but lacks de ESS

```{r}
posterior_summary(fit14_08) |>
  round(digits = 2)
```


The differences with `rethinking` are because `brms` uses non-centering. The
coefficient that is really different is the $\alpha$ in $\lambda_i = \exp(k_{society}) \frac{\alpha P_i^\beta}{\gamma}$.

To evaluate the differences between `brms` and `rethinking` we can look at what
the posterior would be like, without taking into account the covariances.

For `brms` we have

```{r}
pop <- as.integer(c(min(dataKline$pos$population),
             median(dataKline$pos$population),
             max(dataKline$pos$population)))
tools <- as.integer(c(min(dataKline$pos$total_tools),
               mean(dataKline$pos$total_tools),
               max(dataKline$pos$total_tools)))
a <- fixef(fit14_08)["a_Intercept", "Estimate"]
b <- fixef(fit14_08)["b_Intercept", "Estimate"]
g <- fixef(fit14_08)["g_Intercept", "Estimate"]
# brms uses a different equation for lambda
l <- exp(a) * pop ^ b / g
prob_brms <- round(dpois(x = tools, lambda = l), 4)
prob_brms
```
as opposed to `rehinking`` which would give

```{r}
a <- 1.41
b <- 0.28
g <- 0.60
# rethinking uses a different equation for lambda
l <- a * pop ^ b / g
prob_rethink <- round(dpois(x = tools, lambda = l), 4)
prob_rethink
```
The results between the 2 are different and cannot really be compared because

* correlations between $D$ in `rethinking` and $D^2$ in `brms` is not the same and
* `brms` has a very different way of computing the likelihood

Basically, `rethinking` uses the following definition of $\bf{K_{ij}}$

$$
\bf{K_{ij}} = \eta^2 \exp \left(-\rho^2D_{ij}^2 \right) + \delta_{ij}\sigma^2
$$
where the term $\delta_{ij}\sigma^2$ is used when $i=j$, see p. 470 of @elreath2020.
In the oceanic case it is not applicable since we have only one observation per society.
Therefore the equation is this case is

$$
\bf{K_{ij}} = \eta^2 \exp \left(-\rho^2D_{ij}^2 \right)
$$


whereas `brms` uses


$$
\bf{K_{ij}} = sdgp^2 \exp \left(\frac{-D_{ij}^2}{2 \cdot lscale^2} \right)
$$

and so the 2 equations can be compared as follows

$$
\begin{align*}
\eta^2 &= sdgp^2 \\
-\rho^2 &= \frac{1}{2 \cdot lscale^2}
\end{align*}
$$

and we note that the distance used can always be found as follows

$$
\begin{align*}
\bf{K_{ij}} &= sdgp^2 \exp \left(\frac{-D_{ij}^2}{2 \cdot lscale^2} \right) \\
\log \left( \bf{K_{ij}} \right) &= \log \left(sdgp^2 \right) + \frac{-D_{ij}^2}{2 \cdot lscale^2} \\
D_{ij}^2 &= 2 \cdot lscale^2 \cdot \left[ \log \left(sdgp^2 \right) - \log \left( \bf{K_{ij}} \right) \right]
\end{align*}
$$

and therefore we can figure out the covariance, as shown in figure 14.11 on 
p. 473 as follows

```{r ch14_post14_08}
post14_08 <- list()
post14_08 <- within(post14_08, {
  # prior <- prior_draws(fit14_08) |>
  #   mutate(draws = 1:n(),
  #          rho2 = - 1 / (2 * lscale_a__1_gplat_poslon2_pos ^2))
  post <- tidy_draws(fit14_08) |>
    mutate(prior_rho2 = - 1 / (2 * prior_lscale_a__1_gplat_poslon2_pos ^2),
           rho2 = - 1 / (2 * lscale_a_gplat_poslon2_pos ^2)) |>
    identity()
})
# glimpse(post14_08$prior)
glimpse(post14_08$post)
```



```{r}
#| fig-cap: "Figure 14.11"
#| fig-align: "center"
```


### Phylogenic distance





## Summary

## Practice
